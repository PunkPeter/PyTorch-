{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基础"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.Tensor([1.0]) # 由list生成tensor，这里权重就是一个标量\n",
    "w.requires_grad = True # 用于自动求梯度，该参数设为True\n",
    "                        #（如果不设为True，意味我们不需要这个参数的梯度，不会在backward时自动计算grad参数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.])\n",
      "1.0\n",
      "tensor([ 1.])\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "None\n",
      "<class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "print(w)\n",
    "print(w.item())\n",
    "print(w.data)\n",
    "print(w.type())             # a的类型是tensor\n",
    "print(w.data.type())        # a.data的类型是tensor\n",
    "print(w.grad)\n",
    "print(type(w.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 代码的本质是构建计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    ''' 模型 '''\n",
    "    return x * w # w是一个tensor，这里*被重载了，进行的是Tensor间的数乘（x会被自动转换为张量）\n",
    "    # 包含tensor对象的运算，关于这个tensor就会产生计算图！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(x, y):\n",
    "    y_pred = forward(x) # 模型输入x的输出\n",
    "    return (y_pred-y) ** 2 # 由输出和标签计算当前样本loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict (before training) 4 4.0\n",
      "\tgrad: 1.0 2.0 -2.0\n",
      "\tgrad: 2.0 4.0 -7.840000152587891\n",
      "\tgrad: 3.0 6.0 -16.228801727294922\n",
      "progress: 0 7.315943717956543\n",
      "\tgrad: 1.0 2.0 -1.478623867034912\n",
      "\tgrad: 2.0 4.0 -5.796205520629883\n",
      "\tgrad: 3.0 6.0 -11.998146057128906\n",
      "progress: 1 3.9987640380859375\n",
      "\tgrad: 1.0 2.0 -1.0931644439697266\n",
      "\tgrad: 2.0 4.0 -4.285204887390137\n",
      "\tgrad: 3.0 6.0 -8.870372772216797\n",
      "progress: 2 2.1856532096862793\n",
      "\tgrad: 1.0 2.0 -0.8081896305084229\n",
      "\tgrad: 2.0 4.0 -3.1681032180786133\n",
      "\tgrad: 3.0 6.0 -6.557973861694336\n",
      "progress: 3 1.1946394443511963\n",
      "\tgrad: 1.0 2.0 -0.5975041389465332\n",
      "\tgrad: 2.0 4.0 -2.3422164916992188\n",
      "\tgrad: 3.0 6.0 -4.848389625549316\n",
      "progress: 4 0.6529689431190491\n",
      "\tgrad: 1.0 2.0 -0.4417421817779541\n",
      "\tgrad: 2.0 4.0 -1.7316293716430664\n",
      "\tgrad: 3.0 6.0 -3.58447265625\n",
      "progress: 5 0.35690122842788696\n",
      "\tgrad: 1.0 2.0 -0.3265852928161621\n",
      "\tgrad: 2.0 4.0 -1.2802143096923828\n",
      "\tgrad: 3.0 6.0 -2.650045394897461\n",
      "progress: 6 0.195076122879982\n",
      "\tgrad: 1.0 2.0 -0.24144840240478516\n",
      "\tgrad: 2.0 4.0 -0.9464778900146484\n",
      "\tgrad: 3.0 6.0 -1.9592113494873047\n",
      "progress: 7 0.10662525147199631\n",
      "\tgrad: 1.0 2.0 -0.17850565910339355\n",
      "\tgrad: 2.0 4.0 -0.699742317199707\n",
      "\tgrad: 3.0 6.0 -1.4484672546386719\n",
      "progress: 8 0.0582793727517128\n",
      "\tgrad: 1.0 2.0 -0.1319713592529297\n",
      "\tgrad: 2.0 4.0 -0.5173273086547852\n",
      "\tgrad: 3.0 6.0 -1.070866584777832\n",
      "progress: 9 0.03185431286692619\n",
      "\tgrad: 1.0 2.0 -0.09756779670715332\n",
      "\tgrad: 2.0 4.0 -0.3824653625488281\n",
      "\tgrad: 3.0 6.0 -0.7917022705078125\n",
      "progress: 10 0.017410902306437492\n",
      "\tgrad: 1.0 2.0 -0.07213282585144043\n",
      "\tgrad: 2.0 4.0 -0.2827606201171875\n",
      "\tgrad: 3.0 6.0 -0.5853137969970703\n",
      "progress: 11 0.009516451507806778\n",
      "\tgrad: 1.0 2.0 -0.053328514099121094\n",
      "\tgrad: 2.0 4.0 -0.2090473175048828\n",
      "\tgrad: 3.0 6.0 -0.43272972106933594\n",
      "progress: 12 0.005201528314501047\n",
      "\tgrad: 1.0 2.0 -0.039426326751708984\n",
      "\tgrad: 2.0 4.0 -0.15455150604248047\n",
      "\tgrad: 3.0 6.0 -0.3199195861816406\n",
      "progress: 13 0.0028430151287466288\n",
      "\tgrad: 1.0 2.0 -0.029148340225219727\n",
      "\tgrad: 2.0 4.0 -0.11426162719726562\n",
      "\tgrad: 3.0 6.0 -0.23652076721191406\n",
      "progress: 14 0.0015539465239271522\n",
      "\tgrad: 1.0 2.0 -0.021549701690673828\n",
      "\tgrad: 2.0 4.0 -0.08447456359863281\n",
      "\tgrad: 3.0 6.0 -0.17486286163330078\n",
      "progress: 15 0.0008493617060594261\n",
      "\tgrad: 1.0 2.0 -0.01593184471130371\n",
      "\tgrad: 2.0 4.0 -0.062453269958496094\n",
      "\tgrad: 3.0 6.0 -0.12927818298339844\n",
      "progress: 16 0.00046424579340964556\n",
      "\tgrad: 1.0 2.0 -0.011778593063354492\n",
      "\tgrad: 2.0 4.0 -0.046172142028808594\n",
      "\tgrad: 3.0 6.0 -0.09557533264160156\n",
      "progress: 17 0.0002537401160225272\n",
      "\tgrad: 1.0 2.0 -0.00870823860168457\n",
      "\tgrad: 2.0 4.0 -0.03413581848144531\n",
      "\tgrad: 3.0 6.0 -0.07066154479980469\n",
      "progress: 18 0.00013869594840798527\n",
      "\tgrad: 1.0 2.0 -0.006437778472900391\n",
      "\tgrad: 2.0 4.0 -0.025236129760742188\n",
      "\tgrad: 3.0 6.0 -0.052239418029785156\n",
      "progress: 19 7.580435340059921e-05\n",
      "\tgrad: 1.0 2.0 -0.004759550094604492\n",
      "\tgrad: 2.0 4.0 -0.018657684326171875\n",
      "\tgrad: 3.0 6.0 -0.038620948791503906\n",
      "progress: 20 4.143271507928148e-05\n",
      "\tgrad: 1.0 2.0 -0.003518819808959961\n",
      "\tgrad: 2.0 4.0 -0.0137939453125\n",
      "\tgrad: 3.0 6.0 -0.028553009033203125\n",
      "progress: 21 2.264650902361609e-05\n",
      "\tgrad: 1.0 2.0 -0.00260162353515625\n",
      "\tgrad: 2.0 4.0 -0.010198593139648438\n",
      "\tgrad: 3.0 6.0 -0.021108627319335938\n",
      "progress: 22 1.2377059647405986e-05\n",
      "\tgrad: 1.0 2.0 -0.0019233226776123047\n",
      "\tgrad: 2.0 4.0 -0.0075397491455078125\n",
      "\tgrad: 3.0 6.0 -0.0156097412109375\n",
      "progress: 23 6.768445018678904e-06\n",
      "\tgrad: 1.0 2.0 -0.0014221668243408203\n",
      "\tgrad: 2.0 4.0 -0.0055751800537109375\n",
      "\tgrad: 3.0 6.0 -0.011541366577148438\n",
      "progress: 24 3.7000872907810844e-06\n",
      "\tgrad: 1.0 2.0 -0.0010514259338378906\n",
      "\tgrad: 2.0 4.0 -0.0041217803955078125\n",
      "\tgrad: 3.0 6.0 -0.008531570434570312\n",
      "progress: 25 2.021880391112063e-06\n",
      "\tgrad: 1.0 2.0 -0.0007772445678710938\n",
      "\tgrad: 2.0 4.0 -0.0030469894409179688\n",
      "\tgrad: 3.0 6.0 -0.006305694580078125\n",
      "progress: 26 1.1044940038118511e-06\n",
      "\tgrad: 1.0 2.0 -0.0005745887756347656\n",
      "\tgrad: 2.0 4.0 -0.0022525787353515625\n",
      "\tgrad: 3.0 6.0 -0.0046634674072265625\n",
      "progress: 27 6.041091182851233e-07\n",
      "\tgrad: 1.0 2.0 -0.0004248619079589844\n",
      "\tgrad: 2.0 4.0 -0.0016651153564453125\n",
      "\tgrad: 3.0 6.0 -0.003444671630859375\n",
      "progress: 28 3.296045179013163e-07\n",
      "\tgrad: 1.0 2.0 -0.0003139972686767578\n",
      "\tgrad: 2.0 4.0 -0.0012311935424804688\n",
      "\tgrad: 3.0 6.0 -0.0025491714477539062\n",
      "progress: 29 1.805076408345485e-07\n",
      "\tgrad: 1.0 2.0 -0.00023221969604492188\n",
      "\tgrad: 2.0 4.0 -0.0009107589721679688\n",
      "\tgrad: 3.0 6.0 -0.0018854141235351562\n",
      "progress: 30 9.874406714516226e-08\n",
      "\tgrad: 1.0 2.0 -0.00017189979553222656\n",
      "\tgrad: 2.0 4.0 -0.0006742477416992188\n",
      "\tgrad: 3.0 6.0 -0.00139617919921875\n",
      "progress: 31 5.4147676564753056e-08\n",
      "\tgrad: 1.0 2.0 -0.0001270771026611328\n",
      "\tgrad: 2.0 4.0 -0.0004978179931640625\n",
      "\tgrad: 3.0 6.0 -0.00102996826171875\n",
      "progress: 32 2.9467628337442875e-08\n",
      "\tgrad: 1.0 2.0 -9.393692016601562e-05\n",
      "\tgrad: 2.0 4.0 -0.0003681182861328125\n",
      "\tgrad: 3.0 6.0 -0.0007610321044921875\n",
      "progress: 33 1.6088051779661328e-08\n",
      "\tgrad: 1.0 2.0 -6.937980651855469e-05\n",
      "\tgrad: 2.0 4.0 -0.00027179718017578125\n",
      "\tgrad: 3.0 6.0 -0.000560760498046875\n",
      "progress: 34 8.734787115827203e-09\n",
      "\tgrad: 1.0 2.0 -5.125999450683594e-05\n",
      "\tgrad: 2.0 4.0 -0.00020122528076171875\n",
      "\tgrad: 3.0 6.0 -0.0004177093505859375\n",
      "progress: 35 4.8466972657479346e-09\n",
      "\tgrad: 1.0 2.0 -3.790855407714844e-05\n",
      "\tgrad: 2.0 4.0 -0.000148773193359375\n",
      "\tgrad: 3.0 6.0 -0.000308990478515625\n",
      "progress: 36 2.6520865503698587e-09\n",
      "\tgrad: 1.0 2.0 -2.8133392333984375e-05\n",
      "\tgrad: 2.0 4.0 -0.000110626220703125\n",
      "\tgrad: 3.0 6.0 -0.0002288818359375\n",
      "progress: 37 1.4551915228366852e-09\n",
      "\tgrad: 1.0 2.0 -2.09808349609375e-05\n",
      "\tgrad: 2.0 4.0 -8.20159912109375e-05\n",
      "\tgrad: 3.0 6.0 -0.00016880035400390625\n",
      "progress: 38 7.914877642178908e-10\n",
      "\tgrad: 1.0 2.0 -1.5497207641601562e-05\n",
      "\tgrad: 2.0 4.0 -6.103515625e-05\n",
      "\tgrad: 3.0 6.0 -0.000125885009765625\n",
      "progress: 39 4.4019543565809727e-10\n",
      "\tgrad: 1.0 2.0 -1.1444091796875e-05\n",
      "\tgrad: 2.0 4.0 -4.482269287109375e-05\n",
      "\tgrad: 3.0 6.0 -9.1552734375e-05\n",
      "progress: 40 2.3283064365386963e-10\n",
      "\tgrad: 1.0 2.0 -8.344650268554688e-06\n",
      "\tgrad: 2.0 4.0 -3.24249267578125e-05\n",
      "\tgrad: 3.0 6.0 -6.580352783203125e-05\n",
      "progress: 41 1.2028067430946976e-10\n",
      "\tgrad: 1.0 2.0 -5.9604644775390625e-06\n",
      "\tgrad: 2.0 4.0 -2.288818359375e-05\n",
      "\tgrad: 3.0 6.0 -4.57763671875e-05\n",
      "progress: 42 5.820766091346741e-11\n",
      "\tgrad: 1.0 2.0 -4.291534423828125e-06\n",
      "\tgrad: 2.0 4.0 -1.71661376953125e-05\n",
      "\tgrad: 3.0 6.0 -3.719329833984375e-05\n",
      "progress: 43 3.842615114990622e-11\n",
      "\tgrad: 1.0 2.0 -3.337860107421875e-06\n",
      "\tgrad: 2.0 4.0 -1.33514404296875e-05\n",
      "\tgrad: 3.0 6.0 -2.86102294921875e-05\n",
      "progress: 44 2.2737367544323206e-11\n",
      "\tgrad: 1.0 2.0 -2.6226043701171875e-06\n",
      "\tgrad: 2.0 4.0 -1.049041748046875e-05\n",
      "\tgrad: 3.0 6.0 -2.288818359375e-05\n",
      "progress: 45 1.4551915228366852e-11\n",
      "\tgrad: 1.0 2.0 -1.9073486328125e-06\n",
      "\tgrad: 2.0 4.0 -7.62939453125e-06\n",
      "\tgrad: 3.0 6.0 -1.430511474609375e-05\n",
      "progress: 46 5.6843418860808015e-12\n",
      "\tgrad: 1.0 2.0 -1.430511474609375e-06\n",
      "\tgrad: 2.0 4.0 -5.7220458984375e-06\n",
      "\tgrad: 3.0 6.0 -1.1444091796875e-05\n",
      "progress: 47 3.637978807091713e-12\n",
      "\tgrad: 1.0 2.0 -1.1920928955078125e-06\n",
      "\tgrad: 2.0 4.0 -4.76837158203125e-06\n",
      "\tgrad: 3.0 6.0 -1.1444091796875e-05\n",
      "progress: 48 3.637978807091713e-12\n",
      "\tgrad: 1.0 2.0 -9.5367431640625e-07\n",
      "\tgrad: 2.0 4.0 -3.814697265625e-06\n",
      "\tgrad: 3.0 6.0 -8.58306884765625e-06\n",
      "progress: 49 2.0463630789890885e-12\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 50 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 51 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 52 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 53 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 54 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 55 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 56 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 57 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 58 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 59 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 60 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 61 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 62 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 63 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 64 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 65 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 66 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 67 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 68 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 69 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 70 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 71 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 72 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 73 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 74 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 75 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 76 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 77 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 78 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 79 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 80 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 81 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 82 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 83 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 84 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 85 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 86 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 87 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 88 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 89 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 90 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 91 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 92 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 93 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 94 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 95 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 96 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 97 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 98 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 99 9.094947017729282e-13\n",
      "predict (after training) 4 7.999998569488525\n"
     ]
    }
   ],
   "source": [
    "print(\"predict (before training)\", 4, forward(4).item())\n",
    " \n",
    "# 用的依然是原版的随机梯度下降，即每个epoch的每次迭代\n",
    "for epoch in range(100):\n",
    "    for x, y in zip(x_data, y_data):\n",
    "        l = loss(x,y) # forward计算loss: l是一个tensor\n",
    "        l.backward() # 自动将计算l的链路上的每一个（设置为True）的张量计算梯度\n",
    "        # 注意：每一次backward都会使计算图释放，下次forward就重新构建计算图\n",
    "        print('\\tgrad:', x, y, w.grad.item())\n",
    "        w.data = w.data - 0.01 * w.grad.data   # 权重更新时，注意grad也是一个tensor\n",
    "        # 所以权重更新（即权重数值的修改/计算，而非模型的计算）时，要使用data属性而不是用tensor对象本身，因为使用tensor对象会构建计算图！\n",
    "        # 这与打印时要使用.item()是相同的道理，使用一个标量，避免产生计算图\n",
    "        \n",
    "        '''\n",
    "        计算整体损失时，如果写\n",
    "        sum += l\n",
    "        一旦有tensor对象参与运算就会构建计算图，由于没有backward，导致这个计算图不停的扩大（每次增加一个加法结点），把内存都吃光了\n",
    "        所以要写成 sum += l.item()\n",
    "        注意！.item()方法只有在tensor只有一个元素时才可以调用，用于将张量转换为一个标量\n",
    "        '''\n",
    "        w.grad.data.zero_() # after update, remember set the grad to zero\n",
    "        '''\n",
    "        如果不清零，第一次L1对w的偏导还保留着，第二次算梯度时就是 【L1对w偏导+L2对w偏导】，即会造成梯度的累加\n",
    "        在有些情况下我们可能会需要梯度的累加\n",
    "        '''\n",
    " \n",
    "    print('progress:', epoch, l.item()) # 取出loss使用l.item，不要直接使用l（l是tensor会构建计算图）\n",
    " \n",
    "print(\"predict (after training)\", 4, forward(4).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二次模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict (befortraining) 4 tensor([ 21.])\n",
      "\tgrad: 1.0 2.0 2.0 2.0 2.0\n",
      "\tgrad: 2.0 4.0 22.880001068115234 11.440000534057617 5.720000267028809\n",
      "\tgrad: 3.0 6.0 77.04720306396484 25.682401657104492 8.560800552368164\n",
      "Epoch: 0 18.321826934814453\n",
      "\tgrad: 1.0 2.0 -1.1466078758239746 -1.1466078758239746 -1.1466078758239746\n",
      "\tgrad: 2.0 4.0 -15.536651611328125 -7.7683258056640625 -3.8841629028320312\n",
      "\tgrad: 3.0 6.0 -30.432214736938477 -10.144071578979492 -3.381357192993164\n",
      "Epoch: 1 2.858394145965576\n",
      "\tgrad: 1.0 2.0 0.3451242446899414 0.3451242446899414 0.3451242446899414\n",
      "\tgrad: 2.0 4.0 2.4273414611816406 1.2136707305908203 0.6068353652954102\n",
      "\tgrad: 3.0 6.0 19.449920654296875 6.483306884765625 2.161102294921875\n",
      "Epoch: 2 1.1675907373428345\n",
      "\tgrad: 1.0 2.0 -0.32242679595947266 -0.32242679595947266 -0.32242679595947266\n",
      "\tgrad: 2.0 4.0 -5.845773696899414 -2.922886848449707 -1.4614434242248535\n",
      "\tgrad: 3.0 6.0 -3.8828859329223633 -1.294295310974121 -0.43143177032470703\n",
      "Epoch: 3 0.04653334245085716\n",
      "\tgrad: 1.0 2.0 0.01369333267211914 0.01369333267211914 0.01369333267211914\n",
      "\tgrad: 2.0 4.0 -1.9140911102294922 -0.9570455551147461 -0.47852277755737305\n",
      "\tgrad: 3.0 6.0 6.855700492858887 2.285233497619629 0.761744499206543\n",
      "Epoch: 4 0.14506366848945618\n",
      "\tgrad: 1.0 2.0 -0.11818885803222656 -0.11818885803222656 -0.11818885803222656\n",
      "\tgrad: 2.0 4.0 -3.664388656616211 -1.8321943283081055 -0.9160971641540527\n",
      "\tgrad: 3.0 6.0 1.7454700469970703 0.5818233489990234 0.1939411163330078\n",
      "Epoch: 5 0.009403289295732975\n",
      "\tgrad: 1.0 2.0 -0.03326845169067383 -0.03326845169067383 -0.03326845169067383\n",
      "\tgrad: 2.0 4.0 -2.7738723754882812 -1.3869361877441406 -0.6934680938720703\n",
      "\tgrad: 3.0 6.0 4.014009475708008 1.338003158569336 0.4460010528564453\n",
      "Epoch: 6 0.04972923547029495\n",
      "\tgrad: 1.0 2.0 -0.050147056579589844 -0.050147056579589844 -0.050147056579589844\n",
      "\tgrad: 2.0 4.0 -3.1150074005126953 -1.5575037002563477 -0.7787518501281738\n",
      "\tgrad: 3.0 6.0 2.8533897399902344 0.9511299133300781 0.3170433044433594\n",
      "Epoch: 7 0.025129113346338272\n",
      "\tgrad: 1.0 2.0 -0.020544052124023438 -0.020544052124023438 -0.020544052124023438\n",
      "\tgrad: 2.0 4.0 -2.8858280181884766 -1.4429140090942383 -0.7214570045471191\n",
      "\tgrad: 3.0 6.0 3.292379379272461 1.0974597930908203 0.36581993103027344\n",
      "Epoch: 8 0.03345605731010437\n",
      "\tgrad: 1.0 2.0 -0.013420581817626953 -0.013420581817626953 -0.013420581817626953\n",
      "\tgrad: 2.0 4.0 -2.9246826171875 -1.46234130859375 -0.731170654296875\n",
      "\tgrad: 3.0 6.0 2.990907669067383 0.9969692230224609 0.3323230743408203\n",
      "Epoch: 9 0.027609655633568764\n",
      "\tgrad: 1.0 2.0 0.0033445358276367188 0.0033445358276367188 0.0033445358276367188\n",
      "\tgrad: 2.0 4.0 -2.841381072998047 -1.4206905364990234 -0.7103452682495117\n",
      "\tgrad: 3.0 6.0 3.0377025604248047 1.0125675201416016 0.3375225067138672\n",
      "Epoch: 10 0.02848036028444767\n",
      "\tgrad: 1.0 2.0 0.014836311340332031 0.014836311340332031 0.014836311340332031\n",
      "\tgrad: 2.0 4.0 -2.8173885345458984 -1.4086942672729492 -0.7043471336364746\n",
      "\tgrad: 3.0 6.0 2.9260196685791016 0.9753398895263672 0.32511329650878906\n",
      "Epoch: 11 0.02642466314136982\n",
      "\tgrad: 1.0 2.0 0.028025150299072266 0.028025150299072266 0.028025150299072266\n",
      "\tgrad: 2.0 4.0 -2.768169403076172 -1.384084701538086 -0.692042350769043\n",
      "\tgrad: 3.0 6.0 2.891498565673828 0.9638328552246094 0.3212776184082031\n",
      "Epoch: 12 0.025804826989769936\n",
      "\tgrad: 1.0 2.0 0.03969764709472656 0.03969764709472656 0.03969764709472656\n",
      "\tgrad: 2.0 4.0 -2.732961654663086 -1.366480827331543 -0.6832404136657715\n",
      "\tgrad: 3.0 6.0 2.8243446350097656 0.9414482116699219 0.3138160705566406\n",
      "Epoch: 13 0.02462013065814972\n",
      "\tgrad: 1.0 2.0 0.051377296447753906 0.051377296447753906 0.051377296447753906\n",
      "\tgrad: 2.0 4.0 -2.6934165954589844 -1.3467082977294922 -0.6733541488647461\n",
      "\tgrad: 3.0 6.0 2.7755842208862305 0.9251947402954102 0.3083982467651367\n",
      "Epoch: 14 0.023777369409799576\n",
      "\tgrad: 1.0 2.0 0.062380313873291016 0.062380313873291016 0.062380313873291016\n",
      "\tgrad: 2.0 4.0 -2.6580047607421875 -1.3290023803710938 -0.6645011901855469\n",
      "\tgrad: 3.0 6.0 2.7212963104248047 0.9070987701416016 0.3023662567138672\n",
      "Epoch: 15 0.0228563379496336\n",
      "\tgrad: 1.0 2.0 0.07305240631103516 0.07305240631103516 0.07305240631103516\n",
      "\tgrad: 2.0 4.0 -2.622692108154297 -1.3113460540771484 -0.6556730270385742\n",
      "\tgrad: 3.0 6.0 2.672501564025879 0.890833854675293 0.29694461822509766\n",
      "Epoch: 16 0.022044027224183083\n",
      "\tgrad: 1.0 2.0 0.08325767517089844 0.08325767517089844 0.08325767517089844\n",
      "\tgrad: 2.0 4.0 -2.589275360107422 -1.294637680053711 -0.6473188400268555\n",
      "\tgrad: 3.0 6.0 2.6239728927612305 0.8746576309204102 0.2915525436401367\n",
      "Epoch: 17 0.02125072106719017\n",
      "\tgrad: 1.0 2.0 0.09308338165283203 0.09308338165283203 0.09308338165283203\n",
      "\tgrad: 2.0 4.0 -2.5568485260009766 -1.2784242630004883 -0.6392121315002441\n",
      "\tgrad: 3.0 6.0 2.578036308288574 0.8593454360961914 0.28644847869873047\n",
      "Epoch: 18 0.020513182505965233\n",
      "\tgrad: 1.0 2.0 0.10251188278198242 0.10251188278198242 0.10251188278198242\n",
      "\tgrad: 2.0 4.0 -2.5257606506347656 -1.2628803253173828 -0.6314401626586914\n",
      "\tgrad: 3.0 6.0 2.5334815979003906 0.8444938659667969 0.2814979553222656\n",
      "Epoch: 19 0.019810274243354797\n",
      "\tgrad: 1.0 2.0 0.1115732192993164 0.1115732192993164 0.1115732192993164\n",
      "\tgrad: 2.0 4.0 -2.4957752227783203 -1.2478876113891602 -0.6239438056945801\n",
      "\tgrad: 3.0 6.0 2.490780830383301 0.8302602767944336 0.27675342559814453\n",
      "Epoch: 20 0.019148115068674088\n",
      "\tgrad: 1.0 2.0 0.12027454376220703 0.12027454376220703 0.12027454376220703\n",
      "\tgrad: 2.0 4.0 -2.4669342041015625 -1.2334671020507812 -0.6167335510253906\n",
      "\tgrad: 3.0 6.0 2.4496335983276367 0.8165445327758789 0.27218151092529297\n",
      "Epoch: 21 0.018520694226026535\n",
      "\tgrad: 1.0 2.0 0.12863397598266602 0.12863397598266602 0.12863397598266602\n",
      "\tgrad: 2.0 4.0 -2.4391613006591797 -1.2195806503295898 -0.6097903251647949\n",
      "\tgrad: 3.0 6.0 2.4100828170776367 0.8033609390258789 0.26778697967529297\n",
      "Epoch: 22 0.017927465960383415\n",
      "\tgrad: 1.0 2.0 0.13666200637817383 0.13666200637817383 0.13666200637817383\n",
      "\tgrad: 2.0 4.0 -2.4124298095703125 -1.2062149047851562 -0.6031074523925781\n",
      "\tgrad: 3.0 6.0 2.3719911575317383 0.7906637191772461 0.26355457305908203\n",
      "Epoch: 23 0.01736525259912014\n",
      "\tgrad: 1.0 2.0 0.14437294006347656 0.14437294006347656 0.14437294006347656\n",
      "\tgrad: 2.0 4.0 -2.386688232421875 -1.1933441162109375 -0.5966720581054688\n",
      "\tgrad: 3.0 6.0 2.335367202758789 0.7784557342529297 0.25948524475097656\n",
      "Epoch: 24 0.016833148896694183\n",
      "\tgrad: 1.0 2.0 0.1517786979675293 0.1517786979675293 0.1517786979675293\n",
      "\tgrad: 2.0 4.0 -2.3619022369384766 -1.1809511184692383 -0.5904755592346191\n",
      "\tgrad: 3.0 6.0 2.30013370513916 0.7667112350463867 0.2555704116821289\n",
      "Epoch: 25 0.01632905937731266\n",
      "\tgrad: 1.0 2.0 0.1588902473449707 0.1588902473449707 0.1588902473449707\n",
      "\tgrad: 2.0 4.0 -2.338043212890625 -1.1690216064453125 -0.5845108032226562\n",
      "\tgrad: 3.0 6.0 2.2661962509155273 0.7553987503051758 0.2517995834350586\n",
      "Epoch: 26 0.01585075818002224\n",
      "\tgrad: 1.0 2.0 0.16572046279907227 0.16572046279907227 0.16572046279907227\n",
      "\tgrad: 2.0 4.0 -2.3150596618652344 -1.1575298309326172 -0.5787649154663086\n",
      "\tgrad: 3.0 6.0 2.233572006225586 0.7445240020751953 0.24817466735839844\n",
      "Epoch: 27 0.015397666022181511\n",
      "\tgrad: 1.0 2.0 0.17227888107299805 0.17227888107299805 0.17227888107299805\n",
      "\tgrad: 2.0 4.0 -2.2929306030273438 -1.1464653015136719 -0.5732326507568359\n",
      "\tgrad: 3.0 6.0 2.202157974243164 0.7340526580810547 0.24468421936035156\n",
      "Epoch: 28 0.014967591501772404\n",
      "\tgrad: 1.0 2.0 0.17857694625854492 0.17857694625854492 0.17857694625854492\n",
      "\tgrad: 2.0 4.0 -2.271617889404297 -1.1358089447021484 -0.5679044723510742\n",
      "\tgrad: 3.0 6.0 2.171945571899414 0.7239818572998047 0.24132728576660156\n",
      "Epoch: 29 0.014559715054929256\n",
      "\tgrad: 1.0 2.0 0.18462371826171875 0.18462371826171875 0.18462371826171875\n",
      "\tgrad: 2.0 4.0 -2.2510948181152344 -1.1255474090576172 -0.5627737045288086\n",
      "\tgrad: 3.0 6.0 2.142857551574707 0.7142858505249023 0.23809528350830078\n",
      "Epoch: 30 0.014172340743243694\n",
      "\tgrad: 1.0 2.0 0.1904296875 0.1904296875 0.1904296875\n",
      "\tgrad: 2.0 4.0 -2.231321334838867 -1.1156606674194336 -0.5578303337097168\n",
      "\tgrad: 3.0 6.0 2.1148509979248047 0.7049503326416016 0.2349834442138672\n",
      "Epoch: 31 0.013804304413497448\n",
      "\tgrad: 1.0 2.0 0.19600486755371094 0.19600486755371094 0.19600486755371094\n",
      "\tgrad: 2.0 4.0 -2.2122726440429688 -1.1061363220214844 -0.5530681610107422\n",
      "\tgrad: 3.0 6.0 2.087925910949707 0.6959753036499023 0.23199176788330078\n",
      "Epoch: 32 0.013455045409500599\n",
      "\tgrad: 1.0 2.0 0.2013559341430664 0.2013559341430664 0.2013559341430664\n",
      "\tgrad: 2.0 4.0 -2.193929672241211 -1.0969648361206055 -0.5484824180603027\n",
      "\tgrad: 3.0 6.0 2.061979293823242 0.6873264312744141 0.2291088104248047\n",
      "Epoch: 33 0.013122711330652237\n",
      "\tgrad: 1.0 2.0 0.20649433135986328 0.20649433135986328 0.20649433135986328\n",
      "\tgrad: 2.0 4.0 -2.176250457763672 -1.088125228881836 -0.544062614440918\n",
      "\tgrad: 3.0 6.0 2.037019729614258 0.6790065765380859 0.2263355255126953\n",
      "Epoch: 34 0.01280694268643856\n",
      "\tgrad: 1.0 2.0 0.21142578125 0.21142578125 0.21142578125\n",
      "\tgrad: 2.0 4.0 -2.1592159271240234 -1.0796079635620117 -0.5398039817810059\n",
      "\tgrad: 3.0 6.0 2.0130043029785156 0.6710014343261719 0.22366714477539062\n",
      "Epoch: 35 0.012506747618317604\n",
      "\tgrad: 1.0 2.0 0.21615982055664062 0.21615982055664062 0.21615982055664062\n",
      "\tgrad: 2.0 4.0 -2.142810821533203 -1.0714054107666016 -0.5357027053833008\n",
      "\tgrad: 3.0 6.0 1.9898557662963867 0.6632852554321289 0.22109508514404297\n",
      "Epoch: 36 0.012220758944749832\n",
      "\tgrad: 1.0 2.0 0.2207036018371582 0.2207036018371582 0.2207036018371582\n",
      "\tgrad: 2.0 4.0 -2.1269912719726562 -1.0634956359863281 -0.5317478179931641\n",
      "\tgrad: 3.0 6.0 1.967599868774414 0.6558666229248047 0.21862220764160156\n",
      "Epoch: 37 0.01194891706109047\n",
      "\tgrad: 1.0 2.0 0.22506427764892578 0.22506427764892578 0.22506427764892578\n",
      "\tgrad: 2.0 4.0 -2.1117515563964844 -1.0558757781982422 -0.5279378890991211\n",
      "\tgrad: 3.0 6.0 1.9461593627929688 0.6487197875976562 0.21623992919921875\n",
      "Epoch: 38 0.011689926497638226\n",
      "\tgrad: 1.0 2.0 0.2292490005493164 0.2292490005493164 0.2292490005493164\n",
      "\tgrad: 2.0 4.0 -2.0970611572265625 -1.0485305786132812 -0.5242652893066406\n",
      "\tgrad: 3.0 6.0 1.9255084991455078 0.6418361663818359 0.2139453887939453\n",
      "Epoch: 39 0.01144315768033266\n",
      "\tgrad: 1.0 2.0 0.23326539993286133 0.23326539993286133 0.23326539993286133\n",
      "\tgrad: 2.0 4.0 -2.082897186279297 -1.0414485931396484 -0.5207242965698242\n",
      "\tgrad: 3.0 6.0 1.9056644439697266 0.6352214813232422 0.21174049377441406\n",
      "Epoch: 40 0.011208509095013142\n",
      "\tgrad: 1.0 2.0 0.23711872100830078 0.23711872100830078 0.23711872100830078\n",
      "\tgrad: 2.0 4.0 -2.0692520141601562 -1.0346260070800781 -0.5173130035400391\n",
      "\tgrad: 3.0 6.0 1.8864898681640625 0.6288299560546875 0.2096099853515625\n",
      "Epoch: 41 0.0109840864315629\n",
      "\tgrad: 1.0 2.0 0.24081659317016602 0.24081659317016602 0.24081659317016602\n",
      "\tgrad: 2.0 4.0 -2.0560836791992188 -1.0280418395996094 -0.5140209197998047\n",
      "\tgrad: 3.0 6.0 1.8680963516235352 0.6226987838745117 0.2075662612915039\n",
      "Epoch: 42 0.010770938359200954\n",
      "\tgrad: 1.0 2.0 0.24436283111572266 0.24436283111572266 0.24436283111572266\n",
      "\tgrad: 2.0 4.0 -2.0433998107910156 -1.0216999053955078 -0.5108499526977539\n",
      "\tgrad: 3.0 6.0 1.850320816040039 0.6167736053466797 0.20559120178222656\n",
      "Epoch: 43 0.010566935874521732\n",
      "\tgrad: 1.0 2.0 0.24776697158813477 0.24776697158813477 0.24776697158813477\n",
      "\tgrad: 2.0 4.0 -2.0311546325683594 -1.0155773162841797 -0.5077886581420898\n",
      "\tgrad: 3.0 6.0 1.8332405090332031 0.6110801696777344 0.20369338989257812\n",
      "Epoch: 44 0.010372749529778957\n",
      "\tgrad: 1.0 2.0 0.25103092193603516 0.25103092193603516 0.25103092193603516\n",
      "\tgrad: 2.0 4.0 -2.0193519592285156 -1.0096759796142578 -0.5048379898071289\n",
      "\tgrad: 3.0 6.0 1.816786766052246 0.605595588684082 0.20186519622802734\n",
      "Epoch: 45 0.010187389329075813\n",
      "\tgrad: 1.0 2.0 0.2541618347167969 0.2541618347167969 0.2541618347167969\n",
      "\tgrad: 2.0 4.0 -2.0079689025878906 -1.0039844512939453 -0.5019922256469727\n",
      "\tgrad: 3.0 6.0 1.8009252548217773 0.6003084182739258 0.2001028060913086\n",
      "Epoch: 46 0.010010283440351486\n",
      "\tgrad: 1.0 2.0 0.25716400146484375 0.25716400146484375 0.25716400146484375\n",
      "\tgrad: 2.0 4.0 -1.9969863891601562 -0.9984931945800781 -0.49924659729003906\n",
      "\tgrad: 3.0 6.0 1.785630226135254 0.595210075378418 0.19840335845947266\n",
      "Epoch: 47 0.00984097272157669\n",
      "\tgrad: 1.0 2.0 0.2600440979003906 0.2600440979003906 0.2600440979003906\n",
      "\tgrad: 2.0 4.0 -1.9863853454589844 -0.9931926727294922 -0.4965963363647461\n",
      "\tgrad: 3.0 6.0 1.7709360122680664 0.5903120040893555 0.19677066802978516\n",
      "Epoch: 48 0.009679674170911312\n",
      "\tgrad: 1.0 2.0 0.2628040313720703 0.2628040313720703 0.2628040313720703\n",
      "\tgrad: 2.0 4.0 -1.976165771484375 -0.9880828857421875 -0.49404144287109375\n",
      "\tgrad: 3.0 6.0 1.7567567825317383 0.5855855941772461 0.19519519805908203\n",
      "Epoch: 49 0.009525291621685028\n",
      "\tgrad: 1.0 2.0 0.26545143127441406 0.26545143127441406 0.26545143127441406\n",
      "\tgrad: 2.0 4.0 -1.9663009643554688 -0.9831504821777344 -0.4915752410888672\n",
      "\tgrad: 3.0 6.0 1.7430925369262695 0.5810308456420898 0.19367694854736328\n",
      "Epoch: 50 0.00937769003212452\n",
      "\tgrad: 1.0 2.0 0.2679886817932129 0.2679886817932129 0.2679886817932129\n",
      "\tgrad: 2.0 4.0 -1.9567756652832031 -0.9783878326416016 -0.4891939163208008\n",
      "\tgrad: 3.0 6.0 1.7299346923828125 0.5766448974609375 0.1922149658203125\n",
      "Epoch: 51 0.009236648678779602\n",
      "\tgrad: 1.0 2.0 0.27042055130004883 0.27042055130004883 0.27042055130004883\n",
      "\tgrad: 2.0 4.0 -1.9475860595703125 -0.9737930297851562 -0.4868965148925781\n",
      "\tgrad: 3.0 6.0 1.717240333557129 0.572413444519043 0.19080448150634766\n",
      "Epoch: 52 0.00910158734768629\n",
      "\tgrad: 1.0 2.0 0.2727518081665039 0.2727518081665039 0.2727518081665039\n",
      "\tgrad: 2.0 4.0 -1.9387092590332031 -0.9693546295166016 -0.4846773147583008\n",
      "\tgrad: 3.0 6.0 1.705026626586914 0.5683422088623047 0.18944740295410156\n",
      "Epoch: 53 0.00897257961332798\n",
      "\tgrad: 1.0 2.0 0.27498531341552734 0.27498531341552734 0.27498531341552734\n",
      "\tgrad: 2.0 4.0 -1.9301414489746094 -0.9650707244873047 -0.48253536224365234\n",
      "\tgrad: 3.0 6.0 1.6932334899902344 0.5644111633300781 0.18813705444335938\n",
      "Epoch: 54 0.008848887868225574\n",
      "\tgrad: 1.0 2.0 0.27712535858154297 0.27712535858154297 0.27712535858154297\n",
      "\tgrad: 2.0 4.0 -1.9218635559082031 -0.9609317779541016 -0.4804658889770508\n",
      "\tgrad: 3.0 6.0 1.6818780899047852 0.5606260299682617 0.1868753433227539\n",
      "Epoch: 55 0.008730598725378513\n",
      "\tgrad: 1.0 2.0 0.2791757583618164 0.2791757583618164 0.2791757583618164\n",
      "\tgrad: 2.0 4.0 -1.9138717651367188 -0.9569358825683594 -0.4784679412841797\n",
      "\tgrad: 3.0 6.0 1.6709346771240234 0.5569782257080078 0.18565940856933594\n",
      "Epoch: 56 0.00861735362559557\n",
      "\tgrad: 1.0 2.0 0.2811393737792969 0.2811393737792969 0.2811393737792969\n",
      "\tgrad: 2.0 4.0 -1.9061508178710938 -0.9530754089355469 -0.47653770446777344\n",
      "\tgrad: 3.0 6.0 1.6603689193725586 0.5534563064575195 0.18448543548583984\n",
      "Epoch: 57 0.008508718572556973\n",
      "\tgrad: 1.0 2.0 0.28302001953125 0.28302001953125 0.28302001953125\n",
      "\tgrad: 2.0 4.0 -1.8986892700195312 -0.9493446350097656 -0.4746723175048828\n",
      "\tgrad: 3.0 6.0 1.6501893997192383 0.5500631332397461 0.18335437774658203\n",
      "Epoch: 58 0.008404706604778767\n",
      "\tgrad: 1.0 2.0 0.284820556640625 0.284820556640625 0.284820556640625\n",
      "\tgrad: 2.0 4.0 -1.8914775848388672 -0.9457387924194336 -0.4728693962097168\n",
      "\tgrad: 3.0 6.0 1.6403875350952148 0.5467958450317383 0.1822652816772461\n",
      "Epoch: 59 0.008305158466100693\n",
      "\tgrad: 1.0 2.0 0.2865438461303711 0.2865438461303711 0.2865438461303711\n",
      "\tgrad: 2.0 4.0 -1.8845138549804688 -0.9422569274902344 -0.4711284637451172\n",
      "\tgrad: 3.0 6.0 1.630894660949707 0.5436315536499023 0.18121051788330078\n",
      "Epoch: 60 0.00820931326597929\n",
      "\tgrad: 1.0 2.0 0.2881946563720703 0.2881946563720703 0.2881946563720703\n",
      "\tgrad: 2.0 4.0 -1.8777713775634766 -0.9388856887817383 -0.46944284439086914\n",
      "\tgrad: 3.0 6.0 1.621779441833496 0.540593147277832 0.18019771575927734\n",
      "Epoch: 61 0.008117804303765297\n",
      "\tgrad: 1.0 2.0 0.28977346420288086 0.28977346420288086 0.28977346420288086\n",
      "\tgrad: 2.0 4.0 -1.8712615966796875 -0.9356307983398438 -0.4678153991699219\n",
      "\tgrad: 3.0 6.0 1.6129646301269531 0.5376548767089844 0.17921829223632812\n",
      "Epoch: 62 0.008029798977077007\n",
      "\tgrad: 1.0 2.0 0.29128456115722656 0.29128456115722656 0.29128456115722656\n",
      "\tgrad: 2.0 4.0 -1.8649616241455078 -0.9324808120727539 -0.46624040603637695\n",
      "\tgrad: 3.0 6.0 1.6044673919677734 0.5348224639892578 0.17827415466308594\n",
      "Epoch: 63 0.007945418357849121\n",
      "\tgrad: 1.0 2.0 0.29272985458374023 0.29272985458374023 0.29272985458374023\n",
      "\tgrad: 2.0 4.0 -1.8588676452636719 -0.9294338226318359 -0.46471691131591797\n",
      "\tgrad: 3.0 6.0 1.5962448120117188 0.5320816040039062 0.17736053466796875\n",
      "Epoch: 64 0.007864190265536308\n",
      "\tgrad: 1.0 2.0 0.2941126823425293 0.2941126823425293 0.2941126823425293\n",
      "\tgrad: 2.0 4.0 -1.8529701232910156 -0.9264850616455078 -0.4632425308227539\n",
      "\tgrad: 3.0 6.0 1.5883655548095703 0.5294551849365234 0.1764850616455078\n",
      "Epoch: 65 0.007786744274199009\n",
      "\tgrad: 1.0 2.0 0.29543399810791016 0.29543399810791016 0.29543399810791016\n",
      "\tgrad: 2.0 4.0 -1.8472747802734375 -0.9236373901367188 -0.4618186950683594\n",
      "\tgrad: 3.0 6.0 1.5806922912597656 0.5268974304199219 0.17563247680664062\n",
      "Epoch: 66 0.007711691781878471\n",
      "\tgrad: 1.0 2.0 0.29669761657714844 0.29669761657714844 0.29669761657714844\n",
      "\tgrad: 2.0 4.0 -1.841745376586914 -0.920872688293457 -0.4604363441467285\n",
      "\tgrad: 3.0 6.0 1.5733451843261719 0.5244483947753906 0.17481613159179688\n",
      "Epoch: 67 0.007640169933438301\n",
      "\tgrad: 1.0 2.0 0.29790449142456055 0.29790449142456055 0.29790449142456055\n",
      "\tgrad: 2.0 4.0 -1.8364067077636719 -0.9182033538818359 -0.45910167694091797\n",
      "\tgrad: 3.0 6.0 1.5662040710449219 0.5220680236816406 0.17402267456054688\n",
      "Epoch: 68 0.007570972666144371\n",
      "\tgrad: 1.0 2.0 0.2990589141845703 0.2990589141845703 0.2990589141845703\n",
      "\tgrad: 2.0 4.0 -1.8312263488769531 -0.9156131744384766 -0.4578065872192383\n",
      "\tgrad: 3.0 6.0 1.5593376159667969 0.5197792053222656 0.17325973510742188\n",
      "Epoch: 69 0.007504733745008707\n",
      "\tgrad: 1.0 2.0 0.30016040802001953 0.30016040802001953 0.30016040802001953\n",
      "\tgrad: 2.0 4.0 -1.8262138366699219 -0.9131069183349609 -0.45655345916748047\n",
      "\tgrad: 3.0 6.0 1.552694320678711 0.5175647735595703 0.17252159118652344\n",
      "Epoch: 70 0.007440924644470215\n",
      "\tgrad: 1.0 2.0 0.30121278762817383 0.30121278762817383 0.30121278762817383\n",
      "\tgrad: 2.0 4.0 -1.8213577270507812 -0.9106788635253906 -0.4553394317626953\n",
      "\tgrad: 3.0 6.0 1.5462827682495117 0.5154275894165039 0.17180919647216797\n",
      "Epoch: 71 0.007379599846899509\n",
      "\tgrad: 1.0 2.0 0.3022174835205078 0.3022174835205078 0.3022174835205078\n",
      "\tgrad: 2.0 4.0 -1.8166522979736328 -0.9083261489868164 -0.4541630744934082\n",
      "\tgrad: 3.0 6.0 1.5400772094726562 0.5133590698242188 0.17111968994140625\n",
      "Epoch: 72 0.007320486940443516\n",
      "\tgrad: 1.0 2.0 0.3031759262084961 0.3031759262084961 0.3031759262084961\n",
      "\tgrad: 2.0 4.0 -1.8120880126953125 -0.9060440063476562 -0.4530220031738281\n",
      "\tgrad: 3.0 6.0 1.5340948104858398 0.5113649368286133 0.1704549789428711\n",
      "Epoch: 73 0.007263725157827139\n",
      "\tgrad: 1.0 2.0 0.3040900230407715 0.3040900230407715 0.3040900230407715\n",
      "\tgrad: 2.0 4.0 -1.8076667785644531 -0.9038333892822266 -0.4519166946411133\n",
      "\tgrad: 3.0 6.0 1.5283098220825195 0.5094366073608398 0.16981220245361328\n",
      "Epoch: 74 0.007209045812487602\n",
      "\tgrad: 1.0 2.0 0.304962158203125 0.304962158203125 0.304962158203125\n",
      "\tgrad: 2.0 4.0 -1.8033790588378906 -0.9016895294189453 -0.45084476470947266\n",
      "\tgrad: 3.0 6.0 1.5227222442626953 0.5075740814208984 0.1691913604736328\n",
      "Epoch: 75 0.007156429346650839\n",
      "\tgrad: 1.0 2.0 0.30579280853271484 0.30579280853271484 0.30579280853271484\n",
      "\tgrad: 2.0 4.0 -1.7992210388183594 -0.8996105194091797 -0.44980525970458984\n",
      "\tgrad: 3.0 6.0 1.5172977447509766 0.5057659149169922 0.16858863830566406\n",
      "Epoch: 76 0.007105532102286816\n",
      "\tgrad: 1.0 2.0 0.30658483505249023 0.30658483505249023 0.30658483505249023\n",
      "\tgrad: 2.0 4.0 -1.7951812744140625 -0.8975906372070312 -0.4487953186035156\n",
      "\tgrad: 3.0 6.0 1.5120878219604492 0.5040292739868164 0.16800975799560547\n",
      "Epoch: 77 0.00705681974068284\n",
      "\tgrad: 1.0 2.0 0.3073387145996094 0.3073387145996094 0.3073387145996094\n",
      "\tgrad: 2.0 4.0 -1.791269302368164 -0.895634651184082 -0.447817325592041\n",
      "\tgrad: 3.0 6.0 1.5070152282714844 0.5023384094238281 0.16744613647460938\n",
      "Epoch: 78 0.007009552326053381\n",
      "\tgrad: 1.0 2.0 0.3080568313598633 0.3080568313598633 0.3080568313598633\n",
      "\tgrad: 2.0 4.0 -1.7874603271484375 -0.8937301635742188 -0.4468650817871094\n",
      "\tgrad: 3.0 6.0 1.502131462097168 0.5007104873657227 0.16690349578857422\n",
      "Epoch: 79 0.006964194122701883\n",
      "\tgrad: 1.0 2.0 0.30873966217041016 0.30873966217041016 0.30873966217041016\n",
      "\tgrad: 2.0 4.0 -1.7837696075439453 -0.8918848037719727 -0.44594240188598633\n",
      "\tgrad: 3.0 6.0 1.4973936080932617 0.4991312026977539 0.16637706756591797\n",
      "Epoch: 80 0.006920332089066505\n",
      "\tgrad: 1.0 2.0 0.309389591217041 0.309389591217041 0.309389591217041\n",
      "\tgrad: 2.0 4.0 -1.780181884765625 -0.8900909423828125 -0.44504547119140625\n",
      "\tgrad: 3.0 6.0 1.492818832397461 0.4976062774658203 0.16586875915527344\n",
      "Epoch: 81 0.006878111511468887\n",
      "\tgrad: 1.0 2.0 0.31000614166259766 0.31000614166259766 0.31000614166259766\n",
      "\tgrad: 2.0 4.0 -1.7766971588134766 -0.8883485794067383 -0.44417428970336914\n",
      "\tgrad: 3.0 6.0 1.4883899688720703 0.49612998962402344 0.1653766632080078\n",
      "Epoch: 82 0.006837360095232725\n",
      "\tgrad: 1.0 2.0 0.3105926513671875 0.3105926513671875 0.3105926513671875\n",
      "\tgrad: 2.0 4.0 -1.7733135223388672 -0.8866567611694336 -0.4433283805847168\n",
      "\tgrad: 3.0 6.0 1.4840812683105469 0.4946937561035156 0.16489791870117188\n",
      "Epoch: 83 0.006797831039875746\n",
      "\tgrad: 1.0 2.0 0.31114959716796875 0.31114959716796875 0.31114959716796875\n",
      "\tgrad: 2.0 4.0 -1.7700138092041016 -0.8850069046020508 -0.4425034523010254\n",
      "\tgrad: 3.0 6.0 1.4799528121948242 0.4933176040649414 0.16443920135498047\n",
      "Epoch: 84 0.006760062649846077\n",
      "\tgrad: 1.0 2.0 0.3116769790649414 0.3116769790649414 0.3116769790649414\n",
      "\tgrad: 2.0 4.0 -1.7668170928955078 -0.8834085464477539 -0.44170427322387695\n",
      "\tgrad: 3.0 6.0 1.4759016036987305 0.49196720123291016 0.16398906707763672\n",
      "Epoch: 85 0.006723103579133749\n",
      "\tgrad: 1.0 2.0 0.3121776580810547 0.3121776580810547 0.3121776580810547\n",
      "\tgrad: 2.0 4.0 -1.7636966705322266 -0.8818483352661133 -0.44092416763305664\n",
      "\tgrad: 3.0 6.0 1.4720134735107422 0.49067115783691406 0.1635570526123047\n",
      "Epoch: 86 0.00668772729113698\n",
      "\tgrad: 1.0 2.0 0.3126516342163086 0.3126516342163086 0.3126516342163086\n",
      "\tgrad: 2.0 4.0 -1.7606639862060547 -0.8803319931030273 -0.44016599655151367\n",
      "\tgrad: 3.0 6.0 1.4682197570800781 0.4894065856933594 0.16313552856445312\n",
      "Epoch: 87 0.006653300020843744\n",
      "\tgrad: 1.0 2.0 0.31310081481933594 0.31310081481933594 0.31310081481933594\n",
      "\tgrad: 2.0 4.0 -1.7577095031738281 -0.8788547515869141 -0.43942737579345703\n",
      "\tgrad: 3.0 6.0 1.4645805358886719 0.4881935119628906 0.16273117065429688\n",
      "Epoch: 88 0.0066203586757183075\n",
      "\tgrad: 1.0 2.0 0.3135242462158203 0.3135242462158203 0.3135242462158203\n",
      "\tgrad: 2.0 4.0 -1.754842758178711 -0.8774213790893555 -0.43871068954467773\n",
      "\tgrad: 3.0 6.0 1.4610099792480469 0.4870033264160156 0.16233444213867188\n",
      "Epoch: 89 0.0065881176851689816\n",
      "\tgrad: 1.0 2.0 0.31392478942871094 0.31392478942871094 0.31392478942871094\n",
      "\tgrad: 2.0 4.0 -1.7520370483398438 -0.8760185241699219 -0.43800926208496094\n",
      "\tgrad: 3.0 6.0 1.457585334777832 0.48586177825927734 0.16195392608642578\n",
      "Epoch: 90 0.0065572685562074184\n",
      "\tgrad: 1.0 2.0 0.314302921295166 0.314302921295166 0.314302921295166\n",
      "\tgrad: 2.0 4.0 -1.7493114471435547 -0.8746557235717773 -0.43732786178588867\n",
      "\tgrad: 3.0 6.0 1.4542293548583984 0.4847431182861328 0.16158103942871094\n",
      "Epoch: 91 0.0065271081402897835\n",
      "\tgrad: 1.0 2.0 0.31465959548950195 0.31465959548950195 0.31465959548950195\n",
      "\tgrad: 2.0 4.0 -1.7466468811035156 -0.8733234405517578 -0.4366617202758789\n",
      "\tgrad: 3.0 6.0 1.4509849548339844 0.4836616516113281 0.16122055053710938\n",
      "Epoch: 92 0.00649801641702652\n",
      "\tgrad: 1.0 2.0 0.31499528884887695 0.31499528884887695 0.31499528884887695\n",
      "\tgrad: 2.0 4.0 -1.7440509796142578 -0.8720254898071289 -0.43601274490356445\n",
      "\tgrad: 3.0 6.0 1.4478435516357422 0.48261451721191406 0.1608715057373047\n",
      "Epoch: 93 0.0064699104987084866\n",
      "\tgrad: 1.0 2.0 0.3153109550476074 0.3153109550476074 0.3153109550476074\n",
      "\tgrad: 2.0 4.0 -1.7415199279785156 -0.8707599639892578 -0.4353799819946289\n",
      "\tgrad: 3.0 6.0 1.4447879791259766 0.4815959930419922 0.16053199768066406\n",
      "Epoch: 94 0.006442630663514137\n",
      "\tgrad: 1.0 2.0 0.31560707092285156 0.31560707092285156 0.31560707092285156\n",
      "\tgrad: 2.0 4.0 -1.7390518188476562 -0.8695259094238281 -0.43476295471191406\n",
      "\tgrad: 3.0 6.0 1.4418182373046875 0.4806060791015625 0.1602020263671875\n",
      "Epoch: 95 0.006416172254830599\n",
      "\tgrad: 1.0 2.0 0.3158855438232422 0.3158855438232422 0.3158855438232422\n",
      "\tgrad: 2.0 4.0 -1.7366409301757812 -0.8683204650878906 -0.4341602325439453\n",
      "\tgrad: 3.0 6.0 1.4389429092407227 0.4796476364135742 0.1598825454711914\n",
      "Epoch: 96 0.006390606984496117\n",
      "\tgrad: 1.0 2.0 0.3161449432373047 0.3161449432373047 0.3161449432373047\n",
      "\tgrad: 2.0 4.0 -1.7342891693115234 -0.8671445846557617 -0.43357229232788086\n",
      "\tgrad: 3.0 6.0 1.436136245727539 0.4787120819091797 0.15957069396972656\n",
      "Epoch: 97 0.0063657015562057495\n",
      "\tgrad: 1.0 2.0 0.3163881301879883 0.3163881301879883 0.3163881301879883\n",
      "\tgrad: 2.0 4.0 -1.7319889068603516 -0.8659944534301758 -0.4329972267150879\n",
      "\tgrad: 3.0 6.0 1.4334239959716797 0.47780799865722656 0.1592693328857422\n",
      "Epoch: 98 0.0063416799530386925\n",
      "\tgrad: 1.0 2.0 0.31661415100097656 0.31661415100097656 0.31661415100097656\n",
      "\tgrad: 2.0 4.0 -1.7297439575195312 -0.8648719787597656 -0.4324359893798828\n",
      "\tgrad: 3.0 6.0 1.4307546615600586 0.47691822052001953 0.15897274017333984\n",
      "Epoch: 99 0.00631808303296566\n",
      "Predict(after training) 4 8.544171333312988\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "x_data = [1.0,2.0,3.0]\n",
    "y_data = [2.0,4.0,6.0]\n",
    "\n",
    "w1 = torch.Tensor([1.0])#初始权值\n",
    "w1.requires_grad = True#计算梯度，默认是不计算的\n",
    "w2 = torch.Tensor([1.0])\n",
    "w2.requires_grad = True\n",
    "b = torch.Tensor([1.0])\n",
    "b.requires_grad = True\n",
    "\n",
    "def forward(x):\n",
    "    return w1 * x**2 + w2 * x + b\n",
    "\n",
    "def loss(x,y):#构建计算图\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred-y) **2\n",
    "\n",
    "print('Predict (befortraining)',4,forward(4))\n",
    "\n",
    "for epoch in range(100):\n",
    "    l = loss(1, 2)#为了在for循环之前定义l,以便之后的输出，无实际意义\n",
    "    for x,y in zip(x_data,y_data):\n",
    "        l = loss(x, y)\n",
    "        l.backward()\n",
    "        print('\\tgrad:',x,y,w1.grad.item(),w2.grad.item(),b.grad.item())\n",
    "        w1.data = w1.data - 0.01*w1.grad.data #注意这里的grad是一个tensor，所以要取他的data\n",
    "        w2.data = w2.data - 0.01 * w2.grad.data\n",
    "        b.data = b.data - 0.01 * b.grad.data\n",
    "        w1.grad.data.zero_() #释放之前计算的梯度\n",
    "        w2.grad.data.zero_()\n",
    "        b.grad.data.zero_()\n",
    "    print('Epoch:',epoch,l.item())\n",
    "\n",
    "print('Predict(after training)',4,forward(4).item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x29a956c75c0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "func = lambda x:w1.item()*(x**2)+w2.item()*x+b.item()\n",
    "t_list = np.array([1,2,3,4])\n",
    "plt.plot(t_list, func(t_list))\n",
    "plt.scatter(1,2)\n",
    "plt.scatter(2,4)\n",
    "plt.scatter(3,6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:YOLO_ATTACK]",
   "language": "python",
   "name": "conda-env-YOLO_ATTACK-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
