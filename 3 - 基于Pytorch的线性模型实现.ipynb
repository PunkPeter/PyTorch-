{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Fashion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 准备数据集\n",
    "* 用Class定义模型（继承自nn.Module)\n",
    "* 构建loss和optimizer（用Pytorch API）\n",
    "* Training cycle（前馈forward，反馈backward，更新update）\n",
    "\n",
    "注：mini-batch风格，计算图构建用矩阵运算（广播）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# prepare dataset\n",
    "# x,y是矩阵，3行1列 也就是说总共有3个(x,y)数据，每个数据只有1个特征(x均为1维向量，y是标签)\n",
    "x_data = torch.tensor([[1.0], [2.0], [3.0]])\n",
    "y_data = torch.tensor([[2.0], [4.0], [6.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、设计模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(torch.nn.Module):\n",
    "    # 构造任何模型都是用这个模板，继承torch.nn.Module\n",
    "    def __init__(self):\n",
    "        ''' 必须要有这个方法，即初始化对象用的构造函数 '''\n",
    "        super(LinearModel, self).__init__() # 调用父类的构造\n",
    "        self.linear = torch.nn.Linear(1, 1) # 构造Linear对象\n",
    "        '''\n",
    "            torch.nn.Linear对象包含weight和bias两个成员Tensor\n",
    "            即一个线性单元：类似函数可callable，用于完成【.w再+b】的操作\n",
    "            这两个Tensor显然也是可以反向传播求梯度的\n",
    "        '''\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ''' \n",
    "        必须要重写forward \n",
    "        Module有__call__魔法方法，使得类的实例可以像函数一样使用\n",
    "        该方法就是调用forward(self)，所以我们在自己的模型中必须重写forward\n",
    "        因此：model = LinearModel()实例化后时callable的，即可以直接使用model(x)用于模型推理\n",
    "        '''\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "    \n",
    "    # 注：重写计算模块时，可以继承Functions类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Linear in module torch.nn.modules.linear:\n",
      "\n",
      "class Linear(torch.nn.modules.module.Module)\n",
      " |  Applies a linear transformation to the incoming data: :math:`y = Ax + b`\n",
      " |  \n",
      " |  Args:\n",
      " |      in_features: size of each input sample\n",
      " |      out_features: size of each output sample\n",
      " |      bias: If set to False, the layer will not learn an additive bias.\n",
      " |          Default: ``True``\n",
      " |  \n",
      " |  Shape:\n",
      " |      - Input: :math:`(N, *, in\\_features)` where :math:`*` means any number of\n",
      " |        additional dimensions\n",
      " |      - Output: :math:`(N, *, out\\_features)` where all but the last dimension\n",
      " |        are the same shape as the input.\n",
      " |  \n",
      " |  Attributes:\n",
      " |      weight: the learnable weights of the module of shape\n",
      " |          `(out_features x in_features)`\n",
      " |      bias:   the learnable bias of the module of shape `(out_features)`\n",
      " |  \n",
      " |  Examples::\n",
      " |  \n",
      " |      >>> m = nn.Linear(20, 30)\n",
      " |      >>> input = torch.randn(128, 20)\n",
      " |      >>> output = m(input)\n",
      " |      >>> print(output.size())\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Linear\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, in_features, out_features, bias=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  extra_repr(self)\n",
      " |      Set the extra representation of the module\n",
      " |      \n",
      " |      To print customized extra information, you should reimplement\n",
      " |      this method in your own modules. Both single-line and multi-line\n",
      " |      strings are acceptable.\n",
      " |  \n",
      " |  forward(self, input)\n",
      " |      Defines the computation performed at every call.\n",
      " |      \n",
      " |      Should be overridden by all subclasses.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Although the recipe for forward pass needs to be defined within\n",
      " |          this function, one should call the :class:`Module` instance afterwards\n",
      " |          instead of this since the former takes care of running the\n",
      " |          registered hooks while the latter silently ignores them.\n",
      " |  \n",
      " |  reset_parameters(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__(self, *input, **kwargs)\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      __dir__() -> list\n",
      " |      default dir() implementation\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name, module)\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          parameter (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self, fn)\n",
      " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`torch-nn-init`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> def init_weights(m):\n",
      " |                  print(m)\n",
      " |                  if type(m) == nn.Linear:\n",
      " |                      m.weight.data.fill_(1.0)\n",
      " |                      print(m.weight)\n",
      " |      \n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |  \n",
      " |  children(self)\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  cpu(self)\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self, device=None)\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self)\n",
      " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self)\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |  \n",
      " |  float(self)\n",
      " |      Casts all floating point parameters and buffers to float datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  half(self)\n",
      " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict, strict=True)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          state_dict (dict): a dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool, optional): whether to strictly enforce that the keys\n",
      " |              in :attr:`state_dict` match the keys returned by this module's\n",
      " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      " |  \n",
      " |  modules(self)\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          )\n",
      " |          1 -> Linear (2 -> 2)\n",
      " |  \n",
      " |  named_children(self)\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo=None, prefix='')\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> ('', Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear (2 -> 2))\n",
      " |  \n",
      " |  named_parameters(self, memo=None, prefix='')\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>    if name in ['bias']:\n",
      " |          >>>        print(param.size())\n",
      " |  \n",
      " |  parameters(self)\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param.data), param.size())\n",
      " |          <class 'torch.FloatTensor'> (20L,)\n",
      " |          <class 'torch.FloatTensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook)\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to module\n",
      " |      inputs are computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> Tensor or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` may be tuples if the\n",
      " |      module has multiple inputs or outputs. The hook should not modify its\n",
      " |      arguments, but it can optionally return a new gradient with respect to\n",
      " |      input that will be used in place of :attr:`grad_input` in subsequent\n",
      " |      computations.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_buffer(self, name, tensor)\n",
      " |      Adds a persistent buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the persistent state.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor): buffer to be registered.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook)\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input, output) -> None\n",
      " |      \n",
      " |      The hook should not modify the input or output.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook)\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input) -> None\n",
      " |      \n",
      " |      The hook should not modify the input.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_parameter(self, name, param)\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          parameter (Parameter): parameter to be added to the module.\n",
      " |  \n",
      " |  share_memory(self)\n",
      " |  \n",
      " |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      " |      Returns a dictionary containing a whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  to(self, *args, **kwargs)\n",
      " |      Moves and/or casts the parameters and buffers.\n",
      " |      \n",
      " |      This can be called as\n",
      " |      \n",
      " |      .. function:: to(device)\n",
      " |      \n",
      " |      .. function:: to(dtype)\n",
      " |      \n",
      " |      .. function:: to(device, dtype)\n",
      " |      \n",
      " |      It has similar signature as :meth:`torch.Tensor.to`, but does not take\n",
      " |      a Tensor and only takes in floating point :attr:`dtype` s. In\n",
      " |      particular, this method will only cast the floating point parameters and\n",
      " |      buffers to :attr:`dtype`. It will still move the integral parameters and\n",
      " |      buffers to :attr:`device`, if that is given. See below for examples.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): the desired device of the parameters\n",
      " |              and buffers in this module\n",
      " |          dtype (:class:`torch.dtype`): the desired floating point type of\n",
      " |              the floating point parameters and buffers in this module\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]])\n",
      " |          >>> linear.to(torch.double)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      " |          >>> linear.to(gpu1, dtype=torch.half)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      " |          >>> cpu = torch.device(\"cpu\")\n",
      " |          >>> linear.to(cpu)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      " |  \n",
      " |  train(self, mode=True)\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self, dst_type)\n",
      " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self)\n",
      " |      Sets gradients of all model parameters to zero.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n |  Args:\\n |      in_features: size of each input sample 输入维数\\n |      out_features: size of each output sample 输出维数\\n |      bias: If set to False, the layer will not learn an additive bias. 是否含有偏置\\n |          Default: ``True``\\n'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "help(torch.nn.Linear)\n",
    "'''\n",
    " |  Args:\n",
    " |      in_features: size of each input sample 输入维数\n",
    " |      out_features: size of each output sample 输出维数\n",
    " |      bias: If set to False, the layer will not learn an additive bias. 是否含有偏置\n",
    " |          Default: ``True``\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModel() # 实例化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、构建loss和optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss(size_average=True) \n",
    "# 还有一个reduce参数表示维度是否下降，一般不考虑\n",
    "# 这里就是预测向量y hat和实际向量y之间求均方误差，结果是标量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class SGD in module torch.optim.sgd:\n",
      "\n",
      "class SGD(torch.optim.optimizer.Optimizer)\n",
      " |  Implements stochastic gradient descent (optionally with momentum).\n",
      " |  \n",
      " |  Nesterov momentum is based on the formula from\n",
      " |  `On the importance of initialization and momentum in deep learning`__.\n",
      " |  \n",
      " |  Args:\n",
      " |      params (iterable): iterable of parameters to optimize or dicts defining\n",
      " |          parameter groups\n",
      " |      lr (float): learning rate\n",
      " |      momentum (float, optional): momentum factor (default: 0)\n",
      " |      weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
      " |      dampening (float, optional): dampening for momentum (default: 0)\n",
      " |      nesterov (bool, optional): enables Nesterov momentum (default: False)\n",
      " |  \n",
      " |  Example:\n",
      " |      >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
      " |      >>> optimizer.zero_grad()\n",
      " |      >>> loss_fn(model(input), target).backward()\n",
      " |      >>> optimizer.step()\n",
      " |  \n",
      " |  __ http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf\n",
      " |  \n",
      " |  .. note::\n",
      " |      The implementation of SGD with Momentum/Nesterov subtly differs from\n",
      " |      Sutskever et. al. and implementations in some other frameworks.\n",
      " |  \n",
      " |      Considering the specific case of Momentum, the update can be written as\n",
      " |  \n",
      " |      .. math::\n",
      " |                v = \\rho * v + g \\\\\n",
      " |                p = p - lr * v\n",
      " |  \n",
      " |      where p, g, v and :math:`\\rho` denote the parameters, gradient,\n",
      " |      velocity, and momentum respectively.\n",
      " |  \n",
      " |      This is in contrast to Sutskever et. al. and\n",
      " |      other frameworks which employ an update of the form\n",
      " |  \n",
      " |      .. math::\n",
      " |           v = \\rho * v + lr * g \\\\\n",
      " |           p = p - v\n",
      " |  \n",
      " |      The Nesterov version is analogously modified.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      SGD\n",
      " |      torch.optim.optimizer.Optimizer\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, params, lr=<object object at 0x00000144B6C8A3B0>, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  step(self, closure=None)\n",
      " |      Performs a single optimization step.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          closure (callable, optional): A closure that reevaluates the model\n",
      " |              and returns the loss.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.optim.optimizer.Optimizer:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  add_param_group(self, param_group)\n",
      " |      Add a param group to the :class:`Optimizer` s `param_groups`.\n",
      " |      \n",
      " |      This can be useful when fine tuning a pre-trained network as frozen layers can be made\n",
      " |      trainable and added to the :class:`Optimizer` as training progresses.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          param_group (dict): Specifies what Tensors should be optimized along with group\n",
      " |          specific optimization options.\n",
      " |  \n",
      " |  load_state_dict(self, state_dict)\n",
      " |      Loads the optimizer state.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          state_dict (dict): optimizer state. Should be an object returned\n",
      " |              from a call to :meth:`state_dict`.\n",
      " |  \n",
      " |  state_dict(self)\n",
      " |      Returns the state of the optimizer as a :class:`dict`.\n",
      " |      \n",
      " |      It contains two entries:\n",
      " |      \n",
      " |      * state - a dict holding current optimization state. Its content\n",
      " |          differs between optimizer classes.\n",
      " |      * param_groups - a dict containing all parameter groups\n",
      " |  \n",
      " |  zero_grad(self)\n",
      " |      Clears the gradients of all optimized :class:`torch.Tensor` s.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.optim.optimizer.Optimizer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n |  Args:\\n |      params (iterable): iterable of parameters to optimize or dicts defining\\n |          parameter groups 意义在于将含有weight权重成员的模型参数加入优化器最终要训练的集合中\\n                             这些参数会在backward中梯度下降！！！！\\n |      lr (float): learning rate 学习率\\n |      momentum (float, optional): momentum factor (default: 0)\\n |      weight_decay (float, optional): weight decay (L2 penalty) (default: 0) L2正则化\\n |      dampening (float, optional): dampening for momentum (default: 0)\\n |      nesterov (bool, optional): enables Nesterov momentum (default: False)\\n'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "help(torch.optim.SGD)\n",
    "'''\n",
    " |  Args:\n",
    " |      params (iterable): iterable of parameters to optimize or dicts defining\n",
    " |          parameter groups 意义在于将含有weight权重成员的模型参数加入优化器最终要训练的集合中\n",
    "                             这些参数会在backward中梯度下降！！！！\n",
    " |      lr (float): learning rate 学习率\n",
    " |      momentum (float, optional): momentum factor (default: 0)\n",
    " |      weight_decay (float, optional): weight decay (L2 penalty) (default: 0) L2正则化\n",
    " |      dampening (float, optional): dampening for momentum (default: 0)\n",
    " |      nesterov (bool, optional): enables Nesterov momentum (default: False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4、Training cycle: 前馈（y_pred和loss），（清零），反馈，更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(1.00000e-07 *\n",
      "       5.7372)\n",
      "1 tensor(1.00000e-07 *\n",
      "       5.6990)\n",
      "2 tensor(1.00000e-07 *\n",
      "       5.6655)\n",
      "3 tensor(1.00000e-07 *\n",
      "       5.6339)\n",
      "4 tensor(1.00000e-07 *\n",
      "       5.6010)\n",
      "5 tensor(1.00000e-07 *\n",
      "       5.5713)\n",
      "6 tensor(1.00000e-07 *\n",
      "       5.5410)\n",
      "7 tensor(1.00000e-07 *\n",
      "       5.5128)\n",
      "8 tensor(1.00000e-07 *\n",
      "       5.4847)\n",
      "9 tensor(1.00000e-07 *\n",
      "       5.4565)\n",
      "10 tensor(1.00000e-07 *\n",
      "       5.4294)\n",
      "11 tensor(1.00000e-07 *\n",
      "       5.4025)\n",
      "12 tensor(1.00000e-07 *\n",
      "       5.3762)\n",
      "13 tensor(1.00000e-07 *\n",
      "       5.3483)\n",
      "14 tensor(1.00000e-07 *\n",
      "       5.3248)\n",
      "15 tensor(1.00000e-07 *\n",
      "       5.2959)\n",
      "16 tensor(1.00000e-07 *\n",
      "       5.2724)\n",
      "17 tensor(1.00000e-07 *\n",
      "       5.2461)\n",
      "18 tensor(1.00000e-07 *\n",
      "       5.2198)\n",
      "19 tensor(1.00000e-07 *\n",
      "       5.1954)\n",
      "20 tensor(1.00000e-07 *\n",
      "       5.1711)\n",
      "21 tensor(1.00000e-07 *\n",
      "       5.1447)\n",
      "22 tensor(1.00000e-07 *\n",
      "       5.1211)\n",
      "23 tensor(1.00000e-07 *\n",
      "       5.0975)\n",
      "24 tensor(1.00000e-07 *\n",
      "       5.0702)\n",
      "25 tensor(1.00000e-07 *\n",
      "       5.0484)\n",
      "26 tensor(1.00000e-07 *\n",
      "       5.0243)\n",
      "27 tensor(1.00000e-07 *\n",
      "       4.9996)\n",
      "28 tensor(1.00000e-07 *\n",
      "       4.9761)\n",
      "29 tensor(1.00000e-07 *\n",
      "       4.9515)\n",
      "30 tensor(1.00000e-07 *\n",
      "       4.9281)\n",
      "31 tensor(1.00000e-07 *\n",
      "       4.9036)\n",
      "32 tensor(1.00000e-07 *\n",
      "       4.8792)\n",
      "33 tensor(1.00000e-07 *\n",
      "       4.8577)\n",
      "34 tensor(1.00000e-07 *\n",
      "       4.8334)\n",
      "35 tensor(1.00000e-07 *\n",
      "       4.8091)\n",
      "36 tensor(1.00000e-07 *\n",
      "       4.7877)\n",
      "37 tensor(1.00000e-07 *\n",
      "       4.7636)\n",
      "38 tensor(1.00000e-07 *\n",
      "       4.7411)\n",
      "39 tensor(1.00000e-07 *\n",
      "       4.7171)\n",
      "40 tensor(1.00000e-07 *\n",
      "       4.6959)\n",
      "41 tensor(1.00000e-07 *\n",
      "       4.6720)\n",
      "42 tensor(1.00000e-07 *\n",
      "       4.6498)\n",
      "43 tensor(1.00000e-07 *\n",
      "       4.6276)\n",
      "44 tensor(1.00000e-07 *\n",
      "       4.6055)\n",
      "45 tensor(1.00000e-07 *\n",
      "       4.5830)\n",
      "46 tensor(1.00000e-07 *\n",
      "       4.5609)\n",
      "47 tensor(1.00000e-07 *\n",
      "       4.5390)\n",
      "48 tensor(1.00000e-07 *\n",
      "       4.5173)\n",
      "49 tensor(1.00000e-07 *\n",
      "       4.4947)\n",
      "50 tensor(1.00000e-07 *\n",
      "       4.4747)\n",
      "51 tensor(1.00000e-07 *\n",
      "       4.4530)\n",
      "52 tensor(1.00000e-07 *\n",
      "       4.4308)\n",
      "53 tensor(1.00000e-07 *\n",
      "       4.4114)\n",
      "54 tensor(1.00000e-07 *\n",
      "       4.3891)\n",
      "55 tensor(1.00000e-07 *\n",
      "       4.3678)\n",
      "56 tensor(1.00000e-07 *\n",
      "       4.3463)\n",
      "57 tensor(1.00000e-07 *\n",
      "       4.3260)\n",
      "58 tensor(1.00000e-07 *\n",
      "       4.3068)\n",
      "59 tensor(1.00000e-07 *\n",
      "       4.2865)\n",
      "60 tensor(1.00000e-07 *\n",
      "       4.2635)\n",
      "61 tensor(1.00000e-07 *\n",
      "       4.2440)\n",
      "62 tensor(1.00000e-07 *\n",
      "       4.2246)\n",
      "63 tensor(1.00000e-07 *\n",
      "       4.2017)\n",
      "64 tensor(1.00000e-07 *\n",
      "       4.1839)\n",
      "65 tensor(1.00000e-07 *\n",
      "       4.1628)\n",
      "66 tensor(1.00000e-07 *\n",
      "       4.1429)\n",
      "67 tensor(1.00000e-07 *\n",
      "       4.1252)\n",
      "68 tensor(1.00000e-07 *\n",
      "       4.1026)\n",
      "69 tensor(1.00000e-07 *\n",
      "       4.0850)\n",
      "70 tensor(1.00000e-07 *\n",
      "       4.0642)\n",
      "71 tensor(1.00000e-07 *\n",
      "       4.0450)\n",
      "72 tensor(1.00000e-07 *\n",
      "       4.0243)\n",
      "73 tensor(1.00000e-07 *\n",
      "       4.0058)\n",
      "74 tensor(1.00000e-07 *\n",
      "       3.9884)\n",
      "75 tensor(1.00000e-07 *\n",
      "       3.9678)\n",
      "76 tensor(1.00000e-07 *\n",
      "       3.9488)\n",
      "77 tensor(1.00000e-07 *\n",
      "       3.9299)\n",
      "78 tensor(1.00000e-07 *\n",
      "       3.9110)\n",
      "79 tensor(1.00000e-07 *\n",
      "       3.8921)\n",
      "80 tensor(1.00000e-07 *\n",
      "       3.8739)\n",
      "81 tensor(1.00000e-07 *\n",
      "       3.8547)\n",
      "82 tensor(1.00000e-07 *\n",
      "       3.8376)\n",
      "83 tensor(1.00000e-07 *\n",
      "       3.8179)\n",
      "84 tensor(1.00000e-07 *\n",
      "       3.7988)\n",
      "85 tensor(1.00000e-07 *\n",
      "       3.7802)\n",
      "86 tensor(1.00000e-07 *\n",
      "       3.7633)\n",
      "87 tensor(1.00000e-07 *\n",
      "       3.7450)\n",
      "88 tensor(1.00000e-07 *\n",
      "       3.7282)\n",
      "89 tensor(1.00000e-07 *\n",
      "       3.7098)\n",
      "90 tensor(1.00000e-07 *\n",
      "       3.6930)\n",
      "91 tensor(1.00000e-07 *\n",
      "       3.6747)\n",
      "92 tensor(1.00000e-07 *\n",
      "       3.6560)\n",
      "93 tensor(1.00000e-07 *\n",
      "       3.6377)\n",
      "94 tensor(1.00000e-07 *\n",
      "       3.6211)\n",
      "95 tensor(1.00000e-07 *\n",
      "       3.6030)\n",
      "96 tensor(1.00000e-07 *\n",
      "       3.5865)\n",
      "97 tensor(1.00000e-07 *\n",
      "       3.5704)\n",
      "98 tensor(1.00000e-07 *\n",
      "       3.5540)\n",
      "99 tensor(1.00000e-07 *\n",
      "       3.5360)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    y_pred = model(x_data) \n",
    "    '''\n",
    "    注意！SGD优化器就是梯度下降优化器\n",
    "    无论是 批量梯度下降（全体样本计算损失函数后对loss反向传播）/随机梯度下降（随机单个样本）/mini-batch梯度下降（部分）\n",
    "    都用SGD！只取决于输入给model的是什么！\n",
    "    这里 model(x_data) 说明直接给模型输入了全体数据，计算的损失函数自然也是全体的\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(epoch, loss)\n",
    "    \n",
    "    optimizer.zero_grad() # 清空梯度\n",
    "    loss.backward() # 反向传播\n",
    "    optimizer.step() # 更新权重（梯度下降）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 打印权重和偏置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = 1.999310851097107\n",
      "b = 0.0015660704812034965\n"
     ]
    }
   ],
   "source": [
    "print('w =', model.linear.weight.item())\n",
    "print('b =', model.linear.bias.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred =  tensor([[ 7.9988],\n",
      "        [ 9.9981]])\n"
     ]
    }
   ],
   "source": [
    "x_test = torch.Tensor([[4.0],[5.0]]) # test的格式依旧要满足这个格式：几行代表几个样本，每行有几列代表输入向量的维数\n",
    "y_test = model(x_test)\n",
    "print('y_pred = ', y_test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 逻辑回归 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'> 虽然叫回归，其实是做分类问题的！模型的输出是每个class的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "# 这个函数包包含了很多有用的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y_pred = F.sigmoid(self.linear(x))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss(size_average=False)\n",
    "# 设为True，意味着loss = -(1/N) * N个样本BCE损失的累加\n",
    "# 那这样一个系数(1/N)显然在求导时会保留，对我们设置学习率显然会产生影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "# 我发现Adam确实比纯梯度下降SGD好很多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = torch.tensor([[1.0], [2.0], [3.0]])\n",
    "y_data = torch.tensor([[0.0], [0.0], [1.0]]) \n",
    "# 分类问题中，也要注意训练集x和y tensor的格式！\n",
    "# 每行代表一个固定维数的向量！即整体是矩阵形式，每个样本的x/y即每个行向量\n",
    "# 注意！尽量使用浮点型tensor！写整型可能会报错（比如这里计算BCE时，y_pred是float型向量，target的y_data如果是0，1即long型张量会报错！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.12043005228042603\n",
      "1 0.1203174814581871\n",
      "2 0.12020499259233475\n",
      "3 0.12009277939796448\n",
      "4 0.11998060345649719\n",
      "5 0.11986860632896423\n",
      "6 0.11975675821304321\n",
      "7 0.11964506655931473\n",
      "8 0.1195334792137146\n",
      "9 0.1194220781326294\n",
      "10 0.11931079626083374\n",
      "11 0.11919969320297241\n",
      "12 0.11908867210149765\n",
      "13 0.11897792667150497\n",
      "14 0.11886724084615707\n",
      "15 0.11875665187835693\n",
      "16 0.11864627152681351\n",
      "17 0.11853599548339844\n",
      "18 0.11842594295740128\n",
      "19 0.11831596493721008\n",
      "20 0.11820613592863083\n",
      "21 0.11809647083282471\n",
      "22 0.11798694729804993\n",
      "23 0.11787759512662888\n",
      "24 0.1177682876586914\n",
      "25 0.11765914410352707\n",
      "26 0.11755022406578064\n",
      "27 0.11744140833616257\n",
      "28 0.11733267456293106\n",
      "29 0.11722413450479507\n",
      "30 0.11711576581001282\n",
      "31 0.11700750142335892\n",
      "32 0.11689939349889755\n",
      "33 0.11679136008024216\n",
      "34 0.11668354272842407\n",
      "35 0.11657586693763733\n",
      "36 0.11646831035614014\n",
      "37 0.1163608655333519\n",
      "38 0.116253562271595\n",
      "39 0.11614641547203064\n",
      "40 0.11603938788175583\n",
      "41 0.11593251675367355\n",
      "42 0.11582577228546143\n",
      "43 0.11571919918060303\n",
      "44 0.11561271548271179\n",
      "45 0.11550641059875488\n",
      "46 0.11540021747350693\n",
      "47 0.11529412865638733\n",
      "48 0.11518821120262146\n",
      "49 0.11508246511220932\n",
      "50 0.11497679352760315\n",
      "51 0.1148713007569313\n",
      "52 0.11476591974496841\n",
      "53 0.11466067284345627\n",
      "54 0.11455556750297546\n",
      "55 0.1144505962729454\n",
      "56 0.11434576660394669\n",
      "57 0.11424102634191513\n",
      "58 0.1141364648938179\n",
      "59 0.114032082259655\n",
      "60 0.11392777413129807\n",
      "61 0.11382359266281128\n",
      "62 0.11371951550245285\n",
      "63 0.11361559480428696\n",
      "64 0.11351186782121658\n",
      "65 0.11340820789337158\n",
      "66 0.11330466717481613\n",
      "67 0.11320128291845322\n",
      "68 0.11309803277254105\n",
      "69 0.11299487948417664\n",
      "70 0.11289194226264954\n",
      "71 0.112789086997509\n",
      "72 0.11268633604049683\n",
      "73 0.11258372664451599\n",
      "74 0.1124812662601471\n",
      "75 0.11237892508506775\n",
      "76 0.11227671056985855\n",
      "77 0.11217465251684189\n",
      "78 0.11207267642021179\n",
      "79 0.11197086423635483\n",
      "80 0.11186915636062622\n",
      "81 0.11176761239767075\n",
      "82 0.11166617274284363\n",
      "83 0.11156483739614487\n",
      "84 0.11146365851163864\n",
      "85 0.11136256903409958\n",
      "86 0.11126162856817245\n",
      "87 0.11116086691617966\n",
      "88 0.11106014251708984\n",
      "89 0.11095959693193436\n",
      "90 0.11085918545722961\n",
      "91 0.11075887084007263\n",
      "92 0.11065869778394699\n",
      "93 0.1105586364865303\n",
      "94 0.11045870929956436\n",
      "95 0.11035887151956558\n",
      "96 0.11025923490524292\n",
      "97 0.11015969514846802\n",
      "98 0.11006024479866028\n",
      "99 0.10996098071336746\n",
      "100 0.10986173897981644\n",
      "101 0.10976269841194153\n",
      "102 0.10966378450393677\n",
      "103 0.10956493020057678\n",
      "104 0.10946623235940933\n",
      "105 0.10936770588159561\n",
      "106 0.10926923155784607\n",
      "107 0.10917093604803085\n",
      "108 0.1090727224946022\n",
      "109 0.10897465795278549\n",
      "110 0.10887668281793594\n",
      "111 0.10877885669469833\n",
      "112 0.10868114233016968\n",
      "113 0.10858353972434998\n",
      "114 0.10848606377840042\n",
      "115 0.10838872194290161\n",
      "116 0.10829147696495056\n",
      "117 0.10819437354803085\n",
      "118 0.1080973818898201\n",
      "119 0.10800047963857651\n",
      "120 0.10790372639894485\n",
      "121 0.10780709981918335\n",
      "122 0.10771054774522781\n",
      "123 0.1076141893863678\n",
      "124 0.10751789063215256\n",
      "125 0.10742177814245224\n",
      "126 0.10732567310333252\n",
      "127 0.10722976177930832\n",
      "128 0.10713398456573486\n",
      "129 0.10703829675912857\n",
      "130 0.10694269090890884\n",
      "131 0.10684724897146225\n",
      "132 0.10675191134214401\n",
      "133 0.10665667057037354\n",
      "134 0.106561578810215\n",
      "135 0.1064666137099266\n",
      "136 0.10637173801660538\n",
      "137 0.1062769815325737\n",
      "138 0.10618229955434799\n",
      "139 0.10608777403831482\n",
      "140 0.10599339008331299\n",
      "141 0.10589911788702011\n",
      "142 0.1058049201965332\n",
      "143 0.10571086406707764\n",
      "144 0.10561687499284744\n",
      "145 0.10552304983139038\n",
      "146 0.10542932897806168\n",
      "147 0.10533574223518372\n",
      "148 0.10524225980043411\n",
      "149 0.10514884442090988\n",
      "150 0.10505557060241699\n",
      "151 0.10496240854263306\n",
      "152 0.10486938804388046\n",
      "153 0.10477644950151443\n",
      "154 0.10468364506959915\n",
      "155 0.10459095239639282\n",
      "156 0.10449834913015366\n",
      "157 0.10440585762262344\n",
      "158 0.10431347042322159\n",
      "159 0.10422124713659286\n",
      "160 0.10412909835577011\n",
      "161 0.10403706878423691\n",
      "162 0.10394512861967087\n",
      "163 0.10385329276323318\n",
      "164 0.10376161336898804\n",
      "165 0.10367000848054886\n",
      "166 0.10357857495546341\n",
      "167 0.10348719358444214\n",
      "168 0.10339593142271042\n",
      "169 0.10330478101968765\n",
      "170 0.10321373492479324\n",
      "171 0.10312280058860779\n",
      "172 0.1030319556593895\n",
      "173 0.10294123739004135\n",
      "174 0.10285066813230515\n",
      "175 0.10276012867689133\n",
      "176 0.10266974568367004\n",
      "177 0.1025795042514801\n",
      "178 0.10248931497335434\n",
      "179 0.10239923745393753\n",
      "180 0.10230932384729385\n",
      "181 0.10221944004297256\n",
      "182 0.102129727602005\n",
      "183 0.10204007476568222\n",
      "184 0.10195054858922958\n",
      "185 0.1018611490726471\n",
      "186 0.10177186131477356\n",
      "187 0.10168259590864182\n",
      "188 0.10159354656934738\n",
      "189 0.10150448232889175\n",
      "190 0.10141564160585403\n",
      "191 0.10132681578397751\n",
      "192 0.1012381911277771\n",
      "193 0.10114958882331848\n",
      "194 0.1010611429810524\n",
      "195 0.10097277164459229\n",
      "196 0.10088451951742172\n",
      "197 0.10079639405012131\n",
      "198 0.10070833563804626\n",
      "199 0.10062038153409958\n",
      "200 0.10053253918886185\n",
      "201 0.10044483095407486\n",
      "202 0.10035717487335205\n",
      "203 0.10026967525482178\n",
      "204 0.10018223524093628\n",
      "205 0.10009489208459854\n",
      "206 0.10000767558813095\n",
      "207 0.09992057085037231\n",
      "208 0.09983352571725845\n",
      "209 0.09974667429924011\n",
      "210 0.09965983778238297\n",
      "211 0.09957314282655716\n",
      "212 0.09948652982711792\n",
      "213 0.09940006583929062\n",
      "214 0.09931367635726929\n",
      "215 0.09922736138105392\n",
      "216 0.09914115816354752\n",
      "217 0.09905505925416946\n",
      "218 0.09896910190582275\n",
      "219 0.09888318926095963\n",
      "220 0.09879738092422485\n",
      "221 0.09871172904968262\n",
      "222 0.09862611442804337\n",
      "223 0.09854063391685486\n",
      "224 0.09845522046089172\n",
      "225 0.09836997836828232\n",
      "226 0.0982847586274147\n",
      "227 0.09819968789815903\n",
      "228 0.09811467677354813\n",
      "229 0.09802978485822678\n",
      "230 0.09794503450393677\n",
      "231 0.09786030650138855\n",
      "232 0.09777572005987167\n",
      "233 0.09769120812416077\n",
      "234 0.09760681539773941\n",
      "235 0.0975225567817688\n",
      "236 0.09743831306695938\n",
      "237 0.0973542258143425\n",
      "238 0.09727022796869278\n",
      "239 0.09718633443117142\n",
      "240 0.09710249304771423\n",
      "241 0.09701883792877197\n",
      "242 0.09693523496389389\n",
      "243 0.09685170650482178\n",
      "244 0.09676826000213623\n",
      "245 0.09668496251106262\n",
      "246 0.09660174697637558\n",
      "247 0.0965186282992363\n",
      "248 0.096435546875\n",
      "249 0.09635264426469803\n",
      "250 0.09626978635787964\n",
      "251 0.096187062561512\n",
      "252 0.09610441327095032\n",
      "253 0.09602189064025879\n",
      "254 0.09593939781188965\n",
      "255 0.09585705399513245\n",
      "256 0.09577479213476181\n",
      "257 0.09569262713193893\n",
      "258 0.09561055153608322\n",
      "259 0.09552854299545288\n",
      "260 0.09544666856527328\n",
      "261 0.09536489844322205\n",
      "262 0.09528317302465439\n",
      "263 0.09520155936479568\n",
      "264 0.09512007236480713\n",
      "265 0.09503864496946335\n",
      "266 0.09495732188224792\n",
      "267 0.09487608820199966\n",
      "268 0.09479495137929916\n",
      "269 0.09471392631530762\n",
      "270 0.09463295340538025\n",
      "271 0.09455209970474243\n",
      "272 0.09447132796049118\n",
      "273 0.09439069032669067\n",
      "274 0.09431011229753494\n",
      "275 0.09422958642244339\n",
      "276 0.09414922446012497\n",
      "277 0.09406893700361252\n",
      "278 0.09398872405290604\n",
      "279 0.09390860050916672\n",
      "280 0.09382858872413635\n",
      "281 0.09374862164258957\n",
      "282 0.09366878867149353\n",
      "283 0.09358908981084824\n",
      "284 0.09350937604904175\n",
      "285 0.09342982620000839\n",
      "286 0.09335032850503922\n",
      "287 0.09327096492052078\n",
      "288 0.09319164603948593\n",
      "289 0.09311244636774063\n",
      "290 0.0930333137512207\n",
      "291 0.09295431524515152\n",
      "292 0.09287538379430771\n",
      "293 0.09279654175043106\n",
      "294 0.09271775931119919\n",
      "295 0.09263908863067627\n",
      "296 0.09256049990653992\n",
      "297 0.09248200803995132\n",
      "298 0.09240364283323288\n",
      "299 0.09232529252767563\n",
      "300 0.09224707633256912\n",
      "301 0.09216893464326859\n",
      "302 0.09209092706441879\n",
      "303 0.09201297163963318\n",
      "304 0.09193509817123413\n",
      "305 0.09185732156038284\n",
      "306 0.09177961200475693\n",
      "307 0.09170202165842056\n",
      "308 0.09162449836730957\n",
      "309 0.09154706448316574\n",
      "310 0.09146971255540848\n",
      "311 0.09139245748519897\n",
      "312 0.09131526947021484\n",
      "313 0.09123817086219788\n",
      "314 0.09116116911172867\n",
      "315 0.09108427911996841\n",
      "316 0.09100747853517532\n",
      "317 0.09093073010444641\n",
      "318 0.09085405617952347\n",
      "319 0.09077753871679306\n",
      "320 0.09070104360580444\n",
      "321 0.09062465280294418\n",
      "322 0.0905483290553093\n",
      "323 0.09047213196754456\n",
      "324 0.090395987033844\n",
      "325 0.09031993895769119\n",
      "326 0.09024400264024734\n",
      "327 0.09016811847686768\n",
      "328 0.09009230881929398\n",
      "329 0.09001660346984863\n",
      "330 0.08994100242853165\n",
      "331 0.08986545354127884\n",
      "332 0.08979001641273499\n",
      "333 0.0897146537899971\n",
      "334 0.08963938802480698\n",
      "335 0.08956417441368103\n",
      "336 0.08948907256126404\n",
      "337 0.08941403031349182\n",
      "338 0.08933907002210617\n",
      "339 0.08926424384117126\n",
      "340 0.08918946236371994\n",
      "341 0.08911475539207458\n",
      "342 0.0890401229262352\n",
      "343 0.08896561712026596\n",
      "344 0.0888911560177803\n",
      "345 0.0888168141245842\n",
      "346 0.08874252438545227\n",
      "347 0.0886683464050293\n",
      "348 0.0885942354798317\n",
      "349 0.08852019906044006\n",
      "350 0.0884462222456932\n",
      "351 0.08837234973907471\n",
      "352 0.08829856663942337\n",
      "353 0.0882248654961586\n",
      "354 0.08815126866102219\n",
      "355 0.08807770162820816\n",
      "356 0.08800426870584488\n",
      "357 0.08793088048696518\n",
      "358 0.08785757422447205\n",
      "359 0.08778435736894608\n",
      "360 0.08771120756864548\n",
      "361 0.08763816207647324\n",
      "362 0.08756520599126816\n",
      "363 0.08749228715896606\n",
      "364 0.08741948008537292\n",
      "365 0.08734676241874695\n",
      "366 0.08727413415908813\n",
      "367 0.0872015431523323\n",
      "368 0.08712903410196304\n",
      "369 0.08705665916204453\n",
      "370 0.0869842991232872\n",
      "371 0.08691207319498062\n",
      "372 0.08683990687131882\n",
      "373 0.08676782250404358\n",
      "374 0.08669578284025192\n",
      "375 0.08662387728691101\n",
      "376 0.08655203133821487\n",
      "377 0.08648025244474411\n",
      "378 0.08640851825475693\n",
      "379 0.08633691072463989\n",
      "380 0.08626538515090942\n",
      "381 0.08619389683008194\n",
      "382 0.08612257242202759\n",
      "383 0.08605121821165085\n",
      "384 0.08597999811172485\n",
      "385 0.08590885251760483\n",
      "386 0.08583781868219376\n",
      "387 0.08576682209968567\n",
      "388 0.08569588512182236\n",
      "389 0.08562508225440979\n",
      "390 0.08555430173873901\n",
      "391 0.085483618080616\n",
      "392 0.08541301637887955\n",
      "393 0.08534250408411026\n",
      "394 0.08527206629514694\n",
      "395 0.0852016806602478\n",
      "396 0.08513134717941284\n",
      "397 0.08506113290786743\n",
      "398 0.08499101549386978\n",
      "399 0.08492094278335571\n",
      "400 0.08485093712806702\n",
      "401 0.08478107303380966\n",
      "402 0.0847112238407135\n",
      "403 0.08464150875806808\n",
      "404 0.08457178622484207\n",
      "405 0.08450213819742203\n",
      "406 0.0844326838850975\n",
      "407 0.08436316251754761\n",
      "408 0.08429381251335144\n",
      "409 0.08422451466321945\n",
      "410 0.08415526896715164\n",
      "411 0.08408612012863159\n",
      "412 0.08401703834533691\n",
      "413 0.0839480459690094\n",
      "414 0.08387910574674606\n",
      "415 0.08381027728319168\n",
      "416 0.08374146372079849\n",
      "417 0.08367276936769485\n",
      "418 0.08360414952039719\n",
      "419 0.08353560417890549\n",
      "420 0.08346711844205856\n",
      "421 0.08339870721101761\n",
      "422 0.08333032578229904\n",
      "423 0.08326210826635361\n",
      "424 0.08319392800331116\n",
      "425 0.08312580734491348\n",
      "426 0.08305773884057999\n",
      "427 0.08298976719379425\n",
      "428 0.08292187750339508\n",
      "429 0.08285408467054367\n",
      "430 0.08278632909059525\n",
      "431 0.08271867781877518\n",
      "432 0.0826510563492775\n",
      "433 0.08258354663848877\n",
      "434 0.08251611143350601\n",
      "435 0.08244866132736206\n",
      "436 0.08238136768341064\n",
      "437 0.0823141410946846\n",
      "438 0.08224698156118393\n",
      "439 0.08217989653348923\n",
      "440 0.08211289346218109\n",
      "441 0.08204592019319534\n",
      "442 0.08197905123233795\n",
      "443 0.08191225677728653\n",
      "444 0.0818454846739769\n",
      "445 0.08177883177995682\n",
      "446 0.0817122682929039\n",
      "447 0.08164574950933456\n",
      "448 0.08157927542924881\n",
      "449 0.081512950360775\n",
      "450 0.0814465880393982\n",
      "451 0.08138035982847214\n",
      "452 0.08131424337625504\n",
      "453 0.08124811202287674\n",
      "454 0.0811820924282074\n",
      "455 0.08111617714166641\n",
      "456 0.08105025440454483\n",
      "457 0.08098448067903519\n",
      "458 0.08091873675584793\n",
      "459 0.08085306733846664\n",
      "460 0.08078742772340775\n",
      "461 0.08072192221879959\n",
      "462 0.08065646141767502\n",
      "463 0.08059106022119522\n",
      "464 0.08052576333284378\n",
      "465 0.08046049624681473\n",
      "466 0.08039534837007523\n",
      "467 0.08033023029565811\n",
      "468 0.08026514947414398\n",
      "469 0.08020017296075821\n",
      "470 0.080135278403759\n",
      "471 0.08007042855024338\n",
      "472 0.0800056904554367\n",
      "473 0.07994100451469421\n",
      "474 0.0798763558268547\n",
      "475 0.07981180399656296\n",
      "476 0.07974731177091599\n",
      "477 0.07968290150165558\n",
      "478 0.07961852103471756\n",
      "479 0.07955421507358551\n",
      "480 0.07948999851942062\n",
      "481 0.07942584902048111\n",
      "482 0.07936178892850876\n",
      "483 0.07929777354001999\n",
      "484 0.0792338028550148\n",
      "485 0.07916992157697678\n",
      "486 0.07910612225532532\n",
      "487 0.07904238253831863\n",
      "488 0.07897871732711792\n",
      "489 0.0789150819182396\n",
      "490 0.07885158061981201\n",
      "491 0.07878807932138443\n",
      "492 0.07872463762760162\n",
      "493 0.07866134494543076\n",
      "494 0.0785980373620987\n",
      "495 0.07853487133979797\n",
      "496 0.07847172021865845\n",
      "497 0.0784086212515831\n",
      "498 0.0783456489443779\n",
      "499 0.0782826766371727\n",
      "500 0.07821976393461227\n",
      "501 0.0781569704413414\n",
      "502 0.0780942440032959\n",
      "503 0.07803156226873398\n",
      "504 0.07796894758939743\n",
      "505 0.07790639251470566\n",
      "506 0.07784392684698105\n",
      "507 0.07778152823448181\n",
      "508 0.07771914452314377\n",
      "509 0.07765688747167587\n",
      "510 0.07759465277194977\n",
      "511 0.07753252238035202\n",
      "512 0.07747044414281845\n",
      "513 0.07740839570760727\n",
      "514 0.07734640687704086\n",
      "515 0.07728453725576401\n",
      "516 0.07722270488739014\n",
      "517 0.07716091722249985\n",
      "518 0.07709923386573792\n",
      "519 0.07703762501478195\n",
      "520 0.07697605341672897\n",
      "521 0.07691454142332077\n",
      "522 0.07685307413339615\n",
      "523 0.07679171115159988\n",
      "524 0.0767304077744484\n",
      "525 0.07666917145252228\n",
      "526 0.07660797983407974\n",
      "527 0.07654684036970139\n",
      "528 0.0764857828617096\n",
      "529 0.07642478495836258\n",
      "530 0.07636388391256332\n",
      "531 0.07630299776792526\n",
      "532 0.07624220103025436\n",
      "533 0.07618142664432526\n",
      "534 0.07612074911594391\n",
      "535 0.07606016844511032\n",
      "536 0.07599963247776031\n",
      "537 0.0759391188621521\n",
      "538 0.07587870210409164\n",
      "539 0.07581832259893417\n",
      "540 0.07575803250074387\n",
      "541 0.07569778710603714\n",
      "542 0.07563760131597519\n",
      "543 0.07557752728462219\n",
      "544 0.07551746815443039\n",
      "545 0.07545749098062515\n",
      "546 0.0753975585103035\n",
      "547 0.07533770799636841\n",
      "548 0.0752778872847557\n",
      "549 0.07521816343069077\n",
      "550 0.07515846937894821\n",
      "551 0.07509884238243103\n",
      "552 0.07503931224346161\n",
      "553 0.07497981935739517\n",
      "554 0.07492037862539291\n",
      "555 0.07486101239919662\n",
      "556 0.0748017430305481\n",
      "557 0.07474245876073837\n",
      "558 0.0746832862496376\n",
      "559 0.07462415844202042\n",
      "560 0.074565090239048\n",
      "561 0.07450606673955917\n",
      "562 0.07444712519645691\n",
      "563 0.07438824325799942\n",
      "564 0.0743294283747673\n",
      "565 0.07427066564559937\n",
      "566 0.0742119774222374\n",
      "567 0.0741533413529396\n",
      "568 0.07409477978944778\n",
      "569 0.07403625547885895\n",
      "570 0.0739777535200119\n",
      "571 0.0739193856716156\n",
      "572 0.07386106252670288\n",
      "573 0.07380274683237076\n",
      "574 0.07374455779790878\n",
      "575 0.0736863762140274\n",
      "576 0.07362830638885498\n",
      "577 0.07357028871774673\n",
      "578 0.07351229339838028\n",
      "579 0.0734543651342392\n",
      "580 0.07339650392532349\n",
      "581 0.07333871722221375\n",
      "582 0.0732809528708458\n",
      "583 0.07322324812412262\n",
      "584 0.07316561043262482\n",
      "585 0.07310808449983597\n",
      "586 0.07305057346820831\n",
      "587 0.07299310714006424\n",
      "588 0.07293570786714554\n",
      "589 0.0728784054517746\n",
      "590 0.07282114773988724\n",
      "591 0.07276391983032227\n",
      "592 0.07270679622888565\n",
      "593 0.07264968007802963\n",
      "594 0.07259266823530197\n",
      "595 0.07253564149141312\n",
      "596 0.072478748857975\n",
      "597 0.07242187112569809\n",
      "598 0.07236504554748535\n",
      "599 0.07230829447507858\n",
      "600 0.07225161790847778\n",
      "601 0.07219498604536057\n",
      "602 0.07213842868804932\n",
      "603 0.07208189368247986\n",
      "604 0.07202541828155518\n",
      "605 0.07196900993585587\n",
      "606 0.07191269844770432\n",
      "607 0.07185642421245575\n",
      "608 0.07180016487836838\n",
      "609 0.07174400240182877\n",
      "610 0.07168791443109512\n",
      "611 0.07163184136152267\n",
      "612 0.0715758427977562\n",
      "613 0.0715198889374733\n",
      "614 0.07146401703357697\n",
      "615 0.07140814512968063\n",
      "616 0.07135238498449326\n",
      "617 0.07129666209220886\n",
      "618 0.07124098390340805\n",
      "619 0.0711853876709938\n",
      "620 0.07112985849380493\n",
      "621 0.07107434421777725\n",
      "622 0.07101891934871674\n",
      "623 0.07096350938081741\n",
      "624 0.07090818136930466\n",
      "625 0.07085291296243668\n",
      "626 0.07079776376485825\n",
      "627 0.07074255496263504\n",
      "628 0.07068748027086258\n",
      "629 0.0706324651837349\n",
      "630 0.07057743519544601\n",
      "631 0.07052252441644669\n",
      "632 0.07046765834093094\n",
      "633 0.070412777364254\n",
      "634 0.0703580304980278\n",
      "635 0.07030332833528519\n",
      "636 0.07024868577718735\n",
      "637 0.0701940655708313\n",
      "638 0.07013949006795883\n",
      "639 0.07008501142263412\n",
      "640 0.07003055512905121\n",
      "641 0.06997617334127426\n",
      "642 0.0699218362569809\n",
      "643 0.06986752897500992\n",
      "644 0.0698133185505867\n",
      "645 0.06975914537906647\n",
      "646 0.06970503181219101\n",
      "647 0.06965095549821854\n",
      "648 0.06959699094295502\n",
      "649 0.0695430338382721\n",
      "650 0.06948914378881454\n",
      "651 0.06943527609109879\n",
      "652 0.06938150525093079\n",
      "653 0.06932776421308517\n",
      "654 0.06927409768104553\n",
      "655 0.06922047585248947\n",
      "656 0.06916692107915878\n",
      "657 0.06911341100931168\n",
      "658 0.06905994564294815\n",
      "659 0.0690065249800682\n",
      "660 0.06895317882299423\n",
      "661 0.06889989227056503\n",
      "662 0.06884661316871643\n",
      "663 0.06879344582557678\n",
      "664 0.06874030828475952\n",
      "665 0.06868722289800644\n",
      "666 0.06863414496183395\n",
      "667 0.06858120113611221\n",
      "668 0.06852824985980988\n",
      "669 0.06847537308931351\n",
      "670 0.06842256337404251\n",
      "671 0.06836976855993271\n",
      "672 0.06831707805395126\n",
      "673 0.06826440244913101\n",
      "674 0.06821179389953613\n",
      "675 0.06815923750400543\n",
      "676 0.06810671836137772\n",
      "677 0.06805425137281418\n",
      "678 0.06800185143947601\n",
      "679 0.06794950366020203\n",
      "680 0.06789719313383102\n",
      "681 0.0678449347615242\n",
      "682 0.06779276579618454\n",
      "683 0.06774058192968369\n",
      "684 0.0676884651184082\n",
      "685 0.06763646006584167\n",
      "686 0.06758445501327515\n",
      "687 0.06753252446651459\n",
      "688 0.0674806535243988\n",
      "689 0.06742880493402481\n",
      "690 0.067377008497715\n",
      "691 0.06732527166604996\n",
      "692 0.06727361679077148\n",
      "693 0.06722196191549301\n",
      "694 0.06717041879892349\n",
      "695 0.06711884588003159\n",
      "696 0.06706739217042923\n",
      "697 0.06701593846082687\n",
      "698 0.06696458905935287\n",
      "699 0.06691323965787888\n",
      "700 0.06686197221279144\n",
      "701 0.06681076437234879\n",
      "702 0.06675958633422852\n",
      "703 0.06670848280191422\n",
      "704 0.0666574165225029\n",
      "705 0.06660636514425278\n",
      "706 0.06655538827180862\n",
      "707 0.06650447100400925\n",
      "708 0.06645359843969345\n",
      "709 0.06640280038118362\n",
      "710 0.06635203212499619\n",
      "711 0.06630131602287292\n",
      "712 0.06625065952539444\n",
      "713 0.06620002537965775\n",
      "714 0.06614943593740463\n",
      "715 0.06609893590211868\n",
      "716 0.06604842096567154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717 0.06599801033735275\n",
      "718 0.06594762951135635\n",
      "719 0.06589726358652115\n",
      "720 0.06584702432155609\n",
      "721 0.06579678505659103\n",
      "722 0.06574659794569016\n",
      "723 0.06569645553827286\n",
      "724 0.06564640253782272\n",
      "725 0.06559636443853378\n",
      "726 0.06554637849330902\n",
      "727 0.06549645215272903\n",
      "728 0.06544657051563263\n",
      "729 0.0653967335820198\n",
      "730 0.06534691900014877\n",
      "731 0.0652971938252449\n",
      "732 0.06524749845266342\n",
      "733 0.06519786268472672\n",
      "734 0.06514827907085419\n",
      "735 0.06509873270988464\n",
      "736 0.06504926830530167\n",
      "737 0.06499981880187988\n",
      "738 0.06495039910078049\n",
      "739 0.06490109115839005\n",
      "740 0.06485173851251602\n",
      "741 0.06480250507593155\n",
      "742 0.06475331634283066\n",
      "743 0.06470420211553574\n",
      "744 0.06465505808591843\n",
      "745 0.06460600346326828\n",
      "746 0.06455698609352112\n",
      "747 0.06450801342725754\n",
      "748 0.06445908546447754\n",
      "749 0.06441020220518112\n",
      "750 0.06436139345169067\n",
      "751 0.064312644302845\n",
      "752 0.06426388025283813\n",
      "753 0.06421518325805664\n",
      "754 0.0641665831208229\n",
      "755 0.06411799043416977\n",
      "756 0.0640694722533226\n",
      "757 0.06402098387479782\n",
      "758 0.06397251039743423\n",
      "759 0.06392413377761841\n",
      "760 0.06387577205896378\n",
      "761 0.06382748484611511\n",
      "762 0.06377923488616943\n",
      "763 0.06373105943202972\n",
      "764 0.06368289887905121\n",
      "765 0.06363474577665329\n",
      "766 0.06358665972948074\n",
      "767 0.06353867799043655\n",
      "768 0.06349068135023117\n",
      "769 0.06344275176525116\n",
      "770 0.06339482963085175\n",
      "771 0.06334701180458069\n",
      "772 0.06329921633005142\n",
      "773 0.06325148791074753\n",
      "774 0.06320377439260483\n",
      "775 0.06315615028142929\n",
      "776 0.06310854852199554\n",
      "777 0.0630609542131424\n",
      "778 0.06301349401473999\n",
      "779 0.0629660114645958\n",
      "780 0.06291858106851578\n",
      "781 0.06287122517824173\n",
      "782 0.06282389163970947\n",
      "783 0.0627765953540802\n",
      "784 0.0627293512225151\n",
      "785 0.0626821517944336\n",
      "786 0.06263498216867447\n",
      "787 0.0625879243016243\n",
      "788 0.06254087388515472\n",
      "789 0.062493860721588135\n",
      "790 0.06244688853621483\n",
      "791 0.062399983406066895\n",
      "792 0.062353115528821945\n",
      "793 0.06230628117918968\n",
      "794 0.06225952133536339\n",
      "795 0.062212735414505005\n",
      "796 0.06216603145003319\n",
      "797 0.06211940944194794\n",
      "798 0.06207278370857239\n",
      "799 0.06202620640397072\n",
      "800 0.061979714781045914\n",
      "801 0.06193322315812111\n",
      "802 0.06188681721687317\n",
      "803 0.061840418726205826\n",
      "804 0.06179406866431236\n",
      "805 0.06174775958061218\n",
      "806 0.06170156970620155\n",
      "807 0.06165531277656555\n",
      "808 0.061609163880348206\n",
      "809 0.061563074588775635\n",
      "810 0.06151697412133217\n",
      "811 0.061470940709114075\n",
      "812 0.06142497435212135\n",
      "813 0.061379045248031616\n",
      "814 0.06133314594626427\n",
      "815 0.06128726527094841\n",
      "816 0.061241477727890015\n",
      "817 0.06119571626186371\n",
      "818 0.061150018125772476\n",
      "819 0.06110433116555214\n",
      "820 0.06105868145823479\n",
      "821 0.061013106256723404\n",
      "822 0.06096754968166351\n",
      "823 0.0609220452606678\n",
      "824 0.06087660416960716\n",
      "825 0.06083117052912712\n",
      "826 0.06078583002090454\n",
      "827 0.06074047088623047\n",
      "828 0.06069517508149147\n",
      "829 0.06064996495842934\n",
      "830 0.0606047548353672\n",
      "831 0.06055955961346626\n",
      "832 0.06051446869969368\n",
      "833 0.06046938896179199\n",
      "834 0.060424353927373886\n",
      "835 0.06037941202521324\n",
      "836 0.060334462672472\n",
      "837 0.060289546847343445\n",
      "838 0.06024469435214996\n",
      "839 0.060199905186891556\n",
      "840 0.06015513837337494\n",
      "841 0.060110390186309814\n",
      "842 0.06006568670272827\n",
      "843 0.0600210577249527\n",
      "844 0.059976473450660706\n",
      "845 0.05993190035223961\n",
      "846 0.059887390583753586\n",
      "847 0.05984288826584816\n",
      "848 0.05979849398136139\n",
      "849 0.05975408852100372\n",
      "850 0.05970972776412964\n",
      "851 0.059665415436029434\n",
      "852 0.05962115153670311\n",
      "853 0.05957692489027977\n",
      "854 0.05953274294734001\n",
      "855 0.05948863923549652\n",
      "856 0.05944453179836273\n",
      "857 0.05940047279000282\n",
      "858 0.059356432408094406\n",
      "859 0.059312473982572556\n",
      "860 0.05926850810647011\n",
      "861 0.059224653989076614\n",
      "862 0.05918080732226372\n",
      "863 0.059137020260095596\n",
      "864 0.059093233197927475\n",
      "865 0.05904952809214592\n",
      "866 0.05900581553578377\n",
      "867 0.05896218493580818\n",
      "868 0.05891859158873558\n",
      "869 0.058875009417533875\n",
      "870 0.05883149430155754\n",
      "871 0.05878802761435509\n",
      "872 0.058744583278894424\n",
      "873 0.05870119109749794\n",
      "874 0.05865785852074623\n",
      "875 0.058614540845155716\n",
      "876 0.05857124924659729\n",
      "877 0.05852801725268364\n",
      "878 0.058484818786382675\n",
      "879 0.05844170227646828\n",
      "880 0.05839857831597328\n",
      "881 0.058355510234832764\n",
      "882 0.05831250175833702\n",
      "883 0.05826950445771217\n",
      "884 0.05822654068470001\n",
      "885 0.05818362906575203\n",
      "886 0.058140769600868225\n",
      "887 0.0580979585647583\n",
      "888 0.058055147528648376\n",
      "889 0.058012381196022034\n",
      "890 0.05796968564391136\n",
      "891 0.05792700871825218\n",
      "892 0.057884398847818375\n",
      "893 0.05784177780151367\n",
      "894 0.05779928341507912\n",
      "895 0.05775675177574158\n",
      "896 0.05771428719162941\n",
      "897 0.05767187476158142\n",
      "898 0.05762948468327522\n",
      "899 0.05758709833025932\n",
      "900 0.057544831186532974\n",
      "901 0.057502564042806625\n",
      "902 0.05746030807495117\n",
      "903 0.05741814896464348\n",
      "904 0.057375963777303696\n",
      "905 0.057333871722221375\n",
      "906 0.05729177966713905\n",
      "907 0.0572497583925724\n",
      "908 0.05720775946974754\n",
      "909 0.057165805250406265\n",
      "910 0.057123880833387375\n",
      "911 0.057082030922174454\n",
      "912 0.05704016983509064\n",
      "913 0.05699837580323219\n",
      "914 0.05695658549666405\n",
      "915 0.056914906948804855\n",
      "916 0.05687320604920387\n",
      "917 0.05683154985308647\n",
      "918 0.056789934635162354\n",
      "919 0.056748393923044205\n",
      "920 0.05670686066150665\n",
      "921 0.056665364652872086\n",
      "922 0.05662393942475319\n",
      "923 0.05658252164721489\n",
      "924 0.05654115974903107\n",
      "925 0.05649981275200844\n",
      "926 0.05645853281021118\n",
      "927 0.056417256593704224\n",
      "928 0.05637602508068085\n",
      "929 0.05633487179875374\n",
      "930 0.05629372224211693\n",
      "931 0.05625258386135101\n",
      "932 0.056211549788713455\n",
      "933 0.05617053806781769\n",
      "934 0.05612952634692192\n",
      "935 0.056088563054800034\n",
      "936 0.05604766309261322\n",
      "937 0.05600676313042641\n",
      "938 0.05596594512462616\n",
      "939 0.055925119668245316\n",
      "940 0.05588437244296074\n",
      "941 0.055843617767095566\n",
      "942 0.05580295994877815\n",
      "943 0.05576232075691223\n",
      "944 0.0557217039167881\n",
      "945 0.055681128054857254\n",
      "946 0.055640578269958496\n",
      "947 0.05560007318854332\n",
      "948 0.05555962398648262\n",
      "949 0.055519189685583115\n",
      "950 0.05547882243990898\n",
      "951 0.05543845519423485\n",
      "952 0.055398136377334595\n",
      "953 0.055357858538627625\n",
      "954 0.05531761422753334\n",
      "955 0.05527740716934204\n",
      "956 0.05523724853992462\n",
      "957 0.055197104811668396\n",
      "958 0.05515703186392784\n",
      "959 0.05511695519089699\n",
      "960 0.0550769679248333\n",
      "961 0.05503697693347931\n",
      "962 0.054997045546770096\n",
      "963 0.05495714023709297\n",
      "964 0.05491723120212555\n",
      "965 0.0548773817718029\n",
      "966 0.05483759567141533\n",
      "967 0.05479782819747925\n",
      "968 0.054758112877607346\n",
      "969 0.05471846088767052\n",
      "970 0.05467879772186279\n",
      "971 0.054639142006635666\n",
      "972 0.0545995868742466\n",
      "973 0.05456005036830902\n",
      "974 0.05452054366469383\n",
      "975 0.05448107421398163\n",
      "976 0.054441649466753006\n",
      "977 0.054402247071266174\n",
      "978 0.054362908005714417\n",
      "979 0.05432354286313057\n",
      "980 0.05428426340222359\n",
      "981 0.054245006293058395\n",
      "982 0.05420579016208649\n",
      "983 0.05416661873459816\n",
      "984 0.054127488285303116\n",
      "985 0.05408836901187897\n",
      "986 0.0540492869913578\n",
      "987 0.05401024594902992\n",
      "988 0.053971271961927414\n",
      "989 0.053932275623083115\n",
      "990 0.05389338731765747\n",
      "991 0.05385446548461914\n",
      "992 0.05381559208035469\n",
      "993 0.053776782006025314\n",
      "994 0.05373799428343773\n",
      "995 0.05369925871491432\n",
      "996 0.053660523146390915\n",
      "997 0.053621839731931686\n",
      "998 0.05358320102095604\n",
      "999 0.05354458466172218\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    y_pred = model(x_data)\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(epoch, loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.4469061e-06]\n",
      " [5.7216093e-06]\n",
      " [7.3617025e-06]\n",
      " [9.4719135e-06]\n",
      " [1.2187015e-05]\n",
      " [1.5680380e-05]\n",
      " [2.0175088e-05]\n",
      " [2.5958123e-05]\n",
      " [3.3398799e-05]\n",
      " [4.2972188e-05]\n",
      " [5.5289529e-05]\n",
      " [7.1137205e-05]\n",
      " [9.1526810e-05]\n",
      " [1.1775998e-04]\n",
      " [1.5151087e-04]\n",
      " [1.9493316e-04]\n",
      " [2.5079673e-04]\n",
      " [3.2266468e-04]\n",
      " [4.1511847e-04]\n",
      " [5.3404883e-04]\n",
      " [6.8702933e-04]\n",
      " [8.8379288e-04]\n",
      " [1.1368445e-03]\n",
      " [1.4622457e-03]\n",
      " [1.8806108e-03]\n",
      " [2.4183863e-03]\n",
      " [3.1094621e-03]\n",
      " [3.9972300e-03]\n",
      " [5.1371539e-03]\n",
      " [6.6000060e-03]\n",
      " [8.4758652e-03]\n",
      " [1.0879049e-02]\n",
      " [1.3954017e-02]\n",
      " [1.7882423e-02]\n",
      " [2.2891078e-02]\n",
      " [2.9260827e-02]\n",
      " [3.7335318e-02]\n",
      " [4.7528863e-02]\n",
      " [6.0331043e-02]\n",
      " [7.6305449e-02]\n",
      " [9.6076913e-02]\n",
      " [1.2030422e-01]\n",
      " [1.4962952e-01]\n",
      " [1.8460310e-01]\n",
      " [2.2558251e-01]\n",
      " [2.7261758e-01]\n",
      " [3.2533967e-01]\n",
      " [3.8289040e-01]\n",
      " [4.4392315e-01]\n",
      " [5.0669652e-01]\n",
      " [5.6925952e-01]\n",
      " [6.2968653e-01]\n",
      " [6.8630803e-01]\n",
      " [7.3787600e-01]\n",
      " [7.8363907e-01]\n",
      " [8.2332546e-01]\n",
      " [8.5706037e-01]\n",
      " [8.8525152e-01]\n",
      " [9.0847635e-01]\n",
      " [9.2738611e-01]\n",
      " [9.4263566e-01]\n",
      " [9.5483857e-01]\n",
      " [9.6454328e-01]\n",
      " [9.7222316e-01]\n",
      " [9.7827715e-01]\n",
      " [9.8303461e-01]\n",
      " [9.8676425e-01]\n",
      " [9.8968261e-01]\n",
      " [9.9196273e-01]\n",
      " [9.9374217e-01]\n",
      " [9.9512959e-01]\n",
      " [9.9621046e-01]\n",
      " [9.9705231e-01]\n",
      " [9.9770749e-01]\n",
      " [9.9821728e-01]\n",
      " [9.9861395e-01]\n",
      " [9.9892247e-01]\n",
      " [9.9916232e-01]\n",
      " [9.9934882e-01]\n",
      " [9.9949384e-01]\n",
      " [9.9960655e-01]\n",
      " [9.9969423e-01]\n",
      " [9.9976224e-01]\n",
      " [9.9981529e-01]\n",
      " [9.9985635e-01]\n",
      " [9.9988842e-01]\n",
      " [9.9991322e-01]\n",
      " [9.9993253e-01]\n",
      " [9.9994755e-01]\n",
      " [9.9995923e-01]\n",
      " [9.9996829e-01]\n",
      " [9.9997544e-01]\n",
      " [9.9998093e-01]\n",
      " [9.9998510e-01]\n",
      " [9.9998844e-01]\n",
      " [9.9999106e-01]\n",
      " [9.9999297e-01]\n",
      " [9.9999464e-01]\n",
      " [9.9999583e-01]\n",
      " [9.9999678e-01]\n",
      " [9.9999750e-01]\n",
      " [9.9999797e-01]\n",
      " [9.9999845e-01]\n",
      " [9.9999881e-01]\n",
      " [9.9999905e-01]\n",
      " [9.9999928e-01]\n",
      " [9.9999940e-01]\n",
      " [9.9999952e-01]\n",
      " [9.9999964e-01]\n",
      " [9.9999976e-01]\n",
      " [9.9999976e-01]\n",
      " [9.9999988e-01]\n",
      " [9.9999988e-01]\n",
      " [9.9999988e-01]\n",
      " [9.9999988e-01]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "''' 模型测试 '''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(0, 10, 200)\n",
    "x_t = torch.Tensor(x).view(200,1) # 这里很重要！view类似于reshape\n",
    "# x转化为张量后是一个1*200的【行向量】，我们要转换为200行1列的【矩阵】作为输入！！\n",
    "# 输出也是矩阵形式\n",
    "y_t = model(x_t)\n",
    "y = y_t.data.numpy() # 注意y的结果是一个200行1列的矩阵，即200个行向量，对应200个输入的输出\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAgbElEQVR4nO3de3zcdZ3v8dcnl95b2jQ0lKalLS2XchVii1yWCIqArF1XVBB3FdetCHh7nMOK5+HR3XX3rJejKyrKoyKKu0q9oaKwcFx0BLlIabm10Et6S9JCkzRp08k9mc/5YyYwpEk6mcwvv5n5vZ+PRx4zv5nf75f3l6Hzye/y/X7N3RERkegqCTuAiIiES4VARCTiVAhERCJOhUBEJOJUCEREIq4s7ABjVVlZ6YsXL85q246ODqZPn57bQHlObY4GtTkaxtPmDRs2tLj7scO9V3CFYPHixTz99NNZbRuLxaitrc1toDynNkeD2hwN42mzme0Z6T2dGhIRiTgVAhGRiFMhEBGJOBUCEZGIUyEQEYm4wAqBmd1lZk1mtmmE983MvmFmdWb2vJmdE1QWEREZWZBHBD8ALh/l/SuA5amfNcB3AswiIiIjCKwfgbs/YmaLR1llNfBDT46D/aSZzTaz+e7+clCZpHC5O+3d/bR29NLR009P/wBdvQm6+wbo7h+gq3eAgYSTcNhS30f9E7tfXU4knIQ7A+64w0Bi5KHXRxuV3Rn7dqMO8p7DIeB37+llY+/WnO2vEESxzZPa+6kNYL9hdihbADSkLTemXjuiEJjZGpJHDVRVVRGLxbL6hfF4POttC1UhtTnhzt64s6d9gH1x5+WOBK3dTnuP097rDIzle/PFzYHlzCXL2Z4cdtTlbG+FIXptfku1B/LvOcxCMNy/gWH/qbv7WmAtQE1NjWfbs049EfNPR08/D256hT9sbeKJHQc40NELQHmpsXjudJbOn0rljMlUzpzM3OmTmDtjEtMmlTG1vJQp5aVMKS9hankpk8tKKSs1SkuMJ554nIsuuIASM0pKjBKD0hJLLlty2Wzkr+DRvpxH2WzUfQYt3z/nIKjNuRNmIWgEFqYtVwP7QsoiE+yVQ92sfWQnP1lfT0fvAPNmTuYvTjqW80+cyzknzGFRxTTKS7O7hDV7cglzZ0zOcWKR4hVmIbgPuNnM1gGrgEO6PlD8+gYSfPfRnXzj4e30DTjvOOt43rdqETUnzAn1L2qRKAusEJjZPUAtUGlmjcDngXIAd78DeAC4EqgDOoHrg8oi+WF/ezc3/mgjG/a0cdmKKj779hUsmjst7FgikRfkXUPXHuV9B24K6vdLfqlrivO+7z5JvKef2645m9VnLwg7koikFNww1FJ46priXLP2SQDuvfF8TjluVsiJRCSdCoEE6lBXH3//w6cBZ92a81g2b2bYkURkCBUCCYy786mfPEtDa6eKgEge06BzEpifPt3A77c08dm3n0rN4oqw44jICFQIJBDNh3v41/tfYuWSCv72TYvDjiMio1AhkEB8+cEtdPcl+D/vPIOSEvUPEMlnKgSSc7taOrj3mb1cd94ils2bEXYcETkKFQLJuW8+vJ3yUuOjtSeGHUVEMqBCIDnV0NrJr57dy/tXncC8mVPCjiMiGVAhkJz68VP1AHzowiUhJxGRTKkQSM709A/w0/UNXHpqFcfPnhp2HBHJkAqB5MyDm17hQEcv161aFHYUERkDFQLJmZ9vaKR6zlT+YvmxYUcRkTFQIZCcaOvo5fEdB7jqzOPVb0CkwKgQSE48tPkVBhLO28+YH3YUERkjFQLJiftfeJlFFdM4fYGGmBYpNCoEMm4HO5Onha48Y76mmxQpQCoEMm6Pbm9hIOFcdlpV2FFEJAsqBDJuj2xrZtaUMs6qnh12FBHJggqBjIu78+j2Fi5cXkmp7hYSKUgqBDIudU1xXmnv5iL1HRApWCoEMi6PbG8B4MJllSEnEZFsqRDIuDxW18KSyuksrJgWdhQRyZIKgWQtkXA27GljpeYjFiloKgSStR3NcQ519XHu4jlhRxGRcVAhkKw9vacNgJoTVAhECpkKgWRtw542KqZPYknl9LCjiMg4qBBI1jbsaeOcRXM0rIRIgVMhkKy0xHvY1dJBja4PiBQ8FQLJyvONBwF4w8LZoeYQkfFTIZCsbNrbDsBpC44JOYmIjFeghcDMLjezrWZWZ2a3DvP+MWb2GzN7zsw2m9n1QeaR3Nm09xBLK6czY3JZ2FFEZJwCKwRmVgrcDlwBrACuNbMVQ1a7CXjR3c8CaoGvmtmkoDJJ7mze166jAZEiEeQRwUqgzt13unsvsA5YPWQdB2Za8raTGUAr0B9gJsmB1o5e9h7s4vTjNRuZSDEI8rh+AdCQttwIrBqyzreA+4B9wEzgve6eGLojM1sDrAGoqqoiFotlFSgej2e9baEKos2bWgYAGGjZTSzWcJS1J54+52hQm3MnyEIw3M3lPmT5bcCzwCXAicDvzOxRd29/3Ubua4G1ADU1NV5bW5tVoFgsRrbbFqog2vxSbAewhfddcRGzp+XfmTx9ztGgNudOkKeGGoGFacvVJP/yT3c9cK8n1QG7gFMCzCQ5sGnfIarnTM3LIiAiYxdkIVgPLDezJakLwNeQPA2Urh64FMDMqoCTgZ0BZpIc2PJyO6fO1/UBkWIRWCFw937gZuAh4CXgp+6+2cxuMLMbUqt9ATjfzF4AHgY+7e4tQWWS8evpH2D3gU5OrpoZdhQRyZFAbwJ39weAB4a8dkfa833AZUFmkNza1dLBQMJZXjUj7CgikiPqWSxjsvWVwwCcpCMCkaKhQiBjsn1/nNISY+mxGnpapFioEMiYbNt/mMVzpzG5rDTsKCKSIyoEMibbm+I6LSRSZFQIJGPdfQPsPtChQiBSZFQIJGN1TXHcdaFYpNioEEjGdjTHAVg2T7eOihQTFQLJ2M7mDszghLnTwo4iIjmkQiAZ29XSQfWcqUwp1x1DIsVkTIXAzOaY2ZlBhZH8tqulgyWVOi0kUmyOWgjMLGZms8ysAngO+L6ZfS34aJJP3J2dzXGWVqojmUixyeSI4JjU/AB/DXzf3c8F3hJsLMk3zYd76OgdUI9ikSKUSSEoM7P5wHuA3wacR/LUjuYOAJboiECk6GRSCP6Z5FDSde6+3syWAtuDjSX5ZldLshAsPVbXCESKzVGHoXb3nwE/S1veCbwryFCSf3Y2x5lcVsL8WVPCjiIiOZbJxeIvpy4Wl5vZw2bWYmbvn4hwkj+SdwxNp6RkuKmoRaSQZXJq6LLUxeKrSM5DfBJwS6CpJO/sOtCh6wMiRSqTQlCeerwSuMfdWwPMI3loIOE0tnaxSD2KRYpSJlNV/sbMtgBdwI1mdizQHWwsySevtHfTO5DghAodEYgUo6MeEbj7rcCbgBp37wM6gNVBB5P8sedA8o4hjTEkUpwynbx+AfBWM0u/ZeSHAeSRPFR/oBOARRUqBCLF6KiFwMw+D9QCK4AHgCuAP6FCEBl7WjspLzWOnz017CgiEoBMLhZfDVwKvOLu1wNnAZMDTSV5pf5AJ9VzplGqW0dFilImhaDL3RNAv5nNApqApcHGknyyp7VDp4VEilgm1wieNrPZwHeBDUAceCrIUJI/3J09Bzo5Z9GcsKOISEBGLQSpW0XvAnD3O8zsQWCWuz8/EeEkfAc7+zjc3a8jApEiNuKpITP7MLAZ+Cawxcze4e67VQSiZU9r8o6hE+aqD4FIsRrtiOCTwGnu3pwacfRHwH0TkkryRn2rbh0VKXajXSzudfdmeHXEUd0pFEGNbclCUD1Ht46KFKvRjgiqzewbIy27+8eDiyX5orGti4rpk5g+OdO+hyJSaEb71z10hNENY925mV0O3AaUAne6+xeHWacW+DrJwe1a3P3isf4eCU5jW5eOBkSK3IiFwN3vHs+OzawUuB14K8nhq9eb2X3u/mLaOrOBbwOXu3u9mc0bz++U3Gts6+SU42aGHUNEApRJh7JsrSQ5veVOd+8F1nHkYHXvA+5193oAd28KMI+Mkbuzt62L6jm6UCxSzII88bsAaEhbbgRWDVnnJKDczGLATOA2dz9iDCMzWwOsAaiqqiIWi2UVKB6PZ71toRpPmw/2JOjpT9DZ3Egstj+3wQKkzzka1ObcGbEQmNmX3P3TZvbu1LzFYzXcwDQ+zO8/l+RYRlOBJ8zsSXff9rqN3NcCawFqamq8trY2izgQi8XIdttCNZ42b6xvgz88ziWrzqT2lKrcBguQPudoUJtzZ7RTQ1eaWTnwmSz33QgsTFuuBvYNs86D7t7h7i3AIyQHtZM80NjWBaBTQyJFbrRC8CDQApxpZu1mdjj9MYN9rweWm9kSM5sEXMORHdJ+DVxkZmVmNo3kqaOXsmiHBGCwD8ECDT8tUtRGLATufou7HwPc7+6z3H1m+uPRduzu/cDNwEMkv9x/6u6bzewGM7shtc5LJAvO8yQHsrvT3TfloF2SA+pDIBINR/0X7u6rzawKeGPqpT8P9jjOYNsHSE5mk/7aHUOWvwJ8JbO4MpHUh0AkGo56+6iZvZvkX+vvBt4DPGVmVwcdTMLX2NapQiASAZkc838WeOPgPf6poan/G/h5kMEkXIN9CN5yauHcLSQi2cmkQ1nJkI5eBzLcTgpYc7yHnv6EjghEIiCTI4IHzewh4J7U8nsZct5fis9rt46qEIgUu0wuFt9iZn8NXEiyk9had/9l4MkkVOpDIBIdGd0X6O73AvcGnEXyiPoQiESHzvXLsNSHQCQ6VAhkWOpDIBIdmfQjuMrMVDAiRn0IRKIjky/4a4DtZvZlMzs16EASPs1DIBItRy0E7v5+4A3ADuD7ZvaEma0xM01bVaTUh0AkWjI65ePu7cAvSM4yNh94J7DRzD4WYDYJyeCto7pjSCQaMrlG8A4z+yXwe5ITzK909ytIzhvwPwPOJyFQHwKRaMnk3sCrgX9390fSX3T3TjP7UDCxAvDJT3J2LAazZ4edZEKdffDgmNt87sEu1rV2suzxCigZbqK5/JZNmwud2hwNyyorYYJnKBv08tAiYGZfAnD3h3OeSELX05+grLSE0gIsAiIydpkcEbwV+PSQ164Y5rX89vWv82wE5zjNps3/eNdTtHX08puPXRhMqIDpc46GKLa5LhajOoD9jjZ5/UeBG4ETzez5tLdmAo8FkEXyRGNbJydX6aYwkagY7Yjgx8B/Af8G3Jr2+mF3bw00lYRmsA/BpafMCzuKiEyQ0QqBu/tuM7tp6BtmVqFiUJxa4r2pPgS6Y0gkKo52RHAVsAFwkkNQD3JgaYC5JCSDo46qM5lIdIxYCNz9qtTjkomLI2FTHwKR6BntYvE5o23o7htzH0fC9mqvYh0RiETGaKeGvjrKew5ckuMskgca2zqZM62cGZqHQCQyRjs19OaJDCL5oVGjjopEzminhi5x99+n5is+Qmr6SikyjW2dnKQ+BCKRMtrx/8UkB5r7y2HeczSHcdFJJJzGti4uUR8CkUgZ7dTQ51OP109cHAnT4DwEiyp0akgkSjIZhnqumX3DzDaa2QYzu83M5k5EOJlYDa2pPgQqBCKRksnoo+uAZuBdJIekbgZ+EmQoCUdDqjPZQl0sFomUTO4RrHD3L6Qt/4uZ/VVAeSRE9QcGO5OpD4FIlGRyRPAHM7vGzEpSP+8B7g86mEy8hrZOqmZNZkp5adhRRGQCjVgIzOywmbUDHyE57lBv6mcd8KlMdm5ml5vZVjOrM7NbR1nvjWY2YGZXjy2+5FJ9a6cuFItE0IiFwN1nuvus1GOJu5elfkrcfdbRdmxmpcDtJCexWQFca2YrRljvS8BD2TdDcqGxtVPXB0QiKKNxBMxsDrAcmDL42tDpK4exEqhz952pfawDVgMvDlnvY8AvgDdmmFkC0Nuf4OX2bhbqiEAkco5aCMzsw8AngGrgWeA84AmOPtbQAqAhbbkRWDVk3wuAd6b2NWIhMLM1wBqAqqoqYrHY0WIPKx6PZ71tocq0za90JHCHjqY9xGL7gg8WIH3O0aA2504mRwSfIPkl/aS7v9nMTgH+KYPthpv53Icsfx34tLsPmI08Ubq7rwXWAtTU1Hi285TGIjjHaaZtfmRbMzz6FJedfy4rl1QEHyxA+pyjQW3OnUwKQbe7d5sZZjbZ3beY2ckZbNcILExbrgaG/qlZA6xLFYFK4Eoz63f3X2Wwf8mh+lRnsoUVunVUJGoyKQSNZjYb+BXwOzNr48gv9OGsB5ab2RJgL3AN8L70FdInvTGzHwC/VREIR0NbJ5NKS6iaOeXoK4tIUTlqIXD3d6ae/qOZ/QE4Bngwg+36zexmkncDlQJ3uftmM7sh9f4d2ceWXGto7aR6zlRKSkY+RScixSnTu4bOAS4keY7/MXfvzWQ7d38AeGDIa8MWAHf/YCb7lGA0tHZpjCGRiMpk0LnPAXcDc0mex/++mX026GAysRraOlmk6wMikZTJEcG1wBvcvRvAzL4IbAT+JchgMnHau/s42NmnzmQiEZXJWEO7SetIBkwGdgSSRkIxOPy0hpcQiabRpqr8JslrAj3AZjP7XWr5rcCfJiaeTISGV28dVSEQiaLRTg09nXrcAPwy7fVYYGkkFA2tyeGndWpIJJpGm6ry7sHnZjYJOCm1uNXd+4IOJhOnoa2TWVPKOGZaedhRRCQEmYw1VEvyrqHdJIeNWGhmH8hg0DkpEPWtnTotJBJhmdw19FXgMnffCmBmJwH3AOcGGUwmTkNrJydVzQw7hoiEJJO7hsoHiwCAu28DdA6hSCQSTkNbl44IRCIskyOCDWb2PeA/UsvXkbyALEXg5fZuevsTLJ47PewoIhKSTArBDcBNwMdJXiN4BPh2kKFk4uxq7gBgSaUKgUhUjVoIzKwE2ODupwNfm5hIMpF2HVAhEIm6Ua8RuHsCeM7MFk1QHplgu5o7mFpeStWsyWFHEZGQZHJqaD7JnsVPAR2DL7r7OwJLJRNm94EOFldOZ7QZ4kSkuGVSCDKZllIK1K6WDlbMnxV2DBEJ0WhjDU0heaF4GfAC8D1375+oYBK8voEEDa2dXHnGcWFHEZEQjXaN4G6Scwq/AFxBsmOZFJHGti76E86SyhlhRxGREI12amiFu58BkOpH8NTERJKJsqslDsCSSnUmE4my0Y4IXh1YTqeEitOuluTw0zoiEIm20Y4IzjKz9tRzA6amlg1wd9cVxgK3oznO7GnlzNGooyKRNtow1KUTGUQmXt3+OCfNm6lbR0UiLpNB56QIuTvbmg6zrEqnhUSiToUgolrivRzs7GP5PBUCkahTIYio7U2HAVg+T/MQiESdCkFE1TUlbx1drlNDIpGnQhBR2/fHmTmljHkzNdicSNSpEETU9qbDLJ83Q3cMiYgKQVTVNcV1fUBEABWCSDoQ76El3qvrAyICqBBE0uZ9yQ7jK45X53ARCbgQmNnlZrbVzOrM7NZh3r/OzJ5P/TxuZmcFmUeSXnw5VQg0D4GIEGAhMLNS4HaSQ1ivAK41sxVDVtsFXOzuZwJfANYGlUdes3lfOwtmT2X2tElhRxGRPBDkEcFKoM7dd7p7L7AOWJ2+grs/7u5tqcUngeoA80jKi/sO6bSQiLwqk6kqs7UAaEhbbgRWjbL+3wH/NdwbZrYGWANQVVVFLBbLKlA8Hs9620I1tM09/c7O5k5On9VbtP8t9DlHg9qcO0EWguFuUPdhVzR7M8lCcOFw77v7WlKnjWpqary2tjarQLFYjGy3LVRD27yxvg3/78e58vwzqT2tOKeo1OccDWpz7gRZCBqBhWnL1cC+oSuZ2ZnAncAV7n4gwDwCvLhPF4pF5PWCvEawHlhuZkvMbBJwDXBf+gpmtgi4F/gbd98WYBZJ2bT3ELOmlFE9Z2rYUUQkTwR2RODu/WZ2M/AQUArc5e6bzeyG1Pt3AJ8D5gLfTg110O/uNUFlEnim/iBnL5qjoSVE5FVBnhrC3R8AHhjy2h1pzz8MfDjIDPKa9u4+tjUd5oozivPagIhkRz2LI+T5hkO4wzmL5oQdRUTyiApBhGysT3bZOGvh7HCDiEheUSGIkI31bSyfN4NjppaHHUVE8ogKQUS4O8/UH9RpIRE5ggpBRGxvinOoq49zTpgddhQRyTMqBBHxWF0LAOefWBlyEhHJNyoEEfFY3QEWVUxjYcW0sKOISJ5RIYiA/oEEf955gAuW6WhARI6kQhABz+89xOGefi5YNjfsKCKSh1QIIuCx7bo+ICIjUyGIgIe3NHHGgmOomK4ZyUTkSCoERa6tO8GzDQe5/HSNLyQiw1MhKHIb9g8A8LbTqkJOIiL5SoWgyG3Y38+Jx05n2byZYUcRkTylQlDEWjt62dqW0GkhERmVCkER+9Uze0k4XHXm8WFHEZE8pkJQpNyddevrWXpMCadqfmIRGYUKQZF6puEg2/bHubg60EnoRKQIqBAUqR//uZ5pk0pZOV+FQERGp0JQhPYe7OJXz+zl6nOrmVqmSepFZHQqBEVo7R93YAYfufjEsKOISAFQISgy+9u7uWd9A+86p5oFs6eGHUdECoAKQZH51/tfAuDG2mUhJxGRQqFCUEQer2vhvuf28dGLT2TRXE1AIyKZUSEoEu3dfdx67wssqpjGR2t1bUBEMqd7C4uAu3PLz55j38EufvKR85hSXhp2JBEpIDoiKHDuzpce3MpDm/fzD5efzLknVIQdSUQKjApBAXN3/v1327jjjzu4btUi/v6ipWFHEpECpFNDBaqzt5/P3PsCv352H+8+t5ovrD4dM3UeE5GxUyEoMO7OH7c1879/vYmG1i5uedvJ3Fh7ooqAiGRNhaBA9A8k+P2WJu780y6e2tXK0srp/GTNeaxaOjfsaCJS4AItBGZ2OXAbUArc6e5fHPK+pd6/EugEPujuG4PMVCjcnT0HOnm24SBP7DjAw1v20xLv5bhZU/jn1afx3jcuZHKZ7g4SkfELrBCYWSlwO/BWoBFYb2b3ufuLaatdASxP/awCvpN6LDruTk9/gp7+BL39Cbr7BjjU1cehrj4OdvbR1tlL8+EeGto6aWztYnvTYdo6+wCYObmM2lPm8ZdnzueSU+ZRVqpr/CKSO0EeEawE6tx9J4CZrQNWA+mFYDXwQ3d34Ekzm21m89395VyHiW1t4jOPdjJ1Qww8+ZqT/IJOPoLjycfB91NPjnifwXXSl4esm7bf3v4EvQOJo2Y0g+NmTWFhxTQuW3EcZy2czdkLZ3NS1Qx9+YtIYIIsBAuAhrTlRo78a3+4dRYArysEZrYGWANQVVVFLBYbc5i6tgGOm5qgvLQ7uc9X981rywaGve699Euw6eu+tv7rn7+6btr2ZSWllJeUUl4C5aWWfCyBaeXG9NTPjHKYMckoLzGgJ/nT1UrTNmjaNubmvioej2f136uQqc3RoDbnTpCFYLjbWDyLdXD3tcBagJqaGq+trR1zmFpgWSxGNtsWspjaHAlqczQE1eYgzzc0AgvTlquBfVmsIyIiAQqyEKwHlpvZEjObBFwD3DdknfuAv7Wk84BDQVwfEBGRkQV2asjd+83sZuAhkreP3uXum83shtT7dwAPkLx1tI7k7aPXB5VHRESGF2g/And/gOSXffprd6Q9d+CmIDOIiMjodE+iiEjEqRCIiEScCoGISMSpEIiIRJwNDo1QKMysGdiT5eaVQEsO4xQCtTka1OZoGE+bT3D3Y4d7o+AKwXiY2dPuXhN2jomkNkeD2hwNQbVZp4ZERCJOhUBEJOKiVgjWhh0gBGpzNKjN0RBImyN1jUBERI4UtSMCEREZQoVARCTiIlMIzOxyM9tqZnVmdmvYeYJmZgvN7A9m9pKZbTazT4SdaSKYWamZPWNmvw07y0RJTfH6czPbkvq83xR2piCZ2adS/09vMrN7zGxK2JmCYGZ3mVmTmW1Ke63CzH5nZttTj3Ny8bsiUQjMrBS4HbgCWAFca2Yrwk0VuH7gf7j7qcB5wE0RaDPAJ4CXwg4xwW4DHnT3U4CzKOL2m9kC4ONAjbufTnKI+2vCTRWYHwCXD3ntVuBhd18OPJxaHrdIFAJgJVDn7jvdvRdYB6wOOVOg3P1ld9+Yen6Y5JfDgnBTBcvMqoG3A3eGnWWimNks4C+A7wG4e6+7Hww1VPDKgKlmVgZMo0hnNXT3R4DWIS+vBu5OPb8b+Ktc/K6oFIIFQEPaciNF/qWYzswWA28A/hxylKB9HfgHIBFyjom0FGgGvp86JXanmU0PO1RQ3H0v8H+BeuBlkrMa/r9wU02oqsFZHFOP83Kx06gUAhvmtUjcN2tmM4BfAJ909/aw8wTFzK4Cmtx9Q9hZJlgZcA7wHXd/A9BBjk4X5KPUOfHVwBLgeGC6mb0/3FSFLyqFoBFYmLZcTZEeTqYzs3KSReBH7n5v2HkCdgHwDjPbTfLU3yVm9p/hRpoQjUCjuw8e7f2cZGEoVm8Bdrl7s7v3AfcC54ecaSLtN7P5AKnHplzsNCqFYD2w3MyWmNkkkheX7gs5U6DMzEieN37J3b8Wdp6guftn3L3a3ReT/Hx/7+5F/5eiu78CNJjZyamXLgVeDDFS0OqB88xsWur/8Usp4ovjw7gP+EDq+QeAX+dip4HOWZwv3L3fzG4GHiJ5l8Fd7r455FhBuwD4G+AFM3s29dr/Ss0jLcXlY8CPUn/k7ASuDzlPYNz9z2b2c2AjyTvjnqFIh5ows3uAWqDSzBqBzwNfBH5qZn9Hsii+Oye/S0NMiIhEW1RODYmIyAhUCEREIk6FQEQk4lQIREQiToVARCTiVAhEhmFm8SHLHzSzb4WVRyRIKgQiEyg1Eq5IXlEhEBkjMzvBzB42s+dTj4tSr//AzK5OWy+eeqxNzQ3xY5Id/Kab2f1m9lxqTP33htQUESAiPYtFsjA1rUc2QAWvDUvyLeCH7n63mX0I+AZHHw54JXC6u+8ys3cB+9z97QBmdkxOk4uMkY4IRIbX5e5nD/4An0t7703Aj1PP/wO4MIP9PeXuu1LPXwDeYmZfMrOL3P1QzlKLZEGFQGT8Bsdp6Sf1byo1INqktHU6Xl3ZfRtwLsmC8G9mll5kRCacCoHI2D3Oa9MjXgf8KfV8N8kveEiOmV8+3MZmdjzQ6e7/SXKSlWIeNloKgK4RiIzdx4G7zOwWkrODDY72+V3g12b2FMn5ZDtG2P4M4CtmlgD6gI8GnFdkVBp9VEQk4nRqSEQk4lQIREQiToVARCTiVAhERCJOhUBEJOJUCEREIk6FQEQk4v4/0jgjDLnRhlUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, y)\n",
    "plt.plot([0,10], [0.5,0.5], c='r')\n",
    "plt.xlabel('Hours')\n",
    "plt.ylabel('Probability of Pass')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多维特征输入的处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 简单Logistic实现的8维输入的二分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = torch.nn.Linear(8, 1) # Linear Unit中输入数据为8维，输出为标量\n",
    "        self.sigmoid = torch.nn.Sigmoid() # 这里Sigmoid依然是一个可以当作函数被call的类\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.linear(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 人工神经网络（糖尿病二分类）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.loadtxt是可以解析.gz的，不一定非要解压成.csv文件\n",
    "xy = np.loadtxt('./datasets/diabetes.csv.gz', delimiter=',', dtype=np.float32)\n",
    "# xy为矩阵，每个行向量前面为特征，最后一列为类别（0/1）\n",
    "# 神经网络一般都用32位浮点数，游戏显卡一般只支持32位浮点数\n",
    "\n",
    "x_data = torch.Tensor(xy[:, :-1]) # 或者可以用torch.from_numpy: ndarray -> Tensor\n",
    "y_data = torch.Tensor(xy[:, [-1]])\n",
    "# 注意！这里切片的时候加上[-1]的意义在于这是一个【矩阵】，只用-1的话就是一个【列向量】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(8, 6)\n",
    "        self.linear2 = torch.nn.Linear(6, 4)\n",
    "        self.linear3 = torch.nn.Linear(4, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid() \n",
    "        # Sigmoid类也是继承自Module，但是没有参数成员，所以梯度下降时没有训练的\n",
    "        # 这里不用纯函数，而是用类，意义在于激活函数也看作神经网络计算图的一层\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.linear1(x))\n",
    "        x = self.sigmoid(self.linear2(x))\n",
    "        x = self.sigmoid(self.linear3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss(size_average=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Loss:0.312819242477417\n",
      "Epoch:1 Loss:0.3128189146518707\n",
      "Epoch:2 Loss:0.3128180503845215\n",
      "Epoch:3 Loss:0.31281760334968567\n",
      "Epoch:4 Loss:0.31281769275665283\n",
      "Epoch:5 Loss:0.3128177523612976\n",
      "Epoch:6 Loss:0.3128169775009155\n",
      "Epoch:7 Loss:0.3128168284893036\n",
      "Epoch:8 Loss:0.3128160536289215\n",
      "Epoch:9 Loss:0.3128163516521454\n",
      "Epoch:10 Loss:0.3128156363964081\n",
      "Epoch:11 Loss:0.3128156363964081\n",
      "Epoch:12 Loss:0.3128150403499603\n",
      "Epoch:13 Loss:0.3128148019313812\n",
      "Epoch:14 Loss:0.3128136098384857\n",
      "Epoch:15 Loss:0.3128134310245514\n",
      "Epoch:16 Loss:0.31281280517578125\n",
      "Epoch:17 Loss:0.31281211972236633\n",
      "Epoch:18 Loss:0.3128105401992798\n",
      "Epoch:19 Loss:0.3128097653388977\n",
      "Epoch:20 Loss:0.3128097951412201\n",
      "Epoch:21 Loss:0.3128076195716858\n",
      "Epoch:22 Loss:0.31280580163002014\n",
      "Epoch:23 Loss:0.31280508637428284\n",
      "Epoch:24 Loss:0.31280267238616943\n",
      "Epoch:25 Loss:0.31280142068862915\n",
      "Epoch:26 Loss:0.312798410654068\n",
      "Epoch:27 Loss:0.3127945065498352\n",
      "Epoch:28 Loss:0.3127897381782532\n",
      "Epoch:29 Loss:0.31278300285339355\n",
      "Epoch:30 Loss:0.3127736449241638\n",
      "Epoch:31 Loss:0.31276512145996094\n",
      "Epoch:32 Loss:0.3127554953098297\n",
      "Epoch:33 Loss:0.3127458989620209\n",
      "Epoch:34 Loss:0.3127377927303314\n",
      "Epoch:35 Loss:0.31272971630096436\n",
      "Epoch:36 Loss:0.3127244710922241\n",
      "Epoch:37 Loss:0.31271907687187195\n",
      "Epoch:38 Loss:0.3127143979072571\n",
      "Epoch:39 Loss:0.31271225214004517\n",
      "Epoch:40 Loss:0.3127106726169586\n",
      "Epoch:41 Loss:0.31270846724510193\n",
      "Epoch:42 Loss:0.3127061724662781\n",
      "Epoch:43 Loss:0.312702476978302\n",
      "Epoch:44 Loss:0.3127003312110901\n",
      "Epoch:45 Loss:0.3126973807811737\n",
      "Epoch:46 Loss:0.31269556283950806\n",
      "Epoch:47 Loss:0.31269463896751404\n",
      "Epoch:48 Loss:0.3126915991306305\n",
      "Epoch:49 Loss:0.3126905560493469\n",
      "Epoch:50 Loss:0.31268903613090515\n",
      "Epoch:51 Loss:0.31268611550331116\n",
      "Epoch:52 Loss:0.3126852810382843\n",
      "Epoch:53 Loss:0.3126821517944336\n",
      "Epoch:54 Loss:0.31268078088760376\n",
      "Epoch:55 Loss:0.31268036365509033\n",
      "Epoch:56 Loss:0.31267765164375305\n",
      "Epoch:57 Loss:0.3126760423183441\n",
      "Epoch:58 Loss:0.3126735985279083\n",
      "Epoch:59 Loss:0.31267163157463074\n",
      "Epoch:60 Loss:0.312671035528183\n",
      "Epoch:61 Loss:0.31266945600509644\n",
      "Epoch:62 Loss:0.31266745924949646\n",
      "Epoch:63 Loss:0.31266623735427856\n",
      "Epoch:64 Loss:0.31266453862190247\n",
      "Epoch:65 Loss:0.31266361474990845\n",
      "Epoch:66 Loss:0.3126620352268219\n",
      "Epoch:67 Loss:0.31265974044799805\n",
      "Epoch:68 Loss:0.3126588463783264\n",
      "Epoch:69 Loss:0.31265613436698914\n",
      "Epoch:70 Loss:0.312655508518219\n",
      "Epoch:71 Loss:0.3126540780067444\n",
      "Epoch:72 Loss:0.3126528561115265\n",
      "Epoch:73 Loss:0.3126513361930847\n",
      "Epoch:74 Loss:0.31264960765838623\n",
      "Epoch:75 Loss:0.31264859437942505\n",
      "Epoch:76 Loss:0.31264761090278625\n",
      "Epoch:77 Loss:0.31264737248420715\n",
      "Epoch:78 Loss:0.31264612078666687\n",
      "Epoch:79 Loss:0.3126436471939087\n",
      "Epoch:80 Loss:0.3126430809497833\n",
      "Epoch:81 Loss:0.31264305114746094\n",
      "Epoch:82 Loss:0.312641441822052\n",
      "Epoch:83 Loss:0.31264084577560425\n",
      "Epoch:84 Loss:0.3126397132873535\n",
      "Epoch:85 Loss:0.31263959407806396\n",
      "Epoch:86 Loss:0.31263887882232666\n",
      "Epoch:87 Loss:0.3126381039619446\n",
      "Epoch:88 Loss:0.3126371502876282\n",
      "Epoch:89 Loss:0.31263649463653564\n",
      "Epoch:90 Loss:0.31263625621795654\n",
      "Epoch:91 Loss:0.3126349449157715\n",
      "Epoch:92 Loss:0.3126333951950073\n",
      "Epoch:93 Loss:0.3126334846019745\n",
      "Epoch:94 Loss:0.3126329779624939\n",
      "Epoch:95 Loss:0.3126310110092163\n",
      "Epoch:96 Loss:0.31263071298599243\n",
      "Epoch:97 Loss:0.3126300275325775\n",
      "Epoch:98 Loss:0.3126290738582611\n",
      "Epoch:99 Loss:0.31262779235839844\n",
      "Epoch:100 Loss:0.31262722611427307\n",
      "Epoch:101 Loss:0.3126268684864044\n",
      "Epoch:102 Loss:0.3126263916492462\n",
      "Epoch:103 Loss:0.312626451253891\n",
      "Epoch:104 Loss:0.3126250207424164\n",
      "Epoch:105 Loss:0.3126235008239746\n",
      "Epoch:106 Loss:0.3126239478588104\n",
      "Epoch:107 Loss:0.3126232624053955\n",
      "Epoch:108 Loss:0.31262269616127014\n",
      "Epoch:109 Loss:0.3126220405101776\n",
      "Epoch:110 Loss:0.3126204013824463\n",
      "Epoch:111 Loss:0.3126201629638672\n",
      "Epoch:112 Loss:0.3126199245452881\n",
      "Epoch:113 Loss:0.31261926889419556\n",
      "Epoch:114 Loss:0.31261858344078064\n",
      "Epoch:115 Loss:0.31261777877807617\n",
      "Epoch:116 Loss:0.31261783838272095\n",
      "Epoch:117 Loss:0.3126172125339508\n",
      "Epoch:118 Loss:0.3126166760921478\n",
      "Epoch:119 Loss:0.31261610984802246\n",
      "Epoch:120 Loss:0.31261542439460754\n",
      "Epoch:121 Loss:0.31261488795280457\n",
      "Epoch:122 Loss:0.3126140832901001\n",
      "Epoch:123 Loss:0.31261229515075684\n",
      "Epoch:124 Loss:0.31261128187179565\n",
      "Epoch:125 Loss:0.3126109838485718\n",
      "Epoch:126 Loss:0.31261059641838074\n",
      "Epoch:127 Loss:0.31261029839515686\n",
      "Epoch:128 Loss:0.3126099407672882\n",
      "Epoch:129 Loss:0.31260937452316284\n",
      "Epoch:130 Loss:0.3126089572906494\n",
      "Epoch:131 Loss:0.31260839104652405\n",
      "Epoch:132 Loss:0.31260809302330017\n",
      "Epoch:133 Loss:0.3126075565814972\n",
      "Epoch:134 Loss:0.31260719895362854\n",
      "Epoch:135 Loss:0.31260672211647034\n",
      "Epoch:136 Loss:0.3126053214073181\n",
      "Epoch:137 Loss:0.3126049339771271\n",
      "Epoch:138 Loss:0.31260451674461365\n",
      "Epoch:139 Loss:0.3126039206981659\n",
      "Epoch:140 Loss:0.3126033544540405\n",
      "Epoch:141 Loss:0.3126026690006256\n",
      "Epoch:142 Loss:0.31260234117507935\n",
      "Epoch:143 Loss:0.31260189414024353\n",
      "Epoch:144 Loss:0.3126014769077301\n",
      "Epoch:145 Loss:0.3126012086868286\n",
      "Epoch:146 Loss:0.3126007914543152\n",
      "Epoch:147 Loss:0.3126002252101898\n",
      "Epoch:148 Loss:0.31259962916374207\n",
      "Epoch:149 Loss:0.31259971857070923\n",
      "Epoch:150 Loss:0.3125990927219391\n",
      "Epoch:151 Loss:0.3125983476638794\n",
      "Epoch:152 Loss:0.31259700655937195\n",
      "Epoch:153 Loss:0.3125965893268585\n",
      "Epoch:154 Loss:0.31259608268737793\n",
      "Epoch:155 Loss:0.31259554624557495\n",
      "Epoch:156 Loss:0.31259506940841675\n",
      "Epoch:157 Loss:0.3125946819782257\n",
      "Epoch:158 Loss:0.31259405612945557\n",
      "Epoch:159 Loss:0.3125934898853302\n",
      "Epoch:160 Loss:0.3125929534435272\n",
      "Epoch:161 Loss:0.3125928044319153\n",
      "Epoch:162 Loss:0.31259238719940186\n",
      "Epoch:163 Loss:0.3125919699668884\n",
      "Epoch:164 Loss:0.3125914931297302\n",
      "Epoch:165 Loss:0.3125910758972168\n",
      "Epoch:166 Loss:0.31259074807167053\n",
      "Epoch:167 Loss:0.31259027123451233\n",
      "Epoch:168 Loss:0.3125896751880646\n",
      "Epoch:169 Loss:0.3125893771648407\n",
      "Epoch:170 Loss:0.31258895993232727\n",
      "Epoch:171 Loss:0.31258925795555115\n",
      "Epoch:172 Loss:0.3125889003276825\n",
      "Epoch:173 Loss:0.31258851289749146\n",
      "Epoch:174 Loss:0.3125872313976288\n",
      "Epoch:175 Loss:0.31258660554885864\n",
      "Epoch:176 Loss:0.31258636713027954\n",
      "Epoch:177 Loss:0.31258612871170044\n",
      "Epoch:178 Loss:0.3125857412815094\n",
      "Epoch:179 Loss:0.3125850260257721\n",
      "Epoch:180 Loss:0.31258466839790344\n",
      "Epoch:181 Loss:0.3125842213630676\n",
      "Epoch:182 Loss:0.312583863735199\n",
      "Epoch:183 Loss:0.31258347630500793\n",
      "Epoch:184 Loss:0.31258365511894226\n",
      "Epoch:185 Loss:0.31258323788642883\n",
      "Epoch:186 Loss:0.3125828504562378\n",
      "Epoch:187 Loss:0.3125823140144348\n",
      "Epoch:188 Loss:0.31258201599121094\n",
      "Epoch:189 Loss:0.3125815689563751\n",
      "Epoch:190 Loss:0.3125806152820587\n",
      "Epoch:191 Loss:0.3125807046890259\n",
      "Epoch:192 Loss:0.3125803470611572\n",
      "Epoch:193 Loss:0.31258004903793335\n",
      "Epoch:194 Loss:0.31257957220077515\n",
      "Epoch:195 Loss:0.31257936358451843\n",
      "Epoch:196 Loss:0.31257903575897217\n",
      "Epoch:197 Loss:0.31257766485214233\n",
      "Epoch:198 Loss:0.31257718801498413\n",
      "Epoch:199 Loss:0.31257694959640503\n",
      "Epoch:200 Loss:0.31257638335227966\n",
      "Epoch:201 Loss:0.3125757873058319\n",
      "Epoch:202 Loss:0.3125753104686737\n",
      "Epoch:203 Loss:0.3125748336315155\n",
      "Epoch:204 Loss:0.3125743269920349\n",
      "Epoch:205 Loss:0.3125738203525543\n",
      "Epoch:206 Loss:0.31257444620132446\n",
      "Epoch:207 Loss:0.3125740885734558\n",
      "Epoch:208 Loss:0.31257364153862\n",
      "Epoch:209 Loss:0.3125733733177185\n",
      "Epoch:210 Loss:0.31257304549217224\n",
      "Epoch:211 Loss:0.3125726580619812\n",
      "Epoch:212 Loss:0.3125722408294678\n",
      "Epoch:213 Loss:0.3125718832015991\n",
      "Epoch:214 Loss:0.3125716745853424\n",
      "Epoch:215 Loss:0.31257152557373047\n",
      "Epoch:216 Loss:0.3125712275505066\n",
      "Epoch:217 Loss:0.3125706613063812\n",
      "Epoch:218 Loss:0.3125704526901245\n",
      "Epoch:219 Loss:0.3125699758529663\n",
      "Epoch:220 Loss:0.31256943941116333\n",
      "Epoch:221 Loss:0.3125692307949066\n",
      "Epoch:222 Loss:0.3125689923763275\n",
      "Epoch:223 Loss:0.31256890296936035\n",
      "Epoch:224 Loss:0.3125666081905365\n",
      "Epoch:225 Loss:0.3125663101673126\n",
      "Epoch:226 Loss:0.31256693601608276\n",
      "Epoch:227 Loss:0.3125665485858917\n",
      "Epoch:228 Loss:0.31256628036499023\n",
      "Epoch:229 Loss:0.3125658929347992\n",
      "Epoch:230 Loss:0.31256571412086487\n",
      "Epoch:231 Loss:0.3125641942024231\n",
      "Epoch:232 Loss:0.3125639259815216\n",
      "Epoch:233 Loss:0.31256356835365295\n",
      "Epoch:234 Loss:0.31256338953971863\n",
      "Epoch:235 Loss:0.3125638961791992\n",
      "Epoch:236 Loss:0.31256356835365295\n",
      "Epoch:237 Loss:0.31256231665611267\n",
      "Epoch:238 Loss:0.31256189942359924\n",
      "Epoch:239 Loss:0.31256169080734253\n",
      "Epoch:240 Loss:0.31256136298179626\n",
      "Epoch:241 Loss:0.31256213784217834\n",
      "Epoch:242 Loss:0.3125608563423157\n",
      "Epoch:243 Loss:0.31256040930747986\n",
      "Epoch:244 Loss:0.3125602900981903\n",
      "Epoch:245 Loss:0.3125600814819336\n",
      "Epoch:246 Loss:0.3125595152378082\n",
      "Epoch:247 Loss:0.31255921721458435\n",
      "Epoch:248 Loss:0.31255897879600525\n",
      "Epoch:249 Loss:0.31255853176116943\n",
      "Epoch:250 Loss:0.3125583529472351\n",
      "Epoch:251 Loss:0.3125581741333008\n",
      "Epoch:252 Loss:0.3125578463077545\n",
      "Epoch:253 Loss:0.31255650520324707\n",
      "Epoch:254 Loss:0.3125561773777008\n",
      "Epoch:255 Loss:0.3125559985637665\n",
      "Epoch:256 Loss:0.31255558133125305\n",
      "Epoch:257 Loss:0.31255537271499634\n",
      "Epoch:258 Loss:0.3125550150871277\n",
      "Epoch:259 Loss:0.31255456805229187\n",
      "Epoch:260 Loss:0.31255438923835754\n",
      "Epoch:261 Loss:0.31255412101745605\n",
      "Epoch:262 Loss:0.31255385279655457\n",
      "Epoch:263 Loss:0.3125534653663635\n",
      "Epoch:264 Loss:0.3125539720058441\n",
      "Epoch:265 Loss:0.31255385279655457\n",
      "Epoch:266 Loss:0.3125535249710083\n",
      "Epoch:267 Loss:0.31255295872688293\n",
      "Epoch:268 Loss:0.3125526010990143\n",
      "Epoch:269 Loss:0.31255221366882324\n",
      "Epoch:270 Loss:0.31255170702934265\n",
      "Epoch:271 Loss:0.31255149841308594\n",
      "Epoch:272 Loss:0.3125513792037964\n",
      "Epoch:273 Loss:0.3125510513782501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:274 Loss:0.31255075335502625\n",
      "Epoch:275 Loss:0.31254956126213074\n",
      "Epoch:276 Loss:0.3125494122505188\n",
      "Epoch:277 Loss:0.31254902482032776\n",
      "Epoch:278 Loss:0.3125487267971039\n",
      "Epoch:279 Loss:0.3125486969947815\n",
      "Epoch:280 Loss:0.3125482201576233\n",
      "Epoch:281 Loss:0.3125479817390442\n",
      "Epoch:282 Loss:0.31254786252975464\n",
      "Epoch:283 Loss:0.31254759430885315\n",
      "Epoch:284 Loss:0.3125474154949188\n",
      "Epoch:285 Loss:0.3125472366809845\n",
      "Epoch:286 Loss:0.3125472068786621\n",
      "Epoch:287 Loss:0.3125467896461487\n",
      "Epoch:288 Loss:0.31254667043685913\n",
      "Epoch:289 Loss:0.31254610419273376\n",
      "Epoch:290 Loss:0.31254616379737854\n",
      "Epoch:291 Loss:0.3125455677509308\n",
      "Epoch:292 Loss:0.31254565715789795\n",
      "Epoch:293 Loss:0.3125453591346741\n",
      "Epoch:294 Loss:0.3125452399253845\n",
      "Epoch:295 Loss:0.31254372000694275\n",
      "Epoch:296 Loss:0.3125438392162323\n",
      "Epoch:297 Loss:0.31254351139068604\n",
      "Epoch:298 Loss:0.31254321336746216\n",
      "Epoch:299 Loss:0.31254297494888306\n",
      "Epoch:300 Loss:0.31254351139068604\n",
      "Epoch:301 Loss:0.3125430941581726\n",
      "Epoch:302 Loss:0.31254318356513977\n",
      "Epoch:303 Loss:0.31254270672798157\n",
      "Epoch:304 Loss:0.312542587518692\n",
      "Epoch:305 Loss:0.3125423491001129\n",
      "Epoch:306 Loss:0.31254148483276367\n",
      "Epoch:307 Loss:0.31254205107688904\n",
      "Epoch:308 Loss:0.31254178285598755\n",
      "Epoch:309 Loss:0.3125416934490204\n",
      "Epoch:310 Loss:0.31254124641418457\n",
      "Epoch:311 Loss:0.31254109740257263\n",
      "Epoch:312 Loss:0.3125409185886383\n",
      "Epoch:313 Loss:0.31254056096076965\n",
      "Epoch:314 Loss:0.3125404715538025\n",
      "Epoch:315 Loss:0.31254011392593384\n",
      "Epoch:316 Loss:0.31254005432128906\n",
      "Epoch:317 Loss:0.31254035234451294\n",
      "Epoch:318 Loss:0.31253963708877563\n",
      "Epoch:319 Loss:0.3125385046005249\n",
      "Epoch:320 Loss:0.3125382661819458\n",
      "Epoch:321 Loss:0.3125380277633667\n",
      "Epoch:322 Loss:0.31253790855407715\n",
      "Epoch:323 Loss:0.31253767013549805\n",
      "Epoch:324 Loss:0.31253740191459656\n",
      "Epoch:325 Loss:0.31253746151924133\n",
      "Epoch:326 Loss:0.31253698468208313\n",
      "Epoch:327 Loss:0.3125368058681488\n",
      "Epoch:328 Loss:0.3125360608100891\n",
      "Epoch:329 Loss:0.3125356137752533\n",
      "Epoch:330 Loss:0.31253552436828613\n",
      "Epoch:331 Loss:0.3125358521938324\n",
      "Epoch:332 Loss:0.3125355541706085\n",
      "Epoch:333 Loss:0.31253546476364136\n",
      "Epoch:334 Loss:0.3125353157520294\n",
      "Epoch:335 Loss:0.31253522634506226\n",
      "Epoch:336 Loss:0.31253498792648315\n",
      "Epoch:337 Loss:0.31253424286842346\n",
      "Epoch:338 Loss:0.31253451108932495\n",
      "Epoch:339 Loss:0.3125343918800354\n",
      "Epoch:340 Loss:0.3125341236591339\n",
      "Epoch:341 Loss:0.3125338852405548\n",
      "Epoch:342 Loss:0.3125336766242981\n",
      "Epoch:343 Loss:0.312533438205719\n",
      "Epoch:344 Loss:0.31253325939178467\n",
      "Epoch:345 Loss:0.31253311038017273\n",
      "Epoch:346 Loss:0.3125329613685608\n",
      "Epoch:347 Loss:0.3125327527523041\n",
      "Epoch:348 Loss:0.31253179907798767\n",
      "Epoch:349 Loss:0.3125315308570862\n",
      "Epoch:350 Loss:0.31253135204315186\n",
      "Epoch:351 Loss:0.31253111362457275\n",
      "Epoch:352 Loss:0.3125309944152832\n",
      "Epoch:353 Loss:0.3125307857990265\n",
      "Epoch:354 Loss:0.3125307261943817\n",
      "Epoch:355 Loss:0.3125304579734802\n",
      "Epoch:356 Loss:0.3125301003456116\n",
      "Epoch:357 Loss:0.31252995133399963\n",
      "Epoch:358 Loss:0.3125297427177429\n",
      "Epoch:359 Loss:0.31252968311309814\n",
      "Epoch:360 Loss:0.3125295042991638\n",
      "Epoch:361 Loss:0.3125292658805847\n",
      "Epoch:362 Loss:0.31252923607826233\n",
      "Epoch:363 Loss:0.3125290870666504\n",
      "Epoch:364 Loss:0.31252872943878174\n",
      "Epoch:365 Loss:0.3125285506248474\n",
      "Epoch:366 Loss:0.31252849102020264\n",
      "Epoch:367 Loss:0.31252825260162354\n",
      "Epoch:368 Loss:0.3125280737876892\n",
      "Epoch:369 Loss:0.3125280439853668\n",
      "Epoch:370 Loss:0.31252777576446533\n",
      "Epoch:371 Loss:0.31252771615982056\n",
      "Epoch:372 Loss:0.31252750754356384\n",
      "Epoch:373 Loss:0.3125270903110504\n",
      "Epoch:374 Loss:0.3125268816947937\n",
      "Epoch:375 Loss:0.31252679228782654\n",
      "Epoch:376 Loss:0.3125268220901489\n",
      "Epoch:377 Loss:0.3125264644622803\n",
      "Epoch:378 Loss:0.31252628564834595\n",
      "Epoch:379 Loss:0.3125261962413788\n",
      "Epoch:380 Loss:0.31252604722976685\n",
      "Epoch:381 Loss:0.3125256597995758\n",
      "Epoch:382 Loss:0.3125247061252594\n",
      "Epoch:383 Loss:0.3125247359275818\n",
      "Epoch:384 Loss:0.3125244975090027\n",
      "Epoch:385 Loss:0.31252408027648926\n",
      "Epoch:386 Loss:0.3125239312648773\n",
      "Epoch:387 Loss:0.31252390146255493\n",
      "Epoch:388 Loss:0.31252381205558777\n",
      "Epoch:389 Loss:0.31252366304397583\n",
      "Epoch:390 Loss:0.3125235438346863\n",
      "Epoch:391 Loss:0.31252312660217285\n",
      "Epoch:392 Loss:0.3125229477882385\n",
      "Epoch:393 Loss:0.3125227987766266\n",
      "Epoch:394 Loss:0.31252244114875793\n",
      "Epoch:395 Loss:0.31252244114875793\n",
      "Epoch:396 Loss:0.31252238154411316\n",
      "Epoch:397 Loss:0.3125222325325012\n",
      "Epoch:398 Loss:0.3125219941139221\n",
      "Epoch:399 Loss:0.3125217854976654\n",
      "Epoch:400 Loss:0.3125216066837311\n",
      "Epoch:401 Loss:0.3125215172767639\n",
      "Epoch:402 Loss:0.3125212788581848\n",
      "Epoch:403 Loss:0.31252118945121765\n",
      "Epoch:404 Loss:0.3125210106372833\n",
      "Epoch:405 Loss:0.3125208020210266\n",
      "Epoch:406 Loss:0.3125205934047699\n",
      "Epoch:407 Loss:0.31252044439315796\n",
      "Epoch:408 Loss:0.312520295381546\n",
      "Epoch:409 Loss:0.312519907951355\n",
      "Epoch:410 Loss:0.3125195801258087\n",
      "Epoch:411 Loss:0.31251925230026245\n",
      "Epoch:412 Loss:0.31251898407936096\n",
      "Epoch:413 Loss:0.31251877546310425\n",
      "Epoch:414 Loss:0.312518447637558\n",
      "Epoch:415 Loss:0.31251832842826843\n",
      "Epoch:416 Loss:0.31251826882362366\n",
      "Epoch:417 Loss:0.31251800060272217\n",
      "Epoch:418 Loss:0.3125178813934326\n",
      "Epoch:419 Loss:0.31251776218414307\n",
      "Epoch:420 Loss:0.31251761317253113\n",
      "Epoch:421 Loss:0.3125174641609192\n",
      "Epoch:422 Loss:0.31251639127731323\n",
      "Epoch:423 Loss:0.31251639127731323\n",
      "Epoch:424 Loss:0.31251609325408936\n",
      "Epoch:425 Loss:0.3125159442424774\n",
      "Epoch:426 Loss:0.31251588463783264\n",
      "Epoch:427 Loss:0.3125157058238983\n",
      "Epoch:428 Loss:0.3125154674053192\n",
      "Epoch:429 Loss:0.31251516938209534\n",
      "Epoch:430 Loss:0.3125150203704834\n",
      "Epoch:431 Loss:0.3125147819519043\n",
      "Epoch:432 Loss:0.3125147819519043\n",
      "Epoch:433 Loss:0.31251436471939087\n",
      "Epoch:434 Loss:0.31251421570777893\n",
      "Epoch:435 Loss:0.31251344084739685\n",
      "Epoch:436 Loss:0.3125132918357849\n",
      "Epoch:437 Loss:0.31251296401023865\n",
      "Epoch:438 Loss:0.312513530254364\n",
      "Epoch:439 Loss:0.3125133812427521\n",
      "Epoch:440 Loss:0.3125126361846924\n",
      "Epoch:441 Loss:0.31251245737075806\n",
      "Epoch:442 Loss:0.31251293420791626\n",
      "Epoch:443 Loss:0.31251224875450134\n",
      "Epoch:444 Loss:0.31251201033592224\n",
      "Epoch:445 Loss:0.3125118911266327\n",
      "Epoch:446 Loss:0.3125115931034088\n",
      "Epoch:447 Loss:0.3125115633010864\n",
      "Epoch:448 Loss:0.3125113546848297\n",
      "Epoch:449 Loss:0.3125112056732178\n",
      "Epoch:450 Loss:0.3125110864639282\n",
      "Epoch:451 Loss:0.31251096725463867\n",
      "Epoch:452 Loss:0.3125108480453491\n",
      "Epoch:453 Loss:0.3125106394290924\n",
      "Epoch:454 Loss:0.3125104010105133\n",
      "Epoch:455 Loss:0.31251028180122375\n",
      "Epoch:456 Loss:0.31251010298728943\n",
      "Epoch:457 Loss:0.3125099241733551\n",
      "Epoch:458 Loss:0.31250953674316406\n",
      "Epoch:459 Loss:0.31250953674316406\n",
      "Epoch:460 Loss:0.3125094175338745\n",
      "Epoch:461 Loss:0.31250932812690735\n",
      "Epoch:462 Loss:0.3125091791152954\n",
      "Epoch:463 Loss:0.31250903010368347\n",
      "Epoch:464 Loss:0.3125087320804596\n",
      "Epoch:465 Loss:0.31250855326652527\n",
      "Epoch:466 Loss:0.31250837445259094\n",
      "Epoch:467 Loss:0.3125082552433014\n",
      "Epoch:468 Loss:0.31250813603401184\n",
      "Epoch:469 Loss:0.3125079870223999\n",
      "Epoch:470 Loss:0.31250786781311035\n",
      "Epoch:471 Loss:0.3125068247318268\n",
      "Epoch:472 Loss:0.31250667572021484\n",
      "Epoch:473 Loss:0.3125064969062805\n",
      "Epoch:474 Loss:0.3125062584877014\n",
      "Epoch:475 Loss:0.31250613927841187\n",
      "Epoch:476 Loss:0.3125059902667999\n",
      "Epoch:477 Loss:0.3125058114528656\n",
      "Epoch:478 Loss:0.31250566244125366\n",
      "Epoch:479 Loss:0.3125056326389313\n",
      "Epoch:480 Loss:0.31250545382499695\n",
      "Epoch:481 Loss:0.3125052750110626\n",
      "Epoch:482 Loss:0.31250518560409546\n",
      "Epoch:483 Loss:0.31250497698783875\n",
      "Epoch:484 Loss:0.3125045895576477\n",
      "Epoch:485 Loss:0.31250447034835815\n",
      "Epoch:486 Loss:0.3125043511390686\n",
      "Epoch:487 Loss:0.3125041723251343\n",
      "Epoch:488 Loss:0.3125040829181671\n",
      "Epoch:489 Loss:0.3125040531158447\n",
      "Epoch:490 Loss:0.312503844499588\n",
      "Epoch:491 Loss:0.3125036060810089\n",
      "Epoch:492 Loss:0.31250348687171936\n",
      "Epoch:493 Loss:0.3125033676624298\n",
      "Epoch:494 Loss:0.3125030994415283\n",
      "Epoch:495 Loss:0.3125031292438507\n",
      "Epoch:496 Loss:0.3125028610229492\n",
      "Epoch:497 Loss:0.31250280141830444\n",
      "Epoch:498 Loss:0.3125026822090149\n",
      "Epoch:499 Loss:0.31250232458114624\n",
      "Epoch:500 Loss:0.3125021755695343\n",
      "Epoch:501 Loss:0.31250208616256714\n",
      "Epoch:502 Loss:0.31250202655792236\n",
      "Epoch:503 Loss:0.31250181794166565\n",
      "Epoch:504 Loss:0.31250157952308655\n",
      "Epoch:505 Loss:0.31250154972076416\n",
      "Epoch:506 Loss:0.3125014305114746\n",
      "Epoch:507 Loss:0.31250134110450745\n",
      "Epoch:508 Loss:0.31250107288360596\n",
      "Epoch:509 Loss:0.312500923871994\n",
      "Epoch:510 Loss:0.31250083446502686\n",
      "Epoch:511 Loss:0.3125007748603821\n",
      "Epoch:512 Loss:0.312500536441803\n",
      "Epoch:513 Loss:0.31250038743019104\n",
      "Epoch:514 Loss:0.31250008940696716\n",
      "Epoch:515 Loss:0.3124999403953552\n",
      "Epoch:516 Loss:0.3124997317790985\n",
      "Epoch:517 Loss:0.31249967217445374\n",
      "Epoch:518 Loss:0.3124994933605194\n",
      "Epoch:519 Loss:0.3124993145465851\n",
      "Epoch:520 Loss:0.31249916553497314\n",
      "Epoch:521 Loss:0.31249919533729553\n",
      "Epoch:522 Loss:0.31249892711639404\n",
      "Epoch:523 Loss:0.3124987781047821\n",
      "Epoch:524 Loss:0.31249865889549255\n",
      "Epoch:525 Loss:0.312498539686203\n",
      "Epoch:526 Loss:0.31249839067459106\n",
      "Epoch:527 Loss:0.3124983012676239\n",
      "Epoch:528 Loss:0.3124980032444\n",
      "Epoch:529 Loss:0.3124978840351105\n",
      "Epoch:530 Loss:0.3124968111515045\n",
      "Epoch:531 Loss:0.3124966025352478\n",
      "Epoch:532 Loss:0.31249651312828064\n",
      "Epoch:533 Loss:0.312496542930603\n",
      "Epoch:534 Loss:0.3124963045120239\n",
      "Epoch:535 Loss:0.31249624490737915\n",
      "Epoch:536 Loss:0.3124959170818329\n",
      "Epoch:537 Loss:0.3124958574771881\n",
      "Epoch:538 Loss:0.312495619058609\n",
      "Epoch:539 Loss:0.3124954104423523\n",
      "Epoch:540 Loss:0.3124954402446747\n",
      "Epoch:541 Loss:0.31249526143074036\n",
      "Epoch:542 Loss:0.31249505281448364\n",
      "Epoch:543 Loss:0.3124949038028717\n",
      "Epoch:544 Loss:0.3124946653842926\n",
      "Epoch:545 Loss:0.31249454617500305\n",
      "Epoch:546 Loss:0.3124944269657135\n",
      "Epoch:547 Loss:0.3124942183494568\n",
      "Epoch:548 Loss:0.3124940097332001\n",
      "Epoch:549 Loss:0.31249406933784485\n",
      "Epoch:550 Loss:0.3124937415122986\n",
      "Epoch:551 Loss:0.3124935030937195\n",
      "Epoch:552 Loss:0.31249329447746277\n",
      "Epoch:553 Loss:0.31249314546585083\n",
      "Epoch:554 Loss:0.31249314546585083\n",
      "Epoch:555 Loss:0.3124930262565613\n",
      "Epoch:556 Loss:0.3124927282333374\n",
      "Epoch:557 Loss:0.312492698431015\n",
      "Epoch:558 Loss:0.3124925494194031\n",
      "Epoch:559 Loss:0.31249234080314636\n",
      "Epoch:560 Loss:0.3124922513961792\n",
      "Epoch:561 Loss:0.31249213218688965\n",
      "Epoch:562 Loss:0.31249192357063293\n",
      "Epoch:563 Loss:0.3124917447566986\n",
      "Epoch:564 Loss:0.31249162554740906\n",
      "Epoch:565 Loss:0.31249162554740906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:566 Loss:0.31249144673347473\n",
      "Epoch:567 Loss:0.31249135732650757\n",
      "Epoch:568 Loss:0.31249117851257324\n",
      "Epoch:569 Loss:0.31249094009399414\n",
      "Epoch:570 Loss:0.3124907910823822\n",
      "Epoch:571 Loss:0.3124905526638031\n",
      "Epoch:572 Loss:0.31249043345451355\n",
      "Epoch:573 Loss:0.3124902844429016\n",
      "Epoch:574 Loss:0.31249016523361206\n",
      "Epoch:575 Loss:0.31248998641967773\n",
      "Epoch:576 Loss:0.3124898374080658\n",
      "Epoch:577 Loss:0.3124896287918091\n",
      "Epoch:578 Loss:0.3124895393848419\n",
      "Epoch:579 Loss:0.31248942017555237\n",
      "Epoch:580 Loss:0.3124893009662628\n",
      "Epoch:581 Loss:0.31248927116394043\n",
      "Epoch:582 Loss:0.31248900294303894\n",
      "Epoch:583 Loss:0.31248870491981506\n",
      "Epoch:584 Loss:0.3124884366989136\n",
      "Epoch:585 Loss:0.31248828768730164\n",
      "Epoch:586 Loss:0.3124881088733673\n",
      "Epoch:587 Loss:0.3124878406524658\n",
      "Epoch:588 Loss:0.3124876022338867\n",
      "Epoch:589 Loss:0.31248748302459717\n",
      "Epoch:590 Loss:0.31248739361763\n",
      "Epoch:591 Loss:0.3124873638153076\n",
      "Epoch:592 Loss:0.31248724460601807\n",
      "Epoch:593 Loss:0.31248679757118225\n",
      "Epoch:594 Loss:0.3124867379665375\n",
      "Epoch:595 Loss:0.3124864101409912\n",
      "Epoch:596 Loss:0.31248635053634644\n",
      "Epoch:597 Loss:0.31248608231544495\n",
      "Epoch:598 Loss:0.31248602271080017\n",
      "Epoch:599 Loss:0.312485933303833\n",
      "Epoch:600 Loss:0.3124857544898987\n",
      "Epoch:601 Loss:0.3124854564666748\n",
      "Epoch:602 Loss:0.3124855160713196\n",
      "Epoch:603 Loss:0.3124852180480957\n",
      "Epoch:604 Loss:0.3124842047691345\n",
      "Epoch:605 Loss:0.3124840259552002\n",
      "Epoch:606 Loss:0.31248384714126587\n",
      "Epoch:607 Loss:0.31248363852500916\n",
      "Epoch:608 Loss:0.3124834895133972\n",
      "Epoch:609 Loss:0.3124832212924957\n",
      "Epoch:610 Loss:0.3124832212924957\n",
      "Epoch:611 Loss:0.3124832510948181\n",
      "Epoch:612 Loss:0.3124830424785614\n",
      "Epoch:613 Loss:0.3124828040599823\n",
      "Epoch:614 Loss:0.3124825358390808\n",
      "Epoch:615 Loss:0.31248247623443604\n",
      "Epoch:616 Loss:0.3124822974205017\n",
      "Epoch:617 Loss:0.31248217821121216\n",
      "Epoch:618 Loss:0.31248193979263306\n",
      "Epoch:619 Loss:0.3124817907810211\n",
      "Epoch:620 Loss:0.3124816417694092\n",
      "Epoch:621 Loss:0.31248146295547485\n",
      "Epoch:622 Loss:0.3124808669090271\n",
      "Epoch:623 Loss:0.3124811053276062\n",
      "Epoch:624 Loss:0.3124805688858032\n",
      "Epoch:625 Loss:0.3124808073043823\n",
      "Epoch:626 Loss:0.3124804198741913\n",
      "Epoch:627 Loss:0.31248047947883606\n",
      "Epoch:628 Loss:0.3124794661998749\n",
      "Epoch:629 Loss:0.3124808073043823\n",
      "Epoch:630 Loss:0.31247952580451965\n",
      "Epoch:631 Loss:0.31247976422309875\n",
      "Epoch:632 Loss:0.3124801516532898\n",
      "Epoch:633 Loss:0.31248003244400024\n",
      "Epoch:634 Loss:0.31247973442077637\n",
      "Epoch:635 Loss:0.31248027086257935\n",
      "Epoch:636 Loss:0.31247851252555847\n",
      "Epoch:637 Loss:0.31247982382774353\n",
      "Epoch:638 Loss:0.3124793469905853\n",
      "Epoch:639 Loss:0.31248006224632263\n",
      "Epoch:640 Loss:0.3124804198741913\n",
      "Epoch:641 Loss:0.3124793469905853\n",
      "Epoch:642 Loss:0.3124801814556122\n",
      "Epoch:643 Loss:0.3124801814556122\n",
      "Epoch:644 Loss:0.31248074769973755\n",
      "Epoch:645 Loss:0.3124828040599823\n",
      "Epoch:646 Loss:0.3124847710132599\n",
      "Epoch:647 Loss:0.3124862611293793\n",
      "Epoch:648 Loss:0.31248730421066284\n",
      "Epoch:649 Loss:0.312494158744812\n",
      "Epoch:650 Loss:0.31249409914016724\n",
      "Epoch:651 Loss:0.3125002384185791\n",
      "Epoch:652 Loss:0.3124947249889374\n",
      "Epoch:653 Loss:0.3124940097332001\n",
      "Epoch:654 Loss:0.3124862015247345\n",
      "Epoch:655 Loss:0.31248047947883606\n",
      "Epoch:656 Loss:0.31247612833976746\n",
      "Epoch:657 Loss:0.3124770224094391\n",
      "Epoch:658 Loss:0.312479168176651\n",
      "Epoch:659 Loss:0.3124837875366211\n",
      "Epoch:660 Loss:0.31249716877937317\n",
      "Epoch:661 Loss:0.31250810623168945\n",
      "Epoch:662 Loss:0.3125450909137726\n",
      "Epoch:663 Loss:0.3125396966934204\n",
      "Epoch:664 Loss:0.3125477731227875\n",
      "Epoch:665 Loss:0.3124994933605194\n",
      "Epoch:666 Loss:0.3124818205833435\n",
      "Epoch:667 Loss:0.31251442432403564\n",
      "Epoch:668 Loss:0.31254491209983826\n",
      "Epoch:669 Loss:0.31269049644470215\n",
      "Epoch:670 Loss:0.312796950340271\n",
      "Epoch:671 Loss:0.3148820102214813\n",
      "Epoch:672 Loss:0.3259745240211487\n",
      "Epoch:673 Loss:0.3276534974575043\n",
      "Epoch:674 Loss:0.33076491951942444\n",
      "Epoch:675 Loss:0.3402191996574402\n",
      "Epoch:676 Loss:0.3303271234035492\n",
      "Epoch:677 Loss:0.35231897234916687\n",
      "Epoch:678 Loss:0.36717355251312256\n",
      "Epoch:679 Loss:0.362593412399292\n",
      "Epoch:680 Loss:0.3603365123271942\n",
      "Epoch:681 Loss:0.3746586740016937\n",
      "Epoch:682 Loss:0.36327752470970154\n",
      "Epoch:683 Loss:0.3953046500682831\n",
      "Epoch:684 Loss:0.4004196226596832\n",
      "Epoch:685 Loss:0.39830827713012695\n",
      "Epoch:686 Loss:0.4155338704586029\n",
      "Epoch:687 Loss:0.41586869955062866\n",
      "Epoch:688 Loss:0.4079461991786957\n",
      "Epoch:689 Loss:0.40836024284362793\n",
      "Epoch:690 Loss:0.4212511479854584\n",
      "Epoch:691 Loss:0.42253291606903076\n",
      "Epoch:692 Loss:0.42030641436576843\n",
      "Epoch:693 Loss:0.4136830270290375\n",
      "Epoch:694 Loss:0.4116533696651459\n",
      "Epoch:695 Loss:0.41160428524017334\n",
      "Epoch:696 Loss:0.4092588424682617\n",
      "Epoch:697 Loss:0.40323370695114136\n",
      "Epoch:698 Loss:0.40810418128967285\n",
      "Epoch:699 Loss:0.40811771154403687\n",
      "Epoch:700 Loss:0.40441322326660156\n",
      "Epoch:701 Loss:0.39704856276512146\n",
      "Epoch:702 Loss:0.39480990171432495\n",
      "Epoch:703 Loss:0.3958408236503601\n",
      "Epoch:704 Loss:0.3928656578063965\n",
      "Epoch:705 Loss:0.38393494486808777\n",
      "Epoch:706 Loss:0.3809170424938202\n",
      "Epoch:707 Loss:0.3841036856174469\n",
      "Epoch:708 Loss:0.3863601088523865\n",
      "Epoch:709 Loss:0.3854635953903198\n",
      "Epoch:710 Loss:0.37708529829978943\n",
      "Epoch:711 Loss:0.376596599817276\n",
      "Epoch:712 Loss:0.3816916346549988\n",
      "Epoch:713 Loss:0.382647842168808\n",
      "Epoch:714 Loss:0.3729383647441864\n",
      "Epoch:715 Loss:0.37146708369255066\n",
      "Epoch:716 Loss:0.3756162226200104\n",
      "Epoch:717 Loss:0.3716176152229309\n",
      "Epoch:718 Loss:0.3728991150856018\n",
      "Epoch:719 Loss:0.3764275312423706\n",
      "Epoch:720 Loss:0.371577650308609\n",
      "Epoch:721 Loss:0.37206652760505676\n",
      "Epoch:722 Loss:0.37357208132743835\n",
      "Epoch:723 Loss:0.3698405623435974\n",
      "Epoch:724 Loss:0.3672613501548767\n",
      "Epoch:725 Loss:0.36754244565963745\n",
      "Epoch:726 Loss:0.36646485328674316\n",
      "Epoch:727 Loss:0.3660924434661865\n",
      "Epoch:728 Loss:0.36729076504707336\n",
      "Epoch:729 Loss:0.36628347635269165\n",
      "Epoch:730 Loss:0.36595332622528076\n",
      "Epoch:731 Loss:0.3661363124847412\n",
      "Epoch:732 Loss:0.36778515577316284\n",
      "Epoch:733 Loss:0.40326449275016785\n",
      "Epoch:734 Loss:0.40194621682167053\n",
      "Epoch:735 Loss:0.4044854938983917\n",
      "Epoch:736 Loss:0.40211090445518494\n",
      "Epoch:737 Loss:0.40373319387435913\n",
      "Epoch:738 Loss:0.403536319732666\n",
      "Epoch:739 Loss:0.40368446707725525\n",
      "Epoch:740 Loss:0.4032606780529022\n",
      "Epoch:741 Loss:0.4032403528690338\n",
      "Epoch:742 Loss:0.3677593469619751\n",
      "Epoch:743 Loss:0.36436033248901367\n",
      "Epoch:744 Loss:0.36365240812301636\n",
      "Epoch:745 Loss:0.362691193819046\n",
      "Epoch:746 Loss:0.3622916340827942\n",
      "Epoch:747 Loss:0.3607347905635834\n",
      "Epoch:748 Loss:0.359825998544693\n",
      "Epoch:749 Loss:0.3578510582447052\n",
      "Epoch:750 Loss:0.35851386189460754\n",
      "Epoch:751 Loss:0.357787162065506\n",
      "Epoch:752 Loss:0.36074137687683105\n",
      "Epoch:753 Loss:0.3604463040828705\n",
      "Epoch:754 Loss:0.35920074582099915\n",
      "Epoch:755 Loss:0.3587196171283722\n",
      "Epoch:756 Loss:0.35678631067276\n",
      "Epoch:757 Loss:0.35544225573539734\n",
      "Epoch:758 Loss:0.3552549183368683\n",
      "Epoch:759 Loss:0.35608455538749695\n",
      "Epoch:760 Loss:0.35266241431236267\n",
      "Epoch:761 Loss:0.3542531430721283\n",
      "Epoch:762 Loss:0.35569456219673157\n",
      "Epoch:763 Loss:0.3598968982696533\n",
      "Epoch:764 Loss:0.3594819903373718\n",
      "Epoch:765 Loss:0.35968029499053955\n",
      "Epoch:766 Loss:0.35888949036598206\n",
      "Epoch:767 Loss:0.360971599817276\n",
      "Epoch:768 Loss:0.3584801256656647\n",
      "Epoch:769 Loss:0.35887032747268677\n",
      "Epoch:770 Loss:0.35841304063796997\n",
      "Epoch:771 Loss:0.35762402415275574\n",
      "Epoch:772 Loss:0.35879290103912354\n",
      "Epoch:773 Loss:0.3547312617301941\n",
      "Epoch:774 Loss:0.35667645931243896\n",
      "Epoch:775 Loss:0.3574676215648651\n",
      "Epoch:776 Loss:0.3559744358062744\n",
      "Epoch:777 Loss:0.35532885789871216\n",
      "Epoch:778 Loss:0.3537117540836334\n",
      "Epoch:779 Loss:0.3528192639350891\n",
      "Epoch:780 Loss:0.3523786664009094\n",
      "Epoch:781 Loss:0.3501860797405243\n",
      "Epoch:782 Loss:0.3488078713417053\n",
      "Epoch:783 Loss:0.34778741002082825\n",
      "Epoch:784 Loss:0.34603631496429443\n",
      "Epoch:785 Loss:0.3459441363811493\n",
      "Epoch:786 Loss:0.34634655714035034\n",
      "Epoch:787 Loss:0.3444211781024933\n",
      "Epoch:788 Loss:0.34183716773986816\n",
      "Epoch:789 Loss:0.33928993344306946\n",
      "Epoch:790 Loss:0.35312628746032715\n",
      "Epoch:791 Loss:0.34373295307159424\n",
      "Epoch:792 Loss:0.3474016487598419\n",
      "Epoch:793 Loss:0.3488130569458008\n",
      "Epoch:794 Loss:0.3512348532676697\n",
      "Epoch:795 Loss:0.35346171259880066\n",
      "Epoch:796 Loss:0.3537312150001526\n",
      "Epoch:797 Loss:0.3552373945713043\n",
      "Epoch:798 Loss:0.3554472327232361\n",
      "Epoch:799 Loss:0.3553532660007477\n",
      "Epoch:800 Loss:0.3571084141731262\n",
      "Epoch:801 Loss:0.35792142152786255\n",
      "Epoch:802 Loss:0.35869890451431274\n",
      "Epoch:803 Loss:0.35912036895751953\n",
      "Epoch:804 Loss:0.3587300777435303\n",
      "Epoch:805 Loss:0.3581220507621765\n",
      "Epoch:806 Loss:0.35665836930274963\n",
      "Epoch:807 Loss:0.3579324185848236\n",
      "Epoch:808 Loss:0.3571948707103729\n",
      "Epoch:809 Loss:0.35871267318725586\n",
      "Epoch:810 Loss:0.35862624645233154\n",
      "Epoch:811 Loss:0.35843566060066223\n",
      "Epoch:812 Loss:0.357324481010437\n",
      "Epoch:813 Loss:0.35587015748023987\n",
      "Epoch:814 Loss:0.35506027936935425\n",
      "Epoch:815 Loss:0.3546232283115387\n",
      "Epoch:816 Loss:0.3528708219528198\n",
      "Epoch:817 Loss:0.3540057837963104\n",
      "Epoch:818 Loss:0.3535950779914856\n",
      "Epoch:819 Loss:0.35267022252082825\n",
      "Epoch:820 Loss:0.3527342975139618\n",
      "Epoch:821 Loss:0.352483332157135\n",
      "Epoch:822 Loss:0.35241466760635376\n",
      "Epoch:823 Loss:0.3530096113681793\n",
      "Epoch:824 Loss:0.3519102931022644\n",
      "Epoch:825 Loss:0.352917343378067\n",
      "Epoch:826 Loss:0.3517068028450012\n",
      "Epoch:827 Loss:0.3518606126308441\n",
      "Epoch:828 Loss:0.35160431265830994\n",
      "Epoch:829 Loss:0.35092875361442566\n",
      "Epoch:830 Loss:0.35078006982803345\n",
      "Epoch:831 Loss:0.35043439269065857\n",
      "Epoch:832 Loss:0.34983816742897034\n",
      "Epoch:833 Loss:0.3495049476623535\n",
      "Epoch:834 Loss:0.34932634234428406\n",
      "Epoch:835 Loss:0.3490934371948242\n",
      "Epoch:836 Loss:0.34879153966903687\n",
      "Epoch:837 Loss:0.34802791476249695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:838 Loss:0.34781986474990845\n",
      "Epoch:839 Loss:0.347696989774704\n",
      "Epoch:840 Loss:0.34753578901290894\n",
      "Epoch:841 Loss:0.34740909934043884\n",
      "Epoch:842 Loss:0.3474592864513397\n",
      "Epoch:843 Loss:0.3482057750225067\n",
      "Epoch:844 Loss:0.3506859242916107\n",
      "Epoch:845 Loss:0.3508593440055847\n",
      "Epoch:846 Loss:0.34736400842666626\n",
      "Epoch:847 Loss:0.3550426959991455\n",
      "Epoch:848 Loss:0.34786495566368103\n",
      "Epoch:849 Loss:0.3506605625152588\n",
      "Epoch:850 Loss:0.3461874723434448\n",
      "Epoch:851 Loss:0.3490995466709137\n",
      "Epoch:852 Loss:0.34555521607398987\n",
      "Epoch:853 Loss:0.34781768918037415\n",
      "Epoch:854 Loss:0.34638741612434387\n",
      "Epoch:855 Loss:0.3468265235424042\n",
      "Epoch:856 Loss:0.34464502334594727\n",
      "Epoch:857 Loss:0.34418871998786926\n",
      "Epoch:858 Loss:0.34340593218803406\n",
      "Epoch:859 Loss:0.3437170386314392\n",
      "Epoch:860 Loss:0.34332922101020813\n",
      "Epoch:861 Loss:0.3426172435283661\n",
      "Epoch:862 Loss:0.342016339302063\n",
      "Epoch:863 Loss:0.3417193293571472\n",
      "Epoch:864 Loss:0.34165623784065247\n",
      "Epoch:865 Loss:0.34118130803108215\n",
      "Epoch:866 Loss:0.34030112624168396\n",
      "Epoch:867 Loss:0.34027326107025146\n",
      "Epoch:868 Loss:0.34009090065956116\n",
      "Epoch:869 Loss:0.3398340940475464\n",
      "Epoch:870 Loss:0.33972206711769104\n",
      "Epoch:871 Loss:0.3394731879234314\n",
      "Epoch:872 Loss:0.3382486402988434\n",
      "Epoch:873 Loss:0.33736422657966614\n",
      "Epoch:874 Loss:0.33733078837394714\n",
      "Epoch:875 Loss:0.33720362186431885\n",
      "Epoch:876 Loss:0.33618417382240295\n",
      "Epoch:877 Loss:0.33500805497169495\n",
      "Epoch:878 Loss:0.3385540843009949\n",
      "Epoch:879 Loss:0.33390897512435913\n",
      "Epoch:880 Loss:0.3351147770881653\n",
      "Epoch:881 Loss:0.33505114912986755\n",
      "Epoch:882 Loss:0.33543646335601807\n",
      "Epoch:883 Loss:0.33491963148117065\n",
      "Epoch:884 Loss:0.3356458246707916\n",
      "Epoch:885 Loss:0.33491310477256775\n",
      "Epoch:886 Loss:0.3348279297351837\n",
      "Epoch:887 Loss:0.338417112827301\n",
      "Epoch:888 Loss:0.3352636694908142\n",
      "Epoch:889 Loss:0.3336939215660095\n",
      "Epoch:890 Loss:0.3345608115196228\n",
      "Epoch:891 Loss:0.333192378282547\n",
      "Epoch:892 Loss:0.33399268984794617\n",
      "Epoch:893 Loss:0.3326749801635742\n",
      "Epoch:894 Loss:0.3374493718147278\n",
      "Epoch:895 Loss:0.34008342027664185\n",
      "Epoch:896 Loss:0.3475309908390045\n",
      "Epoch:897 Loss:0.34700077772140503\n",
      "Epoch:898 Loss:0.34665241837501526\n",
      "Epoch:899 Loss:0.3473401367664337\n",
      "Epoch:900 Loss:0.34702587127685547\n",
      "Epoch:901 Loss:0.3467343747615814\n",
      "Epoch:902 Loss:0.3453492522239685\n",
      "Epoch:903 Loss:0.3445981442928314\n",
      "Epoch:904 Loss:0.3447367250919342\n",
      "Epoch:905 Loss:0.34116414189338684\n",
      "Epoch:906 Loss:0.3375021815299988\n",
      "Epoch:907 Loss:0.34453466534614563\n",
      "Epoch:908 Loss:0.3496258556842804\n",
      "Epoch:909 Loss:0.346351683139801\n",
      "Epoch:910 Loss:0.3461620807647705\n",
      "Epoch:911 Loss:0.3407251834869385\n",
      "Epoch:912 Loss:0.3439078629016876\n",
      "Epoch:913 Loss:0.34678709506988525\n",
      "Epoch:914 Loss:0.34892919659614563\n",
      "Epoch:915 Loss:0.3511978089809418\n",
      "Epoch:916 Loss:0.34866759181022644\n",
      "Epoch:917 Loss:0.3464342951774597\n",
      "Epoch:918 Loss:0.34525004029273987\n",
      "Epoch:919 Loss:0.3444760739803314\n",
      "Epoch:920 Loss:0.34256651997566223\n",
      "Epoch:921 Loss:0.3413897752761841\n",
      "Epoch:922 Loss:0.3382512331008911\n",
      "Epoch:923 Loss:0.33820831775665283\n",
      "Epoch:924 Loss:0.33836108446121216\n",
      "Epoch:925 Loss:0.33665239810943604\n",
      "Epoch:926 Loss:0.3377837538719177\n",
      "Epoch:927 Loss:0.3367007076740265\n",
      "Epoch:928 Loss:0.33678629994392395\n",
      "Epoch:929 Loss:0.336517870426178\n",
      "Epoch:930 Loss:0.33568957448005676\n",
      "Epoch:931 Loss:0.33606597781181335\n",
      "Epoch:932 Loss:0.335788756608963\n",
      "Epoch:933 Loss:0.33487966656684875\n",
      "Epoch:934 Loss:0.3357641100883484\n",
      "Epoch:935 Loss:0.33675679564476013\n",
      "Epoch:936 Loss:0.33672669529914856\n",
      "Epoch:937 Loss:0.33614692091941833\n",
      "Epoch:938 Loss:0.3372603952884674\n",
      "Epoch:939 Loss:0.335486501455307\n",
      "Epoch:940 Loss:0.3352489173412323\n",
      "Epoch:941 Loss:0.33543848991394043\n",
      "Epoch:942 Loss:0.33509379625320435\n",
      "Epoch:943 Loss:0.3345009684562683\n",
      "Epoch:944 Loss:0.3342761695384979\n",
      "Epoch:945 Loss:0.33432188630104065\n",
      "Epoch:946 Loss:0.33353424072265625\n",
      "Epoch:947 Loss:0.3334350883960724\n",
      "Epoch:948 Loss:0.3331460654735565\n",
      "Epoch:949 Loss:0.3327874541282654\n",
      "Epoch:950 Loss:0.33236879110336304\n",
      "Epoch:951 Loss:0.3324132561683655\n",
      "Epoch:952 Loss:0.33193719387054443\n",
      "Epoch:953 Loss:0.33232495188713074\n",
      "Epoch:954 Loss:0.3319767713546753\n",
      "Epoch:955 Loss:0.3325612246990204\n",
      "Epoch:956 Loss:0.3319530189037323\n",
      "Epoch:957 Loss:0.3320716917514801\n",
      "Epoch:958 Loss:0.3314548432826996\n",
      "Epoch:959 Loss:0.33146366477012634\n",
      "Epoch:960 Loss:0.33130261301994324\n",
      "Epoch:961 Loss:0.33111774921417236\n",
      "Epoch:962 Loss:0.3312034606933594\n",
      "Epoch:963 Loss:0.3309980034828186\n",
      "Epoch:964 Loss:0.3308860957622528\n",
      "Epoch:965 Loss:0.33088722825050354\n",
      "Epoch:966 Loss:0.33078497648239136\n",
      "Epoch:967 Loss:0.3306127190589905\n",
      "Epoch:968 Loss:0.3306182026863098\n",
      "Epoch:969 Loss:0.33045369386672974\n",
      "Epoch:970 Loss:0.3304165005683899\n",
      "Epoch:971 Loss:0.3303491771221161\n",
      "Epoch:972 Loss:0.33020293712615967\n",
      "Epoch:973 Loss:0.33007505536079407\n",
      "Epoch:974 Loss:0.329386442899704\n",
      "Epoch:975 Loss:0.32901808619499207\n",
      "Epoch:976 Loss:0.3289666473865509\n",
      "Epoch:977 Loss:0.328888475894928\n",
      "Epoch:978 Loss:0.3287721276283264\n",
      "Epoch:979 Loss:0.3281494975090027\n",
      "Epoch:980 Loss:0.3276127874851227\n",
      "Epoch:981 Loss:0.32760921120643616\n",
      "Epoch:982 Loss:0.3276483714580536\n",
      "Epoch:983 Loss:0.32771003246307373\n",
      "Epoch:984 Loss:0.3278294801712036\n",
      "Epoch:985 Loss:0.32798269391059875\n",
      "Epoch:986 Loss:0.3278771638870239\n",
      "Epoch:987 Loss:0.3277164101600647\n",
      "Epoch:988 Loss:0.3276157081127167\n",
      "Epoch:989 Loss:0.32751819491386414\n",
      "Epoch:990 Loss:0.32724106311798096\n",
      "Epoch:991 Loss:0.3264235854148865\n",
      "Epoch:992 Loss:0.32579901814460754\n",
      "Epoch:993 Loss:0.3255584239959717\n",
      "Epoch:994 Loss:0.3251557946205139\n",
      "Epoch:995 Loss:0.32512232661247253\n",
      "Epoch:996 Loss:0.32494595646858215\n",
      "Epoch:997 Loss:0.3257845938205719\n",
      "Epoch:998 Loss:0.32457298040390015\n",
      "Epoch:999 Loss:0.324491024017334\n"
     ]
    }
   ],
   "source": [
    "# 注意：这里目前还是整体训练，之后会batch训练\n",
    "for epoch in range(1000):\n",
    "    # Forward\n",
    "    y_pred = model(x_data)\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print('Epoch:{} Loss:{}'.format(epoch, loss.item()))\n",
    "    \n",
    "    # Backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:YOLO_ATTACK]",
   "language": "python",
   "name": "conda-env-YOLO_ATTACK-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
